[#using-backup-restore]
= Scheduling backups

Complete the following steps to schedule backups:

. Run the following command to create a `backupschedule.cluster.open-cluster-management.io` resource:
+
----
oc create -f cluster_v1beta1_backupschedule.yaml
----
+
Your `cluster_v1beta1_backupschedule.yaml` resource might resemble the following file:
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: BackupSchedule
metadata:
  name: schedule-acm
  namespace: open-cluster-management-backup
spec:
  veleroSchedule: 0 */2 * * * <1>
  veleroTtl: 120h <2>
useManagedServiceAccount: true <3> 
paused: false <4>
----
+
<1> Creates a backup every 2 hours
<2> *Optional:* Deletes scheduled backups after 120h. If not specified, the maximum Velero default value of 720h is used.
<3> *Optional:* If you set it to `true`, it enables the automatic import feature, importing clusters in a restore operation.
<4> *Optional:*  If you set it to `true`, it pauses the backup schedule, deleting all Velero schedules created by this resource. 
+
View the following descriptions of the `backupschedule.cluster.open-cluster-management.io` `spec` properties:
+
** `veleroSchedule` is a required property and defines a cron job for scheduling the backups.
** `veleroTtl` is an optional property and defines the expiration time for a scheduled backup resource. If not specified, the maximum default value set by Velero is used, which is `720h`.
** `useManagedServiceAccount` is an optional property which can enable the automatic import feature, importing clusters in a restore operation. 
** `paused` is an optional property which can can pause the backup schedule, deleting all Velero schedules created by the resource. 

. Check the status of your `backupschedule.cluster.open-cluster-management.io` resource, which displays the definition for the three `schedule.velero.io` resources. Run the following command:
+
----
oc get BackupSchedule -n open-cluster-management-backup
----

. Since the restore operation runs on a different hub cluster for restore scenarios, initiate a restore operation by creating a `restore.cluster.open-cluster-management.io` resource on the hub cluster where you want to restore backups.
+
*Note:* When you restore a backup on a new hub cluster, make sure that you shut down the earlier hub cluster wher you created  the backup. If it is running, the earlier hub cluster tries to reimport the managed clusters as soon as the managed cluster reconciliation finds that the managed clusters are no longer available.

A backup schedule is activated when you create the `backupschedule.cluster.open-cluster-management.io` resource. View the following `backupschedule.cluster.open-cluster-management.io` sample:

[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: BackupSchedule
metadata:
  name: schedule-acm
  namespace: open-cluster-management-backup
spec:
  veleroSchedule: 0 */2 * * *
  veleroTtl: 120h
----

After you create a `backupschedule.cluster.open-cluster-management.io` resource, run the following command to get the status of the scheduled cluster backups:

----
oc get BackupSchedule -n open-cluster-management-backup
----

The `backupschedule.cluster.open-cluster-management.io` resource creates six `schedule.velero.io` resources, which are used to generate backups. Run the following command to view the list of the backups that are scheduled:

----
oc get schedules -A | grep acm
----

Resources are separately backed up in the groups as seen in the following table:

.Resource groups table
|===
| Resource | Description

| `Credentials` backup
| Backup file that stores Hive credentials, {acm-short}, and user-created credentials and ConfigMaps.

| `Resources` backup
| Contains one backup for the {acm-short} resources and one for generic resources. These resources use the `cluster.open-cluster-management.io/backup` label.

| `ManagedClusters` backup
| Contains only resources that activate the managed cluster connection to the hub cluster, where the backup is restored.
|===

*Note:* The _resources backup_ file contains managed cluster-specific resources, but does not contain the subset of resources that connect managed clusters to the hub cluster. The resources that connect managed clusters are called activation resources and are contained in the managed clusters backup. When you restore backups only for the _credentials_ and _resources_ backup on a new hub cluster, the new hub cluster shows all managed clusters that are created by using the Hive API in a detached state. The managed clusters that are imported on the primary hub cluster by using the import operation appear only when the activation data is restored on the passive hub cluster. The managed clusters are still connected to the original hub cluster that created the backup files.

When the activation data is restored, only managed clusters created by using the Hive API are automatically connected with the new hub cluster. All other managed clusters appear in a _Pending_ state. You need to manually reattach them to the new cluster.

For descriptions of the various `BackupSchedule` statuses, see the following table: 

.BackupSchedule status table
|===
| `BackupSchedule` status | Description

| `Enabled`
| The `BackupSchedule` is running and generating backups.

| `FailedValidation`
| An error prevents the `BackupSchedule` from running. As a result, the `BackupSchedule` is not generating backups, but instead, it is reconciling and waiting for the error to be fixed. Look at the `BackupSchedule` status section for the reason why the resource is not valid. After the error is addressed, the `BackupSchedule` status changes to `Enabled`, and the resource starts generating backups.

| `BackupCollision`
| The `BackupSchedule` is not generating backups. Look at the `BackupSchedule` status section for the reason why the resource status is `BackupCollision`. To start creating backups, delete this resource and create a new one. 
|===

[#avoid-backup-collision]
=== Avoiding backup collisions

Backup collisions might occur if the hub cluster changes from being a passive hub cluster to becoming a primary hub cluster, or the other way around, and different managed clusters back up data at the same storage location.

As a result, the latest backups are generated by a hub cluster that is no longer set as the primary hub cluster. This hub cluster still produces backups because the `BackupSchedule.cluster.open-cluster-management.io` resource is still enabled.

See the following list to learn about two scenarios that might cause a backup collision:

. The primary hub cluster fails unexpectedly, which is caused by the following conditions:
- Communication from the primary hub cluster to the initial hub cluster fails.
- The initial hub cluster backup data is restored on a secondary hub cluster, called secondary hub cluster.
- The administrator creates the `BackupSchedule.cluster.open-cluster-management.io` resource on the secondary hub cluster, which is now the primary hub cluster and generates backup data to the common storage location.
- The initial hub cluster unexpectedly starts working again.
+
Since the `BackupSchedule.cluster.open-cluster-management.io` resource is still enabled on the initial hub cluster, the initial hub cluster resumes writing backups to the same storage location as the secondary hub cluster. Both hub clusters are now writing backup data at the same storage location. Any hub cluster restoring the latest backups from this storage location might use the initial hub cluster data instead of the secondary hub cluster data.

. The administrator tests a disaster scenario by making the secondary hub cluster a primary hub cluster, which is caused by the following conditions:
- The initial hub cluster is stopped.
- The initial hub cluster backup data is restored on the secondary hub cluster.
- The administrator creates the `BackupSchedule.cluster.open-cluster-management.io` resource on the secondary hub cluster, which is now the primary hub cluster and generates backup data to the common storage location.
- After the disaster test is completed, the administrator reverts to the earlier state and makes the initial hub cluster the primary hub cluster again. 
- The initial hub cluster starts while the secondary hub cluster is still active.
+
Since the `BackupSchedule.cluster.open-cluster-management.io` resource is still enabled on the secondary hub cluster, it writes backups at the same storage location that corrupts the backup data. Any hub cluster restoring the latest backups from this location might use the secondary hub cluster data instead of the initial hub cluster data. In this scenario, stopping the secondary hub cluster first or deleting the `BackupSchedule.cluster.open-cluster-management.io` resource on the secondary hub cluster before starting the initial hub cluster fixes the backup collision issue.

. The administrator tests a disaster scenario by making the secondary hub cluster a primary hub cluster, without stopping the initial hub cluster first, causing the following conditions: 
- The initial hub cluster is still running.
- The initial hub cluster backup data is restored on the secondary hub cluster, including managed clusters backup. Therefore, the secondary hub cluster is now the active cluster.
- Since the `BackupSchedule.cluster.open-cluster-management.io` resource is still enabled on the initial hub cluster, it writes backups at the same storage location which corrupts the backup data. For example, any hub cluster restoring the latest backups from this location might use the initial hub cluster data instead of the secondary hub cluster data. To avoid data corruption, the initial hub cluster `BackupSchedule` resource status automatically changes to `BackupCollision`. In this scenario, to avoid getting into this backup collision state, stop the initial hub cluster first or delete the `BackupSchedule.cluster.open-cluster-management.io` resource on the initial hub cluster before restoring managed clusters data on  the secondary hub cluster.
+
To avoid and report backup collisions, a `BackupCollision` state exists for the `BackupSchedule.cluster.open-cluster-management.io` resource. The controller checks regularly if the latest backup in the storage location has been generated from the current hub cluster. If not, a different hub cluster has recently written backup data to the storage location, indicating that the hub cluster is colliding with a different hub cluster.

In this case, the current hub cluster `BackupSchedule.cluster.open-cluster-management.io` resource status is set to `BackupCollision` and the `Schedule.velero.io` resources created by this resource are deleted to avoid data corruption. The `BackupCollision` is reported by the backup policy. The administrator verifies which hub cluster writes to the storage location, before removing the `BackupSchedule.cluster.open-cluster-management.io` resource from the invalid hub cluster and creating a new `BackupSchedule.cluster.open-cluster-management.io` resource on the valid primary hub cluster, to resume the backup.

Run the following command to check if there is a backup collision:

----
oc get backupschedule -A
----

If there is a backup collision, the output might resemble the following example:

----
NAMESPACE       NAME               PHASE             MESSAGE
openshift-adp   schedule-hub-1   BackupCollision   Backup acm-resources-schedule-20220301234625, from cluster with id [be97a9eb-60b8-4511-805c-298e7c0898b3] is using the same storage location. This is a backup collision with current cluster [1f30bfe5-0588-441c-889e-eaf0ae55f941] backup. Review and resolve the collision then create a new BackupSchedule resource to  resume backups from this cluster.
----

[#dr4hub-schedule-resources]
== Additional resources

- See the xref:../backup_restore/backup_restore.adoc#restore-backup[Restoring a backup] section for descriptions of the parameters and samples of `Restore` YAML resources.

- For more information, see xref:../backup_restore/backup_intro.adoc#backup-intro[Backup and restore]. 