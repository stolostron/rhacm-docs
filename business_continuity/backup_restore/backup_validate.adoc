[#backup-validation-using-a-policy]
= Validating your backup or restore configurations
 
When you set the `cluster-backup` option to `true` on the `MultiClusterHub` resource, {mce-short} installs the cluster backup and restore operator Helm chart that is named the `cluster-backup-chart`. This chart then installs the `backup-restore-enabled` and `backup-restore-auto-import` policies. Use these policies to view information about issues with your backup and restore components.

*Note:* A hub cluster is automatically imported and managed by itself by using the `local-cluster` managed cluster. If you disable this by setting `disableHubSelfManagement=true` on the `MultiClusterHub` resource, the `backup-restore-enabled` policy is not placed on the hub cluster, and the policy templates do not produce any reports.

If a cluster hub is managed by a global hub cluster, or if it is installed on a managed cluster instance, the `disableHubSelfManagement=true` setting is disabled. In this instance, you can enable the `backup-restore-enabled` policy. Enable the policy by setting the `is-hub=true` label on the `ManagedCluster` resource that represents the managed hub cluster. 

The `backup-restore-enabled` policy includes a set of templates that check for the following constraints:

- *OADP channel validation*
+
* When you enable the backup component on the `MultiClusterHub`, the cluster backup and restore operator Helm chart can automatically install the OADP operator in the `open-cluster-management-backup` namespace. You can also manually install the OADP operator in the namespace. 
* The `OADP-channel` that you selected for manual installation must match or exceed the version set by the {product-title-short} backup and restore operator Helm chart. 
* Since the OADP Operator and Velero Custom Resource Definitions (CRDs) are `cluster-scoped`, you cannot have multiple versions on the same cluster. Therefore, install the same version in the `open-cluster-management-backup` namespace and any other namespaces. 
* Use the following templates to check for availability and validate the OADP installation: 
** `oadp-operator-exists`: Use this template to verify if the OADP operator 
is installed in the `open-cluster-management-backup` namespace.
** `oadp-channel-validation`: Use this template to ensure the OADP operator version in the `open-cluster-management-backup` namespace matches or exceeds the version set by the {product-title-short} backup and restore operator.
** `custom-oadp-channel-validation`: Use this template to check if OADP operators in other namespaces match the version in the `open-cluster-management-backup` namespace.

- *Pod validation*
+
The following templates check the pod status for the backup component and dependencies:
+
** `acm-backup-pod-running` template checks if the backup and restore operator pod is running.
** `oadp-pod-running` template checks if the OADP operator pod is running. 
** `velero-pod-running` template checks if the Velero pod is running.

- *Data Protection Application validation*
+
* `data-protection-application-available` template checks if a `DataProtectioApplicatio.oadp.openshift.io` resource is created. This OADP resource sets up Velero configurations.

- *Backup storage validation*
+
* `backup-storage-location-available` template checks if a `BackupStorageLocation.velero.io` resource is created and if the status value is `Available`. This implies that the connection to the backup storage is valid. 

- *BackupSchedule collision validation*
+
* `acm-backup-clusters-collision-report` template verifies that the status is not `BackupCollision`, if a `BackupSchedule.cluster.open-cluster-management.io` exists on the current hub cluster. This verifies that the current hub cluster is not in collision with any other hub cluster when you write backup data to the storage location.
+
For a definition of the `BackupCollision`, see xref:../backup_restore/backup_schedule.adoc#avoid-backup-collision[Avoiding backup collisions].

- *BackupSchedule and restore status validation*
+
* `acm-backup-phase-validation` template checks that the status is not in `Failed`, or `Empty` state, if a `BackupSchedule.cluster.open-cluster-management.io` exists on the current cluster. This ensures that if this cluster is the primary hub cluster and is generating backups, the `BackupSchedule.cluster.open-cluster-management.io` status is healthy.
* The same template checks that the status is not in a `Failed`, or `Empty` state, if a `Restore.cluster.open-cluster-management.io` exists on the current cluster. This ensures that if this cluster is the secondary hub cluster and is restoring backups, the `Restore.cluster.open-cluster-management.io` status is healthy.

- *Backups exist validation*
+
* `acm-managed-clusters-schedule-backups-available` template checks if `Backup.velero.io` resources are available at the location specified by the `BackupStorageLocation.velero.io`, and if the backups are created by a `BackupSchedule.cluster.open-cluster-management.io` resource. This validates that the backups have been run at least once, using the backup and restore operator.

- *Backups for completion*
+
* An `acm-backup-in-progress-report` template checks if `Backup.velero.io` resources are stuck in the `InProgress` state. This validation is added because with a large number of resources, the velero pod restarts as the backup runs, and the backup stays in progress without proceeding to completion. During a normal backup, the backup resources are in progress at some point when it is run, but are not stuck and run to completion. It is normal to see the `acm-backup-in-progress-report` template report a warning during the time the schedule is running and backups are in progress.

- *Backups that actively run as a cron job*
+
* A `BackupSchedule.cluster.open-cluster-management.io` actively runs and saves new backups at the storage location. This validation is done by the `backup-schedule-cron-enabled` policy template. The template checks that there is a `Backup.velero.io` with `velero.io/schedule-name: acm-validation-policy-schedule` label at the storage location.
+
* The `acm-validation-policy-schedule` backups are set to expire after the time is set for the backups cron schedule. If no cron job is running to create backups, the old `acm-validation-policy-schedule` backup is deleted because it expired and a new one is not created. As a result, if no `acm-validation-policy-schedule backups` exist at any moment, it means that there are no active cron jobs generating backups.
+
* This policy is intended to help notify the hub cluster administrator of any backup issues when the hub cluster is active and produces or restore backups.

The `backup-restore-auto-import` policy includes a set of templates that check for the following constraints:

- *Auto import secret validation*
+
* The `auto-import-account-secret` template checks whether a `ManagedServiceAccount` secret is created in the managed cluster namespaces other than the `local-cluster`. The backup controller regularly scans for imported managed clusters. As soon as a managed cluster is discovered, the backup controller creates the `ManagedServiceAccount` resource in the managed cluster namespace. This process initiates token creation on the managed cluster. However, if the managed cluster is not accessible at the time of this operation, the `ManagedServiceAccount` is unable to create the token. For example, if the managed cluster is hibernating, it is unable to create the token. So, if a hub backup is executed during this period, the backup then lacks a token for auto-importing the managed cluster.

- *Auto import backup label validation*
+
* The `auto-import-backup-label` template verifies the existence of a `ManagedServiceAccount` secret in the managed cluster namespaces other than the `local-cluster`. If the template finds the `ManagedServiceAccount` secret, then the template enforces the `cluster.open-cluster-management.io/backup` label on the secret. This label is crucial for including the `ManagedServiceAccount` secrets in {product-title-short} backups.

[#protecting-data-using-server-side-encryption]
== Protecting data using server-side encryption

Server-side encryption is data encryption for the application or service that receives the data at the storage location. The backup mechanism itself does not encrypt data while in-transit (as it travels to and from backup storage location), or at rest (while it is stored on disks at backup storage location). Instead it relies on the native mechanisms in the object and snapshot systems.

**Best practice**: Encrypt the data at the destination using the available backup storage server-side encryption. The backup contains resources, such as credentials and configuration files that need to be encrypted when stored outside of the hub cluster.

You can use `serverSideEncryption` and `kmsKeyId` parameters to enable encryption for the backups stored in Amazon S3. For more details, see the _Backup Storage Location YAML_. The following sample specifies an AWS KMS key ID when setting up the `DataProtectionApplication` resource:

[source,yaml]
----
spec:
  backupLocations:
    - velero:
        config:
          kmsKeyId: 502b409c-4da1-419f-a16e-eif453b3i49f
          profile: default
          region: us-east-1
----

Refer to _Velero supported storage providers_ to find out about all of the configurable parameters of other storage providers.

[#dr4hub-validate-resources]
== Additional resources

- See the link:https://github.com/vmware-tanzu/velero-plugin-for-aws/blob/main/backupstoragelocation.md[Backup Storage Location YAML].

- See link:https://github.com/vmware-tanzu/velero/blob/main/site/content/docs/main/supported-providers.md[Velero supported storage providers].

- Return to <<backup-validation-using-a-policy,Validating your backup or restore configurations>>.
