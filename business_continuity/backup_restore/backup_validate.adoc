[#backup-validation-using-a-policy]
= Validating your backup or restore configurations

The cluster backup and restore operator Helm chart, `cluster-backup-chart`, installs the `backup-restore-enabled` policy on your hub cluster, which is used to inform you about issues with the backup and restore component. 

*Note:* A hub cluster is automatically imported and managed by itself by using the `local-cluster` managed cluster. If you disable this by setting `disableHubSelfManagement=true` on the `MultiClusterHub` resource, the `backup-restore-enabled` policy is not placed on the hub cluster, and the policy templates do not produce any reports.

If a cluster hub is managed by a global hub cluster, or if it is installed on a managed cluster instance, the `disableHubSelfManagement=true` setting is disabled. In this instance, you can enable the `backup-restore-enabled` policy. Enable the policy by setting the `is-hub=true` label on the `ManagedCluster` resource that represents the managed hub cluster. 

The `backup-restore-enabled` policy includes a set of templates that check for the following constraints:

- *OADP channel validation*
+
* When you enable the backup component on the `MultiClusterHub`, the cluster backup and restore operator Helm chart installs the OADP operator. The `OADP-channel` template checks if the installed Red Hat OADP Operator version matches the version set by the {product-title-short} cluster backup and restore operator.
* The template shows violations if it finds an installed Red Hat OADP Operator on the hub cluster, but the Red Hat OADP Operator does not match the version installed by the {product-title-short} cluster backup and restore operator Helm chart. The violation finds and shows a wrong version of the OADP Operator on the cluster. Since the OADP Operator and the Velero Custom Resource Definitions (CRDs) are `cluster-scoped`, you cannot have multiple versions of them installed on the same cluster. Instead, you must only install the right version.  
* In the following examples, the backup and restore operator could run with wrong CRDs resulting in erroneous behavior: 
** {product-title-short} has many installed versions of OADP. 
** If the OADP version installed by `MultiClusterHub` is uninstalled, and you manually install a different version.

- *Pod validation*
+
The following templates check the pod status for the backup component and dependencies:
+
** `acm-backup-pod-running` template checks if the backup and restore operator pod is running.
** `oadp-pod-running` template checks if the OADP operator pod is running. 
** `velero-pod-running` template checks if the Velero pod is running.

- *Data Protection Application validation*
+
* `data-protection-application-available` template checks if a `DataProtectioApplicatio.oadp.openshift.io` resource is created. This OADP resource sets up Velero configurations.

- *Backup storage validation*
+
* `backup-storage-location-available` template checks if a `BackupStorageLocation.velero.io` resource is created and if the status value is `Available`. This implies that the connection to the backup storage is valid. 

- *BackupSchedule collision validation*
+
* `acm-backup-clusters-collision-report` template verifies that the status is not `BackupCollision`, if a `BackupSchedule.cluster.open-cluster-management.io` exists on the current hub cluster. This verifies that the current hub cluster is not in collision with any other hub cluster when you write backup data to the storage location.
+
For a definition of the `BackupCollision`, see xref:../backup_restore/backup_schedule.adoc#avoid-backup-collision[Avoiding backup collisions].

- *BackupSchedule and restore status validation*
+
* `acm-backup-phase-validation` template checks that the status is not in `Failed`, or `Empty` state, if a `BackupSchedule.cluster.open-cluster-management.io` exists on the current cluster. This ensures that if this cluster is the primary hub cluster and is generating backups, the `BackupSchedule.cluster.open-cluster-management.io` status is healthy.
* The same template checks that the status is not in a `Failed`, or `Empty` state, if a `Restore.cluster.open-cluster-management.io` exists on the current cluster. This ensures that if this cluster is the secondary hub cluster and is restoring backups, the `Restore.cluster.open-cluster-management.io` status is healthy.

- *Backups exist validation*
+
* `acm-managed-clusters-schedule-backups-available` template checks if `Backup.velero.io` resources are available at the location specified by the `BackupStorageLocation.velero.io`, and if the backups are created by a `BackupSchedule.cluster.open-cluster-management.io` resource. This validates that the backups have been run at least once, using the backup and restore operator.

- *Backups for completion*
+
* An `acm-backup-in-progress-report` template checks if `Backup.velero.io` resources are stuck in the `InProgress` state. This validation is added because with a large number of resources, the velero pod restarts as the backup runs, and the backup stays in progress without proceeding to completion. During a normal backup, the backup resources are in progress at some point when it is run, but are not stuck and run to completion. It is normal to see the `acm-backup-in-progress-report` template report a warning during the time the schedule is running and backups are in progress.

- *Backups that actively run as a cron job*
+
* A `BackupSchedule.cluster.open-cluster-management.io` actively runs and saves new backups at the storage location. This validation is done by the `backup-schedule-cron-enabled` policy template. The template checks that there is a `Backup.velero.io` with `velero.io/schedule-name: acm-validation-policy-schedule` label at the storage location.
+
The `acm-validation-policy-schedule` backups are set to expire after the time is set for the backups cron schedule. If no cron job is running to create backups, the old `acm-validation-policy-schedule` backup is deleted because it expired and a new one is not created. As a result, if no `acm-validation-policy-schedule backups` exist at any moment, it means that there are no active cron jobs generating backups.
+
This policy is intended to help notify the hub cluster administrator of any backup issues when the hub cluster is active and produces or restore backups.


[#protecting-data-using-server-side-encryption]
== Protecting data using server-side encryption

Server-side encryption is data encryption for the application or service that receives the data at the storage location. The backup mechanism itself does not encrypt data while in-transit (as it travels to and from backup storage location), or at rest (while it is stored on disks at backup storage location). Instead it relies on the native mechanisms in the object and snapshot systems.

**Best practice**: Encrypt the data at the destination using the available backup storage server-side encryption. The backup contains resources, such as credentials and configuration files that need to be encrypted when stored outside of the hub cluster.

You can use `serverSideEncryption` and `kmsKeyId` parameters to enable encryption for the backups stored in Amazon S3. For more details, see the _Backup Storage Location YAML_. The following sample specifies an AWS KMS key ID when setting up the `DataProtectionApplication` resource:

[source,yaml]
----
spec:
  backupLocations:
    - velero:
        config:
          kmsKeyId: 502b409c-4da1-419f-a16e-eif453b3i49f
          profile: default
          region: us-east-1
----

Refer to _Velero supported storage providers_ to find out about all of the configurable parameters of other storage providers.

[#dr4hub-validate-resources]
== Additional resources

- See the link:https://github.com/vmware-tanzu/velero-plugin-for-aws/blob/main/backupstoragelocation.md[Backup Storage Location YAML].

- See link:https://github.com/vmware-tanzu/velero/blob/main/site/content/docs/main/supported-providers.md[Velero supported storage providers].

- Return to <<backup-validation-using-a-policy,Validating your backup or restore configurations>>.
