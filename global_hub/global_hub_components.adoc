[#global-hub-integrating-existing-components]
= Integrating existing components

The {global-hub} requires middleware components, Kafka and PostgreSQL, along with Grafana as the Observability platform to provide the policy compliance view. The {global-hub} provides versions of Kafka, PostgreSQL, and Grafana. You can also integrate your own existing Kafka, PostgreSQL, and Grafana.

* <<integrating-existing-kafka,Integrating an existing version of Kafka>>
* <<integrating-existing-postgresql,Integrating an existing version of PostgreSQL>>
* <<integrating-existing-grafana,Integrating an existing version of Grafana>>

[#integrating-existing-kafka]
== Integrating an existing version of Kafka

If you have your own instance of Kafka, you can use it as the transport for {global-hub}. Kafka 3.3 is the tested version. Complete the following steps to integrate an instance of Kafka: 

. If you do not have a persistent volume for your Kafka instance, you need to create one.

. Create a secret named `multicluster-global-hub-transport` in the `multicluster-global-hub` namespace. 

.. Extract the information in the following required fields: 
+
* `bootstrap.servers`: Specifies the Kafka bootstrap servers.

* `ca.crt`: Required if you use the `KafkaUser` custom resource to configure authentication credentials. See the _User authentication_ topic in the STRIMZI documentation for the required steps to extract the `ca.crt` certificate from the secret.

* `client.crt`: Required, see the _User authentication_ topic in the STRIMZI documentation for the steps to extract the `user.crt` certificate from the secret.

* `client.key`: Required, see the _User authentication_ topic in the STRIMZI documentation for the steps to extract the `user.key` from the secret.

. Create the secret by running the following command, replacing the values with your extracted values where necessary:

+
[source,bash]
----
oc create secret generic multicluster-global-hub-transport -n multicluster-global-hub \
    --from-literal=bootstrap_server=<kafka-bootstrap-server-address> \
    --from-file=ca.crt=<CA-cert-for-kafka-server> \
    --from-file=client.crt=<Client-cert-for-kafka-server> \
    --from-file=client.key=<Client-key-for-kafka-server> 
----

. If automatic topic creation is configured on your Kafka instance, then skip this step. If it is not configured, create the `spec`, `status`, and `event` topics manually. 

. Ensure that the global hub user that accesses Kafka has the permission to read data from the topics and write data to the topics.

[#integrating-existing-postgresql]
== Integrating an existing version of PostgreSQL

If you have your own PostgreSQL relational database, you can use it as the storage for {global-hub}. PostgreSQL 13 is the tested version.

The minimum required storage size is 20GB. This amount can store 3 managed hubs with 250 managed clusters and 50 policies per managed hub for 18 months. You need to create a secret named `multicluster-global-hub-storage` in the `multicluster-global-hub` namespace. The secret must contain the following fields:

* `database_uri`:  It is used to create the database and insert data. Your value must resemble the following format: `postgres://<user>:<password>@<host>:<port>/<database>?sslmode=<mode>`. 
* `database_uri_with_readonlyuser`: It is used to query data by the instance of Grafana that is used by {global-hub}. It is an optional value. Your value must resemble the following format: `postgres://<user>:<password>@<host>:<port>/<database>?sslmode=<mode>`. 
* The `ca.crt`, which is based on the `sslmode`, is an optional value.

. Verify that your cluster has the minimum required storage size of 20GB. This amount can store three managed hubs with 250 managed clusters and 50 policies per managed hub for 18 months.

. Create the secret by running the following command:
----
oc create secret generic multicluster-global-hub-storage -n multicluster-global-hub \
    --from-literal=database_uri=<postgresql-uri> \
    --from-literal=database_uri_with_readonlyuser=<postgresql-uri-with-readonlyuser> \
    --from-file=ca.crt=<CA-for-postgres-server>
----

The host must be accessible from the {global-hub} cluster. If your PostgreSQL database is in a Kubernetes cluster, you can consider using the service type with `nodePort` or `LoadBalancer` to expose the database. For more information, see link:../troubleshooting/trouble_access_postgres.adoc#gh-access-provisioned-postgres-database[Accessing the provisioned postgres database for troubleshooting].

[#integrating-existing-grafana]
== Integrating an existing version of Grafana

Using an existing Grafana instance might work with {global-hub} if you are relying on your own Grafana to get metrics from multiple sources, such as Prometheus, from different clusters and if you aggregate the metrics yourself. To get {global-hub} data into your own Grafana, you need to configure the data source and import the dashboards.

. Collect the PostgreSQL connection information from the {global-hub} Grafana `datasource` secret by running the following command: 

+
----
oc get secret multicluster-global-hub-grafana-datasources -n multicluster-global-hub -ojsonpath='{.data.datasources\.yaml}' | base64 -d
----

+
The output resembles the following example:

+
[source,yaml]
----
apiVersion: 1
datasources:
- access: proxy
  isDefault: true
  name: Global-Hub-DataSource
  type: postgres
  url: postgres-primary.multicluster-global-hub.svc:5432
  database: hoh
  user: guest
  jsonData:
    sslmode: verify-ca
    tlsAuth: true
    tlsAuthWithCACert: true
    tlsConfigurationMethod: file-content
    tlsSkipVerify: true
    queryTimeout: 300s
    timeInterval: 30s
  secureJsonData:
    password: xxxxx
    tlsCACert: xxxxx
----

. Configure the `datasource` in your own Grafana instance by adding a source, such as PostgreSQL, and complete the required fields with the information you previously extracted.

+
See the following required fields:

* Name
* Host
* Database
* User
* Password
* TLS/SSL Mode
* TLS/SSL Method
* CA Cert

. If your Grafana is not in the {global-hub} cluster, you need to expose the PostgreSQL by using the `LoadBalancer` so the PostgreSQL can be accessed from outside. You can add the following value into the `PostgresCluster` operand:

+
[source,yaml]
----
service:
  type: LoadBalancer
----
+
After you add that content, then you can get the `EXTERNAL-IP` from the `postgres-ha` service. See the following example:

+
[source,bash]
----
oc get svc postgres-ha -n multicluster-global-hub
NAME          TYPE           CLUSTER-IP      EXTERNAL-IP                        PORT(S)          AGE
postgres-ha   LoadBalancer   172.30.227.58   xxxx.us-east-1.elb.amazonaws.com   5432:31442/TCP   128m
----

+
After running that command, you can use `xxxx.us-east-1.elb.amazonaws.com:5432` as the PostgreSQL Connection Host.

. Import the existing dashboards. 

.. Follow the steps in link:https://grafana.com/docs/grafana/latest/dashboards/manage-dashboards/#export-and-import-dashboards[Export and import dashboards] in the official Grafana documentation to export the dashboard from the existing Grafana instance.

.. Follow the steps in link:https://grafana.com/docs/grafana/latest/dashboards/manage-dashboards/#export-and-import-dashboards[Export and import dashboards] in the official Grafana documentation to import a dashboard into the {global-hub} Grafana instance. 

[#integrating-existing-additional-resources]
== Additional resources

See link:https://strimzi.io/docs/operators/latest/deploying.html#con-securing-client-authentication-str[User authentication] in the STRIMZI documentation for more information about how to extract the `ca.crt` certificate from the secret.

See link:https://strimzi.io/docs/operators/latest/deploying.html#con-securing-client-authentication-str[User authentication] in the STRIMZI documentation for the steps to extract the `user.crt` certificate from the secret.
