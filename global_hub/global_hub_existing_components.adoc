[#global-hub-integrating-existing-components]
= Integrating existing components

Multicluster global hub requires the middleware components Kafka and PostgreSQL, along with Grafana as the observability platform to provide the policy compliance view. Multicluster global hub provides versions of Kafka, PostgreSQL, and Grafana. You can also integrate your own existing Kafka, PostgreSQL, and Grafana.

* <<integrating-existing-kafka,Integrating an existing verison of Kafka>>
* <<integrating-existing-postgresql,Integrating an existing verison of PostgreSQL>>
* <<integrating-existing-grafana,Integrating an existing verison of Grafana>>

[#integrating-existing-kafka]
== Integrating an existing version of Kafka

If you have your own instance of Kafka, you can use it as the transport for multicluster global hub. Kafka 3.3 is the tested version. Complete the following steps to integrate an instance of Kafka: 

. If you do not have a persistent volume for your Kafka instance, you neeed to create one.

. Create a secret named `multicluster-global-hub-transport` in the `multicluster-global-hub` namespace. 

.. Extract the information in the following reequired fields: 
+
* `bootstrap.servers`: Specifies the Kafka bootstrap servers.

* `ca.crt`: Required if you use the `KafkaUser` custom resource to configure authentication credentials. See the _User authentication_ topic in the STRIMZI documentation for the required steps to extract the `ca.crt` certificate from the secret.

* `client.crt`: Required, see the _User authentication_ topic in the STRIMZI documentation for the steps to extract the `user.crt` certificate from the secret.

* `client.key`: Required, see the _User authentication_ topic in the STRIMZI documentation for the steps to extract the `user.key` from the secret.

. Create the secret by running the following command, replacing the values with your extracted values where necessary:
+
[source,bash]
----
kubectl create secret generic multicluster-global-hub-transport -n multicluster-global-hub \
    --from-literal=bootstrap_server=<kafka-bootstrap-server-address> \
    --from-file=ca.crt=<CA-cert-for-kafka-server> \
    --from-file=client.crt=<Client-cert-for-kafka-server> \
    --from-file=client.key=<Client-key-for-kafka-server> 
----

. If automatic topic creation is configured on your Kafka instance, then skip this step. If it is not configured, create the `spec`, `status`, and `event` topics manually. 

. Ensure that the Kafka user has the permission to read data from the topics and write data to the topics.

[#integrating-existing-postgresql]
== Integrating an existing version of PostgreSQL

If you have your own PostgreSQL relational database, you can use it as the storage for multicluster global hub. PostgreSQL 13 is the tested version.

The minimum required storage size is 20GB. This amount can store 3 managed hubs with 250 managed clusters and 50 policies per managed hub for 18 months. You need to create a secret named `multicluster-global-hub-storage` in the `multicluster-global-hub` namespace. The secret must contain the following fields:

* `database_uri`:  It is used to create the database and insert data. Your value must resemble the following format: `postgres://<user>:<password>@<host>:<port>/<database>?sslmode=<mode>`. 
* `database_uri_with_readonlyuser`: It is used to query data by the instance of Grafana that is used by multicluster global hub. It is an optional value. Your value must resemble the following format: `postgres://<user>:<password>@<host>:<port>/<database>?sslmode=<mode>`. 
* The `ca.crt`, which is based on the `sslmode`, is an optional value.

. Verify that your cluster has the minimum required storage size of 20GB. This amount can store 3 managed hubs with 250 managed clusters and 50 policies per managed hub for 18 months.

. Create the secret by running the following command:
----
kubectl create secret generic multicluster-global-hub-storage -n multicluster-global-hub \
    --from-literal=database_uri=<postgresql-uri> \
    --from-literal=database_uri_with_readonlyuser=<postgresql-uri-with-readonlyuser> \
    --from-file=ca.crt=<CA-for-postgres-server>
----

The host must be accessible from the global hub cluster. If your PostgreSQL database is in a Kubernetes cluster, you can consider using the service type with `nodePort` or `LoadBalancer` to expose the database. For more information, see link:../global_hub/trouble_access_postgres.adoc#gh-access-provisioned-postgres-database[Accessing the provisioned postgres database for troubleshooting].

[#integrating-existing-grafana]
== Integrating an existing version of Grafana

Using an existing Grafana instance might be helpful with multicluster global hub if you have been relying on your own Grafana to get metrics from multiple sources, like Prometheus, from different clusters and have to aggregate the metrics yourself. To get multicluster global hub data into your own Grafana, you need to configure the data source and import the dashboards.

. Collect the PostgreSQL connection information from the multicluster global hub Grafana datasource secret by running the following command: 
+
----
oc get secret multicluster-global-hub-grafana-datasources -n multicluster-global-hub -ojsonpath='{.data.datasources\.yaml}' | base64 -d
----
+
The output resembles the following example:
+
[source,yaml]
----
apiVersion: 1
datasources:
- access: proxy
  isDefault: true
  name: Global-Hub-DataSource
  type: postgres
  url: postgres-primary.multicluster-global-hub.svc:5432
  database: hoh
  user: guest
  jsonData:
    sslmode: verify-ca
    tlsAuth: true
    tlsAuthWithCACert: true
    tlsConfigurationMethod: file-content
    tlsSkipVerify: true
    queryTimeout: 300s
    timeInterval: 30s
  secureJsonData:
    password: xxxxx
    tlsCACert: xxxxx
----

. Configure the datasource in your own Grafana instance by adding a source, such as PostgreSQL, and complete the required fields with the information you previously extracted.
+
The required fields are:

* Name
* Host
* Database
* User
* Password
* TLS/SSL Mode
* TLS/SSL Method
* CA Cert

. If your Grafana is not in the multicluster global hub cluster, you need to expose the PostgreSQL by using the `LoadBalancer` so the PostgreSQL can be accessed from outside. You can add the following value into the `PostgresCluster` operand:
+
[source,yaml]
----
service:
  type: LoadBalancer
----
+
After you add that content, then you can get the `EXTERNAL-IP` from the `postgres-ha` service. For example:
+
[source,bash]
----
oc get svc postgres-ha -n multicluster-global-hub
NAME          TYPE           CLUSTER-IP      EXTERNAL-IP                        PORT(S)          AGE
postgres-ha   LoadBalancer   172.30.227.58   xxxx.us-east-1.elb.amazonaws.com   5432:31442/TCP   128m
----
+
After running that command, you can use `xxxx.us-east-1.elb.amazonaws.com:5432` as the PostgreSQL Connection Host.

. Import the existing dashboards. 

.. Follow the steps in link:https://grafana.com/docs/grafana/latest/dashboards/manage-dashboards/#export-and-import-dashboards[Export and import dashboards] in the official Grafana documentation to export the dashboard from the existing Grafana instance.

.. Follow the steps in link:https://grafana.com/docs/grafana/latest/dashboards/manage-dashboards/#export-and-import-dashboards[Export and import dashboards] in the official Grafana documentation to import a dashboard into the multicluster global hub Grafana instance. 

[#integrating-existing-additional-resources]
== Additional resources

See link:https://strimzi.io/docs/operators/latest/deploying.html#con-securing-client-authentication-str[User authentication] in the STRIMZI documentation for more information about how to extract the `ca.crt` certificate from the secret.

See link:https://strimzi.io/docs/operators/latest/deploying.html#con-securing-client-authentication-str[User authentication] in the STRIMZI documentation for the steps to extract the `user.crt` certificate from the secret.
