[#global-hub-configuring-cronjobs]
= Configuring the cron jobs

You can configure the cron job settings of the {global-hub}. 

After you install the {global-hub} operand, the {global-hub} manager runs and displays a job scheduler for you to schedule the following cron jobs:

* *Local compliance status sync job:* This cron job runs at midnight every day based on the policy status and events collected by the manager on the previous day. Run this job to summarize the compliance status and the change frequency of the policy on the cluster, and store them to the `history.local_compliance` table as the data source of the Grafana dashboards. 

* *Data retention job:* Some data tables in {global-hub} continue to grow over time, which normally can cause problems when the tables get too large. The following two actions help to minimize the issues that result from tables that are too large:

. Delete older data that is no longer needed.

. Enable partitioning on the large table to run queries and deletions on faster.

+
For event tables, such as the `event.local_policies` and the `history.local_compliance` that increase in size daily, range partitioning divides the large tables into smaller partitions. This process also creates the partition tables for the next month each time it is run. 

+
For the policy and cluster tables, such as `local_spec.policies` and `status.managed_clusters`, the `deleted_at` indexes on the tables to improve performance when you delete.

+
You can change the duration of time that the data is retained by changing the `retention` setting on the {global-hub} operand. The recommended minimum value is `1 month`, and the default value is `18 months`. The run interval of this job should be less than one month.

The listed cron jobs run every time the {global-hub} manager starts. The local compliance status sync job is run once a day and can be run multiple times within the day without changing the result. 

The data retention job is run once a week and also can be run many times per month without a change in the results. 

The status of these jobs are are saved in the `multicluster_global_hub_jobs_status` metrics, which can be viewed from the console of the {ocp} cluster. A value of `0` indicates that the job ran successfully, while a value of `1` indicates failure. 
