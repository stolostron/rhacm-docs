[#adv-network-config]
= Advanced network configuration

* <<network-configuration-additional,Additional networking requirements for infrastructure operator table>>
* <<network-configuration-submariner,Submariner networking requirements table>>
* <<network-configuration-hive,Additional networking requirements for Hive table>>
* <<network-configuration-hosted-control-planes,Hosted control planes networking requirements table (Technology Preview)>>
* <<network-configuration-app-deploy,Application deployment network requirements table>>
* <<network-configuration-namespace,Namespace connection network requirements table>>

[#network-configuration-additional]
== Additional networking requirements for infrastructure operator table

When you are installing bare metal managed clusters with the Infrastructure Operator, see the following table for the additional networking requirements:

|===
| Direction | Protocol | Connection | Port (if specified)

| Hub cluster outbound to the ISO/rootfs image repository
| HTTPS (HTTP in a disconnected environment
| Used to create an ISO image on the {product-title-short} hub
| 443 (80 in disconnected environments)

| Hub cluster outbound to BMC interface at single node {ocp-short} managed cluster
| HTTPS (HTTP in disconnected environment)
| Boot the {ocp-short} cluster
| 443

| Outbound from the {ocp-short} managed cluster to the hub cluster
| HTTPS
| Reports hardware information using the `assistedService` route 
| 443

| Outbound from the {ocp-short} managed cluster to the ISO/rootfs image repository
| HTTP
| Downloads the rootfs image
| 80

|===

[#network-configuration-submariner]
== Submariner networking requirements table

Clusters that are using Submariner require three open ports. The following table shows which ports you might use:

|===
| Direction | Protocol | Connection | Port (if specified)

| Outbound and inbound
| UDP
| Each of the managed clusters
| 4800

| Outbound and inbound
| UDP
| Each of the managed clusters
| 4500, 500, and any other ports that are used for IPSec traffic on the gateway nodes

| Inbound
| TCP
| Each of the managed clusters
| 8080

|===

[#network-configuration-hive]
== Additional networking requirements for Hive table

When you are installing bare metal managed clusters with the Hive Operator, which includes using Central Infrastructure Management, you must configure a layer 2 or layer 3 port connection between the hub cluster and the `libvirt` provisioning host. This connection to the provisioning host is required during the creation of a base metal cluster with Hive. See the following table for more information:

|===
| Direction | Protocol | Connection | Port (if specified)

| Hub cluster outbound and inbound to the `libvirt` provisioning host
| IP
| Connects the hub cluster, where the Hive operator is installed, to the `libvirt` provisioning host that serves as a bootstrap when creating the bare metal cluster
| 

|===

**Note:** These requirements only apply when installing, and are not required when upgrading clusters that were installed with Infrastructure Operator.

[#network-configuration-hosted-control-planes]
== (Deprecated) Hosted control planes networking requirements table (Technology Preview)

When you use hosted control planes, the `HypershiftDeployment` resource must have connectivity to the endpoints listed in the following table:

|===
| Direction | Connection | Port (if specified)

| Outbound
| {ocp-short} control-plane and worker nodes
| 

| Outbound
| For hosted clusters on Amazon Web Services only: Outbound connection to AWS API and S3 API
| 

| Outbound
| For hosted clusters on Microsoft Azure cloud services only: Outbound connection to Azure API
| 

| Outbound
| {ocp-short} image repositories that store the ISO images of the coreOS and the image registry for {ocp-short} pods
| 

| Outbound
| Local API client of the klusterlet on the hosting cluster communicates with the API of the HyperShift hosted cluster
| 

|===

[#network-configuration-app-deploy]
== Application deployment network requirements table

In general, the application deployment communication is one way from a managed cluster to the hub cluster. The connection uses `kubeconfig`, which is configured by the agent on the managed cluster. The application deployment on the managed cluster needs to access the following namespaces on the hub cluster:

* The namespace of the channel resource
* The namespace of the managed cluster

[#network-configuration-namespace]
== Namespace connection network requirements table

* Application lifecycle connections:
** The namespace `open-cluster-management` needs to access the console API on port 4000.
** The namespace `open-cluster-management` needs to expose the Application UI on port 3001.

* Application lifecycle backend components (pods):
+
On the hub cluster, all of the application lifecycle pods are installed in the `open-cluster-management` namespace, including the following pods:

** multicluster-operators-hub-subscription
** multicluster-operators-standalone-subscription
** multicluster-operators-channel
** multicluster-operators-application
** multicluster-integrations

+
As a result of these pods being in the `open-cluster-management` namespace:

** The namespace `open-cluster-management` needs to access the Kube API on port 6443.

+
On the managed cluster, only the `klusterlet-addon-appmgr` application lifecycle pod is installed in the `open-cluster-management-agent-addon` namespace:

** The namespace `open-cluster-management-agent-addon` needs to access the Kube API on port 6443.
    
* Governance and risk:
+
On the hub cluster, the following access is required:

** The namespace `open-cluster-management` needs to access the Kube API on port 6443.
** The namespace `open-cluster-management` needs to access the OpenShift DNS on port 5353.

+ 
On the managed cluster, the following access is required:

** The namespace `open-cluster-management-addon` needs to access the Kube API on port 6443.
