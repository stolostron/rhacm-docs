[#upgrading-disconnected-clusters-policies]
= Upgrading disconnected clusters using policies

You can use {cincinnati} with {acm} policies to upgrade multiple clusters in a disconnected environment.

In some cases, security concerns prevent clusters from being connected directly to the internet. This makes it difficult to know when upgrades are available, and how to process those upgrades. Configuring {cincinnati-short} can help. 

{cincinnati-short} is a separate operator and operand that monitors the available versions of your managed clusters in a disconnected environment, and makes them available for upgrading your clusters in a disconnected environment. After {cincinnati-short} is configured, it can perform the following actions:

. Monitor when upgrades are available for your disconnected clusters.
. Identify which updates are mirrored to your local site for upgrading by using the graph data file.
. Notify you that an upgrade is available for your cluster by using the console.

* <<cincinnati-prerequisites,Prerequisites>>
* <<prepare-your-disconnected-mirror-registry,Prepare your disconnected mirror registry>>
* <<deploy-the-operator-for-cincinnati,Deploy the operator for {cincinnati-short}>>
* <<build-the-graph-data-init-container,Build the graph data init container>>
* <<configure-certificate-for-the-mirrored-registry,Configure certificate for the mirrored registry>>
* <<deploy-the-cincinnati-instance,Deploy the {cincinnati-short} instance>>
* <<deploy-a-policy-to-override-the-default-registry,Deploy a policy to override the default registry (optional)>>
* <<deploy-a-policy-to-deploy-a-disconnected-catalog-source,Deploy a policy to deploy a disconnected catalog source>>
* <<deploy-a-policy-to-change-the-managed-cluster-parameter,Deploy a policy to change the managed cluster parameter>>
* <<viewing-available-upgrades,Viewing available upgrades>>
* <<selecting-a-channel-discon,Selecting a channel>>
* <<upgrading-the-cluster,Upgrading the cluster>>

[#cincinnati-prerequisites]
== Prerequisites

You must have the following prerequisites before you can use {cincinnati-short} to upgrade your disconnected clusters:

* A deployed hub cluster that is running on a supported {ocp-short} version with restricted OLM configured. See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/index#olm-restricted-networks[Using Operator Lifecycle Manager on restricted networks] for details about how to configure restricted OLM. 
+
*Tip:* Make a note of the catalog source image when you configure restricted OLM.
* An {ocp-short} cluster that is managed by the hub cluster
* Access credentials to a local repository where you can mirror the cluster images. See link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html/disconnected_installation_mirroring[Disconnected installation mirroring] for more information about how to create this repository.
+
*Note:* The image for the current version of the cluster that you upgrade must always be available as one of the mirrored images. If an upgrade fails, the cluster reverts back to the version of the cluster at the time that the upgrade was attempted.

[#prepare-your-disconnected-mirror-registry]
== Prepare your disconnected mirror registry

You must mirror both the image that you want to upgrade to and the current image that you are upgrading from to your local mirror registry. Complete the following steps to mirror the images:

. Create a script file that contains content that resembles the following example:
+
----
UPSTREAM_REGISTRY=quay.io
PRODUCT_REPO=openshift-release-dev
RELEASE_NAME=ocp-release
OCP_RELEASE=4.14.2-x86_64
LOCAL_REGISTRY=$(hostname):5000
LOCAL_SECRET_JSON=/path/to/pull/secret

oc adm -a ${LOCAL_SECRET_JSON} release mirror \
--from=${UPSTREAM_REGISTRY}/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE} \
--to=${LOCAL_REGISTRY}/ocp4 \
--to-release-image=${LOCAL_REGISTRY}/ocp4/release:${OCP_RELEASE}
----
+
Replace `/path/to/pull/secret` with the path to your {ocp-short} pull secret.

. Run the script to mirror the images, configure settings, and separate the release images from the release content.

*Tip:* You can use the output of the last line of this script when you create your `ImageContentSourcePolicy`.

[#deploy-the-operator-for-cincinnati]
== Deploy the operator for {cincinnati-short}

To deploy the operator for {cincinnati-short} in your {ocp-short} environment, complete the following steps:

. On the hub cluster, access the {ocp-short} operator hub. 
. Deploy the operator by selecting `{cincinnati} Operator`. Update the default values, if necessary. The deployment of the operator creates a new project named `openshift-cincinnati`.
. Wait for the installation of the operator to finish. 
+
*Tip:* You can check the status of the installation by entering the `oc get pods` command on your {ocp-short} command line. Verify that the operator is in the `running` state.

[#build-the-graph-data-init-container]
== Build the graph data init container

{cincinnati-short} uses graph data information to determine the available upgrades. In a connected environment, {cincinnati-short} pulls the graph data information for available upgrades directly from the link:https://github.com/openshift/cincinnati-graph-data[Cincinnati graph data GitHub repository]. Because you are configuring a disconnected environment, you must make the graph data available in a local repository by using an `init container`. Complete the following steps to create a graph data `init container`:

. Clone the _graph data_ Git repository by entering the following command:
+
----
git clone https://github.com/openshift/cincinnati-graph-data
----

. Create a file that contains the information for your graph data `init`. You can find this sample link:https://github.com/openshift/cincinnati-operator/blob/master/dev/Dockerfile[Dockerfile] in the `cincinnati-operator` GitHub repository. The contents of the file is shown in the following sample:
+
----
FROM registry.access.redhat.com/ubi8/ubi:8.1

RUN curl -L -o cincinnati-graph-data.tar.gz https://github.com/openshift/cincinnati-graph-data/archive/master.tar.gz

RUN mkdir -p /var/lib/cincinnati/graph-data/

CMD exec /bin/bash -c "tar xvzf cincinnati-graph-data.tar.gz -C /var/lib/
cincinnati/graph-data/ --strip-components=1"  
----
+
In this example:
+
* The `FROM` value is the external registry where {cincinnati-short} finds the images.

* The `RUN` commands create the directory and package the upgrade files. 

* The `CMD` command copies the package file to the local repository and extracts the files for an upgrade.

. Run the following commands to build the `graph data init container`:
+
----
podman build -f <path_to_Dockerfile> -t ${DISCONNECTED_REGISTRY}/cincinnati/cincinnati-graph-data-container:latest
podman push ${DISCONNECTED_REGISTRY}/cincinnati/cincinnati-graph-data-container:latest --authfile=/path/to/pull_secret.json
----
+
Replace `path_to_Dockerfile` with the path to the file that you created in the previous step.
+
Replace `${DISCONNECTED_REGISTRY}/cincinnati/cincinnati-graph-data-container` with the path to your local graph data init container.
+
Replace `/path/to/pull_secret` with the path to your pull secret file.
+
*Note:* You can also replace `podman` in the commands with `docker`, if you don't have `podman` installed.

[#configure-certificate-for-the-mirrored-registry]
== Configure certificate for the mirrored registry 

If you are using a secure external container registry to store your mirrored {ocp-short} release images, {cincinnati-short} requires access to this registry to build an upgrade graph. Complete the following steps to configure your CA certificate to work with the {cincinnati-short} pod:

. Find the {ocp-short} external registry API, which is located in `image.config.openshift.io`. This is where the external registry CA certificate is stored.  
+
See link:https://docs.openshift.com/container-platform/4.14/registry/configuring-registry-operator.html#images-configuration-cas_configuring-registry-operator[Configuring additional trust stores for image registry access] in the {ocp-short} documentation for more information.

. Create a ConfigMap in the `openshift-config` namespace. 

. Add your CA certificate under the key `updateservice-registry`. {cincinnati-short} uses this setting to locate your certificate:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: trusted-ca
data:
  updateservice-registry: |
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
----

. Edit the `cluster` resource in the `image.config.openshift.io` API to set the `additionalTrustedCA` field to the name of the ConfigMap that you created.
+
----
oc patch image.config.openshift.io cluster -p '{"spec":{"additionalTrustedCA":{"name":"trusted-ca"}}}' --type merge
----
+
Replace `_trusted-ca_` with the path to your new ConfigMap.

The {cincinnati-short} Operator watches the `image.config.openshift.io` API and the
ConfigMap you created in the `openshift-config` namespace for changes, then
restart the deployment if the CA cert has changed.

[#deploy-the-cincinnati-instance]
== Deploy the {cincinnati-short} instance

When you finish deploying the {cincinnati-short} instance on your hub cluster, this instance is located where the images for the cluster upgrades are mirrored and made available to the disconnected managed cluster. Complete the following steps to deploy the instance:

. If you do not want to use the default namespace of the operator, which is `openshift-cincinnati`, create a namespace for your {cincinnati-short} instance:
.. In the {ocp-short} hub cluster console navigation menu, select *Administration* > *Namespaces*.
.. Select *Create Namespace*.
.. Add the name of your namespace, and any other information for your namespace.
.. Select *Create* to create the namespace.
. In the _Installed Operators_ section of the {ocp-short} console, select *{cincinnati} Operator*.
. Select *Create Instance* in the menu.
. Paste the contents from your {cincinnati-short} instance. Your YAML instance might resemble the following manifest:
+
[source,yaml]
----
apiVersion: cincinnati.openshift.io/v1beta2
kind: Cincinnati
metadata:
  name: openshift-update-service-instance
  namespace: openshift-cincinnati
spec:
  registry: <registry_host_name>:<port> <1>
  replicas: 1
  repository: ${LOCAL_REGISTRY}/ocp4/release
  graphDataImage: '<host_name>:<port>/cincinnati-graph-data-container' <2>
----
+
<1> Replace the `spec.registry` value with the path to your local disconnected registry for your images.
+
<2> Replace the `spec.graphDataImage` value with the path to your graph data init container. This is the same value that you used when you ran the `podman push` command to push your graph data init container.
. Select *Create* to create the instance. 
. From the hub cluster CLI, enter the `oc get pods` command to view the status of the instance creation. It might take a while, but the process is complete when the result of the command shows that the instance and the operator are running.

[#deploy-a-policy-to-override-the-default-registry]
== Deploy a policy to override the default registry (optional)

*Note:* The steps in this section only apply if you have mirrored your releases into your mirrored registry. *Deprecated:* `PlacementRule`

{ocp-short} has a default image registry value that specifies where it finds the upgrade packages. In a disconnected environment, you can create a policy to replace that value with the path to your local image registry where you mirrored your release images. 

For these steps, the policy is named _policy-mirror_. Complete the following steps to create the policy:

. Log in to the {ocp-short} environment of your hub cluster.

. From the console, select *Governance* > *Create policy*.

. Set the *YAML* switch to _On_ to view the YAML version of the policy.

. Delete all of the content in the YAML code. 

. Paste the following YAML content into the window to create a custom policy:
+
[source,yaml]
----
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-mirror
  namespace: default
spec:
  disabled: false
  remediationAction: enforce
  policy-templates:
    - objectDefinition: 
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: policy-image-content-source-policy
        spec:
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: operator.openshift.io/v1alpha1
                kind: ImageContentSourcePolicy
                metadata:
                  name: <your-local-mirror-name>
                spec:
                  repositoryDigestMirrors:
                    - mirrors:
                        - <your-registry> <1>
                      source: registry.redhat.io
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: binding-policy-mirror
  namespace: default
placementRef:
  name: placement-policy-mirror
  kind: PlacementRule
  apiGroup: apps.open-cluster-management.io
subjects:
- name: policy-mirror
  kind: Policy
  apiGroup: policy.open-cluster-management.io
---
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: placement-policy-mirror
  namespace: default
spec:
  clusterConditions:
  - status: "True"
    type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      []  # selects all clusters if not specified
----
+
<1> Replace `your-registry` with the path to your local mirror repository. You can find your path to your local mirror by entering the `oc adm release mirror` command.

. Select *Enforce if supported*.
. Select *Create* to create the policy. 

[#deploy-a-policy-to-deploy-a-disconnected-catalog-source]
== Deploy a policy to deploy a disconnected catalog source

Push the _Catalogsource_ policy to the managed cluster to change the default location from a connected location to your disconnected local registry. 

. In the console menu, select *Governance* > *Create policy*.

. Set the `YAML` switch to _On_ to view the YAML version of the policy.

. Delete all of the content in the `YAML` code. 

. Paste the following `YAML` content into the window to create a custom policy:
+
[source,yaml]
----
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-catalog
  namespace: default
spec:
  disabled: false
  remediationAction: enforce
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: policy-catalog
        spec:
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: config.openshift.io/v1
                kind: OperatorHub
                metadata:
                  name: cluster
                spec:
                  disableAllDefaultSources: true
            - complianceType: musthave
              objectDefinition:
                apiVersion: operators.coreos.com/v1alpha1
                kind: CatalogSource
                metadata:
                  name: my-operator-catalog
                  namespace: openshift-marketplace
                spec:
                  sourceType: grpc
                  image: '<registry_host_name>:<port>/olm/redhat-operators:v1' <1>
                  displayName: My Operator Catalog
                  publisher: grpc
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: binding-policy-catalog
  namespace: default
placementRef:
  name: placement-policy-catalog
  kind: PlacementRule
  apiGroup: apps.open-cluster-management.io
subjects:
- name: policy-catalog
  kind: Policy
  apiGroup: policy.open-cluster-management.io
---
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: placement-policy-catalog
  namespace: default
spec:
  clusterConditions:
  - status: "True"
    type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      []  # selects all clusters if not specified
----
+
<1> Replace the value of `spec.image` with the path to your local restricted catalog source image.

. Select *Enforce if supported*.

. Select *Create* to create the policy.

[#deploy-a-policy-to-change-the-managed-cluster-parameter]
== Deploy a policy to change the managed cluster parameter

Push the _ClusterVersion_ policy to the managed cluster to change the default location where it retrieves its upgrades. 

. From the managed cluster, confirm that the _ClusterVersion_ upstream parameter is currently the default public {cincinnati-short} operand by entering the following command:
+
----
oc get clusterversion -o yaml
----
+
The returned content might resemble the following content:
+
[source,yaml]
----
apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: ClusterVersion
[..]
  spec:
    channel: stable-4.4
    upstream: https://api.openshift.com/api/upgrades_info/v1/graph
----

. From the hub cluster, identify the route URL to the {cincinnati-short} operand by entering the following command: `oc get routes`. Note this value for later steps.

. In the hub cluster console menu, select *Governance* > *Create a policy*.
. Set the `YAML` switch to _On_ to view the YAML version of the policy.
. Delete all of the content in the `YAML` code. 
. Paste the following `YAML` content into the window to create a custom policy:
+
[source,yaml]
----
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-cluster-version
  namespace: default
  annotations:
    policy.open-cluster-management.io/standards: null
    policy.open-cluster-management.io/categories: null
    policy.open-cluster-management.io/controls: null
spec:
  disabled: false
  remediationAction: enforce
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: policy-cluster-version
        spec:
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: config.openshift.io/v1
                kind: ClusterVersion
                metadata:
                  name: version
                spec:
                  channel: stable-4.4
                  upstream: >-
                    https://example-cincinnati-policy-engine-uri/api/upgrades_info/v1/graph <1>

---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: binding-policy-cluster-version
  namespace: default
placementRef:
  name: placement-policy-cluster-version
  kind: PlacementRule
  apiGroup: apps.open-cluster-management.io
subjects:
- name: policy-cluster-version
  kind: Policy
  apiGroup: policy.open-cluster-management.io
---
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: placement-policy-cluster-version
  namespace: default
spec:
  clusterConditions:
  - status: "True"
    type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      []  # selects all clusters if not specified 
----
+
<1> Replace the value of `objectDefinition.spec.upstream` with the path to your hub cluster {cincinnati-short} operand. 
+
You can complete the following steps to determine the path to the operand:
+
.. Run the `oc get get routes -A` command on the hub cluster.
.. Find the route to `cincinnati`.
 +
 The path to the operand is the value in the `HOST/PORT` field.

. Select *Enforce if supported*. 

. Select *Create* to create the policy.

. In the managed cluster CLI, confirm that the upstream parameter in the `ClusterVersion` is updated with the local hub cluster {cincinnati-short} URL by entering: 
+
----
oc get clusterversion -o yaml
----
+
Verify that the results resemble the following content:
+
[source,yaml]
----
apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: ClusterVersion
[..]
  spec:
    channel: stable-4.4
    upstream: https://<hub-cincinnati-uri>/api/upgrades_info/v1/graph
----

[#viewing-available-upgrades]
== Viewing available upgrades

You can view a list of available upgrades for your managed cluster by completing the following steps:

. Log in to your {mce} console.
. In the navigation menu, select *Infrastructure* > *Clusters*.
. Select a cluster that is in the _Ready_ state.
. From the *Actions* menu, select *Upgrade cluster*. 
. Verify that the optional upgrade paths are available. 
+
*Note:* No available upgrade versions are shown if the current version is not mirrored into the local image repository.  

[#selecting-a-channel-discon]
== Selecting a channel

You can use the {acm-short} console to select a channel for your cluster upgrades on {ocp-short} version 4.6 or later. Those versions must be available on the mirror registry. Complete the steps in link:../clusters/install_upgrade/upgrade_cluster.adoc#selecting-a-channel[Selecting a channel] to specify a channel for your upgrades. 

[#upgrading-the-cluster]
== Upgrading the cluster

After configuring the disconnected registry, {acm-short} and {cincinnati-short} use the disconnected registry to determine if upgrades are available. If no available upgrades are displayed, make sure that you have the release image of the current level of the cluster and at least one later level mirrored in the local repository. If the release image for the current version of the cluster is not available, no upgrades are available.

Complete the following steps to upgrade:

. In the console, select *Infrastructure* > *Clusters*.

. Find the cluster that you want to determine if there is an available upgrade. 

. If there is an upgrade available, the *Distribution version* column for the cluster indicates that there is an upgrade available. 

. Select the _Options_ menu for the cluster, and select *Upgrade cluster*.

. Select the target version for the upgrade, and select *Upgrade*. 

The managed cluster is updated to the selected version. 

If your cluster upgrade fails, the Operator generally retries the upgrade a few times, stops, and reports the status of the failing component. In some cases, the upgrade process  continues to cycle through attempts to complete the process. Rolling your cluster back to a previous version following a failed upgrade is not supported. Contact Red Hat support for assistance if your cluster upgrade fails.
