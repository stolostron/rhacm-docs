[#performance-and-scalability]
= Performance and scalability

{product-title} is tested to determine certain scalability and performance data. The major areas that are tested are cluster scalability and search performance. You can use this information as you plan your environment.

*Note:* Data is based on the results from a lab environment at the time of testing.

{product-title-short} is tested by using a three node hub cluster on bare metal machines. At testing, there is a sufficient amount of resource capacity (CPU, memory, and disk) to find software component limits. Your results might vary, depending on your environment, network speed, and changes to the product.

* <<maximum-number-of-managed-clusters,Maximum number of managed clusters>>
* <<search-scalability,Search scalability>>
* <<scaling-for-observability,Observability scalability>>
* <<backup-restore-scalability,Backup and restore scalability>>

[#maximum-number-of-managed-clusters]
== Maximum number of managed clusters

The maximum number of clusters that {product-title-short} can manage varies based on several factors, including:

* Number of resources in the cluster, which depends on factors like the number of policies and applications that are deployed.
* Configuration of the hub cluster, such as how many pods are used for scaling.

The managed clusters are {sno} virtual machines hosted on Red Hat Enterprise Linux hypervisors. Virtual machines are used to achieve high-density counts of clusters per single bare metal machine in the testbed. Sushy-emulator is used with libvirt for virtual machines to have an accessible bare metal cluster by using Redfish APIs. The following operators are a part of the test installation, Topology Aware Lifecycle Manager, Local Storage Operator, and Red Hat OpenShift GitOps. The following table shows the lab environment scaling information:

.Table for environment scaling
|===
| Node | Count | Operating system | Hardware | CPU cores | Memory | Disks  

| Hub cluster control plane
| 3
| {ocp-short}
| Bare metal
| 112
| 512 GiB
| 446 GB SSD, 2.9 TB NVMe, 2 x 1.8 TB SSD

| Managed cluster 
| 3500
| {sno}
| Virtual machine
| 8
| 18 GiB
| 120 GB
|===

[#search-scalability]
== Search scalability

The scalability of the Search component depends on the performance of the data store. The query run time is an important variable when analyzing the search performance.

[#query-execution-considerations]
=== Query run time considerations

There are some things that can affect the time that it takes to run and return results from a query.
Consider the following items when planning and configuring your environment:

* Searching for a keyword is not efficient.
+
If you search for `RedHat` and you manage a large number of clusters, it might take a longer time to receive search results.

* The first search takes longer than later searches because it takes additional time to gather user role-based access control rules.
* The length of time to complete a request is proportional to the number of namespaces and resources the user is authorized to access.
+
*Note:* If you save and share a Search query with another user, returned results depend on access level for that user.
For more information on role access, see link:https://docs.openshift.com/container-platform/4.12/authentication/using-rbac.html[Using RBAC to define and apply permissions] in the {ocp-short} documentation.

* The worst performance is observed for a request by a non-administrator user with access to all of the namespaces, or all of the managed clusters.

[#scaling-for-observability]
== Observability scalability

You need to plan your environment if you want to enable and use the observability service. The resource consumption later is for the {ocp-short} project, where observability components are installed. Values that you plan to use are sums for all observability components.

*Note:* Data is based on the results from a lab environment at the time of testing. Your results might vary, depending on your environment, network speed, and changes to the product.

[#sample-observability-environment]
=== Sample observability environment

In the sample environment, hub clusters and managed clusters are located in Amazon Web Services cloud platform and have the following topology and configuration:

|===
| Node | Flavor | vCPU | RAM (GiB) | Disk type | Disk size (GiB) | Count | Region

| Master node
| m5.4xlarge
| 16
| 64 
| gp2
| 100 
| 3
| sa-east-1

| Worker node
| m5.4xlarge
| 16
| 64 
| gp2
| 100
| 3
| sa-east-1
|===

The observability deployment is configured for high availability environments. With a high availability environment, each Kubernetes deployment has two instances, and each StatefulSet has three instances.

During the sample test, a different number of managed clusters are simulated to push metrics, and each test lasts for 24 hours. See the following throughput:

[#write-throughput]
=== Write throughput 

|===
| Pods| Interval (minute)| Time series per min

| 400
| 1
| 83000
|===

[#cpu-usage]
=== CPU usage (millicores)

CPU usage is stable during testing:

|===
| Size | CPU Usage 

| 10 clusters 
| 400
| 20 clusters 
| 800
|===

[#RSS-memory]
=== RSS and working set memory

View the following descriptions of the RSS and working set memory:

- *Memory usage RSS:* From the metrics `container_memory_rss` and remains stable during the test. 

- *Memory usage working set:* From the metrics `container_memory_working_set_bytes`, increases along with the test. 

The following results are from a 24-hour test:

|===
| Size| Memory usage RSS| Memory usage working set

| 10 clusters
| 9.84 
| 4.93

| 20 clusters
| 13.10
| 8.76
|===

[#persistent-volume-thanos]
=== Persistent volume for `thanos-receive` component

*Important:* Metrics are stored in `thanos-receive` until retention time (four days) is reached. Other components do not require as much volume as `thanos-receive` components.
 
Disk usage increases along with the test. Data represents disk usage after one day, so the final disk usage is multiplied by four. 

See the following disk usage:

|===
| Size| Disk usage (GiB)

| 10 clusters
| 2

| 20 clusters
| 3
|===


[#network-transfer]
=== Network transfer

During tests, network transfer provides stability. See the sizes and network transfer values:

|===
|Size | Inbound network transfer | Outbound network transfer

| 10 clusters
| 6.55 MBs per second
| 5.80 MBs per second

| 20 clusters
| 13.08 MBs per second
| 10.9 MBs per second
|===

[#s3-storage]
=== Amazon Simple Storage Service (S3)

Total usage in Amazon Simple Storage Service (S3) increases. The metrics data is stored in S3 until default retention time (five days) is reached. See the following disk usages:

|===
| Size| Disk usage (GiB)

| 10 clusters
| 16.2

| 20 clusters
| 23.8
|===

[#backup-restore-scalability]
== Backup and restore scalability

The tests performed on large scaled environment show the following data for backup and restore:

.Table of run times for managed cluster backups
|===
| Backups | Duration | Number of resources| Backup memory

| credentials
| 2m5s
| 18272 resources
| 55MiB backups size

| managed clusters
| 3m22s
| 58655 resources
| 38MiB backups size

| resources
| 1m34s
| 1190 resources
| 1.7MiB backups size

| generic/user
| 2m56s
| 0 resources
| 16.5KiB backups size
|===

The total backup time is `10m`.

.Table of run time for restoring passive hub cluster
|===
| Backups | Duration | Number of resources 

| redentials
| 47m8s
| 18272 resources

| resources
| 3m10s 
| 1190 resources

| generic/user backup
| 0m 
| 0 resources
|===

Total restore time is `50m18s`.

The number of backup file are pruned using the `veleroTtl` parameter option set when the `BackupSchedule` is created. Any backups with a creation time older than the specified TTL (time to live) are expired and automatically deleted from the storage location by Velero.

[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: BackupSchedule
 metadata:
 name:schedule-acm
 namespace:open-cluster-management-backup
spec:
veleroSchedule:0 */1 * * *
veleroTtl:120h
----