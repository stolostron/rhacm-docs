[#installing-while-connected-online]
= Installing while connected online

{product-title} is installed using an operator that deploys all of the required components.

[#connect_prerequisites]
== Prerequisites

You must meet the following requirements before you install {product-title-short}:

* Your {ocp} must have access to the {product-title-short} operator in the OperatorHub catalog.
* {ocp-short} version 4.3, or later, must be deployed in your environment, and you must be logged into it with the CLI.
See the https://docs.openshift.com/container-platform/4.3/welcome/index.html[OpenShift version 4.3 documentation] or https://docs.openshift.com/container-platform/4.4/welcome/index.html[{ocp-short} version 4.4 documentation].
* Your {ocp-short} command line interface (CLI) must be version 4.3, or later, and configured to run `oc` commands.
See https://docs.openshift.com/container-platform/4.3/cli_reference/openshift_cli/getting-started-cli.html[Getting started with the CLI] for information about installing and configuring the {ocp-short} CLI.
* Your {ocp-short} permissions must allow you to create a namespace.
* You must have an Internet connection to access the dependencies for the operator.

[#installing-red-hat-advanced-cluster-management-from-the-cli]
== Installing Red Hat Advanced Cluster Management from the CLI

. Create a hub cluster namespace where the operator requirements are contained:
+
----
oc create namespace <namespace>
----
+
Replace _namespace_ with a name for your hub cluster namespace. **Note:** The value for _namespace_ might be referred to as _Project_ in the {ocp} environment. 
+
*Important:* The {product-title-short} operator must be installed in its own namespace.
A `ServiceAccount` with a `ClusterRoleBinding` automatically gives cluster administrator privileges to {product-title-short} and to any ID with access to the namespace.
For security, make sure that anyone who is given access to this namespace already has cluster-administrator access.

. Switch your project namespace to the one that you created:
+
----
oc project <namespace>
----
+
Replace _namespace_ with the name of the hub cluster namespace that you created in step 1.

. If you plan to import Kubernetes clusters that were not created by {ocp-short} or {product-title-short}, generate a secret that contains your {ocp-short} pull secret information to access the entitled content from the distribution registry.
The secret requirements for {ocp-short} clusters are automatically resolved by {ocp-short} and {product-title-short}, so you do not have to create the secret if you are not importing other types of Kubernetes clusters to be managed.
*Important:* These secrets are namespace-specific, so make sure that you are in the namespace that you created in step 1.
 .. Download your {ocp-short} pull secret file from https://cloud.redhat.com/openshift/install/pull-secret[cloud.redhat.com/openshift/install/pull-secret] by selecting *Download pull secret*.
Your {ocp-short} pull secret is associated with your Red Hat Customer Portal ID, and is the same across all Kubernetes providers.
 .. Run the following command to create your secret:
+
----
oc create secret generic <secret> -n <namespace> --from-file=.dockerconfigjson=<path-to-pull-secret> --type=kubernetes.io/dockerconfigjson
----
+
Replace _secret_ with the name of the secret that you want to create.
Replace _namespace_ with your project namespace.
Replace _path-to-pull-secret_ with the path to your {ocp-short} pull secret that you downloaded.

. Create an operator group.
Each namespace can have only one operator group.
 .. Create a `.yaml` file that defines the operator group.
Your file should look similar to the following example:
+
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: <default>
spec:
  targetNamespaces:
  - <namespace>
----
+
Replace _default_ with the name of your operator group.
Replace _namespace_ with the name of your project namespace.

 .. Apply the file that you created to define the operator group:
+
----
oc apply -f local/<operator-group>.yaml
----
+
Replace _operator-group_ with the name of the operator group `.yaml` file that you created.

. Apply the subscription.
 .. Create a `.yaml` file that defines the subscription.
Your file should look similar to the following example:
+
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: acm-operator-subscription
spec:
  sourceNamespace: openshift-marketplace
  source: redhat-operators
  channel: release-2.0
  installPlanApproval: Automatic
  name: advanced-cluster-management
----

 .. Apply the subscription:
+
----
oc apply -f local/<subscription>.yaml
----
+
Replace _subscription_ with the name of the subscription file that you created.
. Create the MultiClusterHub custom resource by creating a `.yaml` file that defines the custom resource.
Your file should look similar to the following example:
+
----
apiVersion: operator.open-cluster-management.io/v1
kind: MultiClusterHub
metadata:
  name: multiclusterhub
  namespace: <namespace>
spec:
  imagePullSecret: <secret>
  disableHubSelfManagement: false
----
+
Replace _namespace_ with your project namespace. 
Replace _secret_ with the name of the secret that you created.
+
*Important:* The `local-cluster` namespace is used for the imported self-managed hub cluster. You must not have a local-cluster namespace on you cluster prior to installing. After the `local-cluster` namespace is created for the hub cluster, anyone who has access to the `local-cluster` namespace is automatically granted _cluster administrator_ access. Ensure that anyone who is granted access to that namespace is intended to have the _cluster administrator_ access. 
+
If this step fails with the following error, the resources are still being created and applied:
+
----
error: unable to recognize "./mch.yaml": no matches for kind "MultiClusterHub" in version "operator.open-cluster-management.io/v1"
----
+
Run the command again in a few minutes when the resources are created.
. *Optional:* Disable hub self management, if necessary. By default, the hub cluster is automatically imported and managed by itself, like any other cluster. If you do not want the hub cluster to manage itself, then change the setting for `disableHubSelfManagement` from `false` to `true`. If the setting is not included in the `.yaml` file that defines the custom resource, add it as shown in the example of the previous step.
. It can take up to 10 minutes for the `MultiClusterHub` custom resource status to show as `Running` in the _status.phase_ field after you run the following command:
+
----
oc get mch -o=jsonpath='{.items[0].status.phase}'
----

. After the status is `Running`, view the list of routes to find your route:
+
----
oc get routes
----

If you are reinstalling {product-title-short} and the pods do not start, see link:../troubleshooting/trouble_reinstall.adoc#troubleshooting-reinstallation-failure[Troubleshooting reinstallation failure] for steps to work around this problem. 

[#installing-red-hat-advanced-cluster-management-from-the-console]
== Installing {product-title-short} from the console

. Create a hub cluster namespace for the operator requirements:
 .. In the {ocp-short} console navigation, select *Administration* > *Namespaces*.
 .. Select *Create Namespace*.
 .. Provide a name for your namespace.
This is the namespace that you use throughout the installation process. *Note:* The value for _namespace_ might be referred to as _Project_ in the {ocp} environment.
 .. Select *Create*.
+
*Important:* The Red Hat Advanced Cluster Management operator must be installed in its own namespace.
A `ServiceAccount` with a `ClusterRoleBinding` automatically gives cluster administrator privileges to {product-title-short} and to any ID with access to the namespace.
For security, make sure that anyone who is given access to this namespace already has cluster-administrator access.
. Switch your project namespace to the one that you created in step 1.
This ensures that the steps are completed in the correct namespace.
Some resources are namespace-specific.
 .. In the {ocp-short} console navigation, select *Administration* > *Namespaces*.
 .. Select the namespace that you created in step 1.
. If you plan to import Kubernetes clusters that were not created by {ocp-short} or {product-title-short}, creater a secret that contains your {ocp-short} pull secret to access the entitled content from the distribution registry. Secret requirements for {ocp-short} clusters are automatically resolved by {ocp-short} and {product-title-short}, so you do not have to create the secret if you are not importing other types of Kubernetes clusters to be managed.
 .. Copy your {ocp-short} pull secret from https://cloud.redhat.com/openshift/install/pull-secret[cloud.redhat.com/openshift/install/pull-secret] by selecting *Copy pull secret*.
You will use the content of this pull secret in a step later in this procedure.
Your {ocp-short} pull secret is associated with your Red Hat Customer Portal ID, and is the same across all Kubernetes providers.
 .. In the {ocp-short} console navigation, select *Workloads* > *Secrets*.
 .. Select *Create* > *Image Pull Secret*.
 .. Enter a name for your secret.
 .. Select *Upload Configuration File* as the authentication type.
 .. In the _Configuration file_ field, paste the pull secret that you copied from `cloud.redhat.com`.
 .. Select *Create* to create the secret.
. Subscribe to the operator. **Note:** The value for _namespace_ might be referred to as _Project_ in the {ocp-short} environment.
 .. In the {ocp-short} console navigation, select *Operators* > *OperatorHub*.
 .. Select *{product-title-short}*.
*Tip:* You can filter on the _Integration & Delivery_ category to narrow the choices.
 .. Select *Install*.
 .. Update the values, if necessary.
 .. Select *Subscribe*.
. Create the _MultiClusterHub_ custom resource.
 .. In the {ocp-short} console navigation, select *Installed Operators* > *Advanced Cluster Management for Kubernetes*.
 .. Select the *MultiClusterHub* tab.
 .. Select *Create MultiClusterHub*.
 .. Update the default values in the `.yaml` file, according to your needs.
The following example shows some sample data:
+
----
apiVersion: operator.open-cluster-management.io/v1
kind: MultiClusterHub
metadata:
  name: multiclusterhub
  namespace: <namespace>
spec:
  imagePullSecret: <secret>
  disableHubSelfManagement: false
----
+
Replace _secret_ with the name of the pull secret that you created.
Confirm that the _namespace_ is your project namespace.
+
*Important:* The `local-cluster` namespace is used for the imported self-managed hub cluster. You must not have a local-cluster namespace on you cluster prior to installing. After the `local-cluster` namespace is created for the hub cluster, anyone who has access to the `local-cluster` namespace is automatically granted _cluster administrator_ access. Ensure that anyone who is granted access to that namespace is intended to have the _cluster administrator_ access. 
 .. *Optional:* Disable hub self management, if necessary. By default, the hub cluster is automatically imported and managed by itself, like any other cluster. If you do not want the hub cluster to manage itself, then change the setting for `disableHubSelfManagement` from `false` to `true`. If the setting is not included in the `.yaml` file that defines the custom resource, add it as shown in the example of the previous step.
. Select *Create* to initialize the custom resource.
It can take up to 10 minutes for the hub to build and start.
+
After the hub is created, the status for the operator is _Running_ on the _Installed Operators_ page.

. Access the console for the hub.
 .. In the {ocp-short} console navigation, select *Networking* > *Routes*.
 .. View the URL for your hub in the list, and navigate to it to access the console for your hub.

If you are reinstalling {product-title-short} and the pods do not start, see link:../troubleshooting/trouble_reinstall.adoc#troubleshooting-reinstallation-failure[Troubleshooting reinstallation failure] for steps to work around this problem.
