[#sizing-your-cluster]
= Sizing your hub cluster

Each {product-title} cluster is unique and the following guidelines provide sample deployment sizes for you. Recommendations are classified by size and purpose. {product-title-short} applies the following dimensions for sizing and placement of supporting services:

* Availability Zones isolate potential fault domains across the cluster. Typical clusters should have nearly equivalent worker node capacity in three or more availability zones. 

* vCPU reservations and limits, which establish vCPU capacity on a worker node to assign to a container. A vCPU is equivalent to a Kubernetes compute unit. For more information, see Kubernetes link:https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu[Meaning of CPU].

* Memory reservations and limits, which establish memory capacity on a worker node to assign to a container. 
* Persistent data, which is managed by the product and stored in the etcd cluster that is used by Kubernetes. 

*Best practice:* For {ocp-short}, distribute the master nodes of the cluster across three (3) availability zones.

[#product-environment]
== Product environment

**Note:** The following requirements are _not_ minimum requirements.

.Product environment
|===
| Node type | Availability zones | etcd | Total reserved memory | Total reserved CPU

| Master
| 3
| 3
| Per {ocp-short} sizing guidelines
| Per {ocp-short} sizing guidelines

| Worker
| 3
| 1
| 12 GB
| 6
|===

In addition to {product-title-short}, the {ocp-short} cluster runs additional services to support cluster features. The recommended node size is three nodes.

//is the second statement in line 37 neccessary since we have the Best practice notice o line 13? 

[#openshift-cluster-on-additional-services]
=== {ocp-short} on additional services

.Additional services
|===
| Service | Node count | Availability zones | Instance size | vCPU | Memory | Storage size | Resources

| {ocp-short} on Amazon Web Services
| 3
| 3
| m5.xlarge
| 4
| 16 GB
| 120 GB
| See the https://docs.openshift.com/container-platform/4.10/installing/installing_aws/installing-aws-customizations.html#installing-aws-customizations[Amazon Web Services information in the {ocp-short} product documentation] for more information.

Also learn more about https://aws.amazon.com/ec2/instance-types/m5/[machine types].

| {ocp-short} on Google Cloud Platform
| 3
| 3
| N1-standard-4 (0.95–6.5 GB)
| 4
| 15 GB
| 120 GB
| See the https://cloud.google.com/docs/quota[Google Cloud Platform product documentation] for more information about quotas.

Also learn more about https://cloud.google.com/compute/docs/machine-types[machine types].

| {ocp-short} on Microsoft Azure
| 3
| 3
| Standard_D4_v3
| 4
| 16 GB
| 120 GB
| See the following https://docs.openshift.com/container-platform/4.10/installing/installing_azure/installing-azure-account.html[product documentation] for more details.

| {ocp-short} on VMware vSphere
| 3
| 3
| 
| 4 (2 cores per socket)
| 16 GB
| 120 GB
| See the following https://docs.openshift.com/container-platform/4.6/installing/installing_vsphere/installing-vsphere-installer-provisioned.html[product documentation] for more details.


| {ocp-short} on  IBM Z systems
| 3
| 3	
|
| 10
| 16 GB 
| 100 GB
| See https://access.redhat.com/documentation/en-us/openshift_container_platform/4.10/html-single/installing/index#installing-ibm-z[Installing a cluster on IBM Z systems] in the {ocp-short} documentation for more information.

IBM Z systems provide the ability to configure simultaneous multithreading (SMT), which extends the number of vCPUs that can run on each core. If you configured SMT, One physical core (IFL) provides two logical cores (threads). The hypervisor can provide two or more vCPUs.

One vCPU is equivalent to one physical core when simultaneous multithreading (SMT), or hyperthreading, is not enabled. When enabled, use the following formula to calculate the corresponding ratio: (threads per core × cores) × sockets = vCPUs.

For more information about SMT, see https://www.ibm.com/docs/en/aix/7.2?topic=concepts-simultaneous-multithreading[Simultaneous multithreading].

| {ocp-short} on IBM Power systems
| 3 
| 3								
|
| 16
| 16 GB
| 120 GB
| See https://access.redhat.com/documentation/en-us/openshift_container_platform/4.10/html-single/installing/index#installing-on-ibm-power-systems[Installing a cluster on Power systems] in the {ocp-short} documentation for more information.

IBM Power systems provide the ability to configure simultaneous multithreading (SMT), which extends the number of vCPUs that can run on each core. If you configured SMT, your SMT level determines how you satisfy the 16 vCPU requirement. The most common configurations are:

Two cores running on SMT-8 (the default configuration for systems that are running IBM PowerVM) provides the required 16 vCPUs.

Four cores running on SMT-4 provides the required 16 vCPUs. 

For more information about SMT, see https://www.ibm.com/docs/en/aix/7.2?topic=concepts-simultaneous-multithreading[Simultaneous multithreading].

| {ocp-short} on Bare metal assets
| 3
|
|
| 4
| 16 GB
| 120 GB
| See the following https://docs.openshift.com/container-platform/4.10/installing/installing_bare_metal/installing-restricted-networks-bare-metal.html#installation-three-node-cluster_installing-restricted-networks-bare-metal[product documentation] for more details.

A {product-title} hub cluster can be installed and supported on {ocp-short} bare metal. The hub cluster can run on a compact bare metal topology, in which there are 3 schedulable control plane nodes, and 0 additional workers.
|===

[#single-node]
=== Creating and managing single node {ocp-short} clusters

See example requirements for creating and managing 2200 single node OpenShift Container Platform clusters. See the minimum requirements for using {product-title-short} to create single node OpenShift (SNO) clusters (230 and more provisioned at the same time), and manage those SNO clusters with a hub cluster:

.Master (schedulable)
|===
| Node count | Memory (peak cluster usage) | Memory (single node max) | CPU cluster max | CPU single node max

| 3
| 289 GB
| 110 GB
| 90 
| 44 
|===

*Note:* The CPU utilization values peaked while multiple clusters were created at the same time.

