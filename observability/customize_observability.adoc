[#customizing-observability]
= Customizing observability configuration

After you enable observability, customize the observability configuration to the specific needs of your environment. Manage and view cluster fleet data that the observability service collects.

*Required access:* Cluster administrator

- <<creating-custom-rules,Creating custom rules>>
- <<adding-custom-metrics, Adding custom metrics>>
- <<adding-advanced-config,Adding _advanced_ configuration for retention>>
- <<updating-replicas,Updating the _MultiClusterObservability_ custom resource replicas from the console>>
- <<increase-decrease-pv-pvc,Increasing and decreasing persistent volumes and persistent volume claims>>
- <<customizing-route-cert,Customizing route certification>>
- <<customizing-certificates-object-store,Customizing certificates for accessing the object store>>
- <<configuring-proxy-settings-for-observability-add-ons,Configuring proxy settings for observability add-ons>>
- <<disabling-proxy-settings-for-observability-add-ons,Disabling proxy settings for observability add-ons>>


[#creating-custom-rules]
== Creating custom rules

Create custom rules for the observability installation by adding Prometheus link:https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/[recording rules] and link:https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[alerting rules] to the observability resource.

To precalculate expensive expressions, use the recording rules abilities with Prometheus to create alert conditions and send notifications based on how you want to send an alert to an external service. The results are saved as a new set of time series. View the following examples to create a custom alert rule within the `observability-thanos-rule-custom-rules` config map:

- To get a notification for when your CPU usage paases your defined value, create the following custom alert rule:

+
[source,yaml]
----
data:
  custom_rules.yaml: |
    groups:
      - name: cluster-health
        rules:
        - alert: ClusterCPUHealth-jb
          annotations:
            summary: Notify when CPU utilization on a cluster is greater than the defined utilization limit
            description: "The cluster has a high CPU usage: {{ $value }} core for {{ $labels.cluster }} {{ $labels.clusterID }}."
          expr: |
            max(cluster:cpu_usage_cores:sum) by (clusterID, cluster, prometheus) > 0
          for: 5s
          labels:
            cluster: "{{ $labels.cluster }}"
            prometheus: "{{ $labels.prometheus }}"
            severity: critical
----

+
*Notes:*

* When you update your custom rules, `observability-thanos-rule` pods restart automatically.
* You can create multiple rules in the configuration. 
* The default alert rules are in the `observability-thanos-rule-default-rules` config map of the `open-cluster-management-observability` namespace.

+
- To create a custom recording rule to get the sum of the container memory cache of a pod, create the following custom rule:

+
[source,yaml]
----
data:
  custom_rules.yaml: |
    groups:
      - name: container-memory
        rules:
        - record: pod:container_memory_cache:sum
          expr: sum(container_memory_cache{pod!=""}) BY (pod, container)
----

+
*Note:* After you make changes to the config map, the configuration automatically reloads. The configuration reloads because of the `config-reload` within the `observability-thanos-rule` sidecar.

To verify that the alert rules are functioning correctly, go to the Grafana dashboard, select the *Explore* page, and query `ALERTS`. The alert is only available in Grafana if you created the alert. 

[#adding-custom-metrics]
== Adding custom metrics

Add metrics to the `metrics_list.yaml` file to collect from managed clusters. Complete the following steps:

. Before you add a custom metric, verify that `mco observability` is enabled with the following command: 

+
[source,bash]
----
oc get mco observability -o yaml
----

. Check for the following message in the `status.conditions.message` section reads:

+
[source,bash]
----
Observability components are deployed and running
----

. Create the `observability-metrics-custom-allowlist` config map in the `open-cluster-management-observability` namespace with the following command:

+
[source,bash]
----
oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml
----

. Add the name of the custom metric to the `metrics_list.yaml` parameter. Your YAML for the config map might resemble the following content:

+
[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-custom-allowlist
data:
  metrics_list.yaml: |
    names: <1>
      - node_memory_MemTotal_bytes
    rules: <2>
    - record: apiserver_request_duration_seconds:histogram_quantile_90
      expr: histogram_quantile(0.90,sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",
        verb!=\"WATCH\"}[5m])) by (verb,le))
----
+
<1> *Optional:* Add the name of the custom metrics that are to be collected from the managed cluster.
<2> *Optional:* Enter only one value for the `expr` and `record` parameter pair to define the query expression. The metrics are collected as the name that is defined in the `record` parameter from your managed cluster. The metric value returned are the results after you run the query expression.
+
You can use either one or both of the sections. For user workload metrics, see the _Adding user workload metrics_ section.
+
*Note:* You can also individually customize each managed cluster in the custom metrics allowlist instead of applying it across your entire fleet. You can create the same YAML directly on your managed cluster to customize it.

. Verify the data collection from your custom metric by querying the metric from the Grafana dashboard *Explore* page. You can also use the custom metrics in your own dashboard.

[#adding-user-workload-metrics]
=== Adding user workload metrics

Collect {ocp-short} user-defined metrics from workloads in {ocp-short} to display the metrics from your Grafana dashboard. Complete the following steps:

. Enable monitoring on your {ocp-short} cluster. See _Enabling monitoring for user-defined projects_ in the _Additional resources_ section.
+
If you have a managed cluster with monitoring for user-defined workloads enabled, the user workloads are located in the `test` namespace and generate metrics. These metrics are collected by Prometheus from the {ocp-short} user workload.

. Add user workload metrics to the `observability-metrics-custom-allowlist` config map to collect the metrics in the `test` namespace. View the following example:

+
[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-custom-allowlist
  namespace: test
data:
  uwl_metrics_list.yaml: <1>
    names: <2>
      - sample_metrics
----
+
<1> Enter the key for the config map data.
<2> Enter the value of the config map data in YAML format. The `names` section includes the list of metric names, which you want to collect from the `test` namespace. After you create the config map, the observability collector collects and pushes the metrics from the target namespace to the hub cluster.

[#removing-default-metrics]
=== Removing default metrics

If you do not want to collect data for a specific metric from your managed cluster, remove the metric from the `observability-metrics-custom-allowlist.yaml` file. When you remove a metric, the metric data is not collected from your managed clusters. Complete the following steps to remove a default metric:

. Verify that `mco observability` is enabled by using the following command: 

+
[source,bash]
----
oc get mco observability -o yaml
----

. Add the name of the default metric to the `metrics_list.yaml` parameter with a hyphen `-` at the start of the metric name. View the following metric example:

+
[source,bash]
----
-cluster_infrastructure_provider
----

. Create the `observability-metrics-custom-allowlist` config map in the 
`open-cluster-management-observability` namespace with the following command: 

+
[source,bash]
----
oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml
----

. Verify that the observability service is not collecting the specific metric from your managed clusters. When you query the metric from the Grafana dashboard, the metric is not displayed.

[#adding-advanced-config]
== Adding advanced configuration for retention

To update the retention for each observability component according to your need, add the `advanced` configuration section. Complete the following steps: 

. Edit the `MultiClusterObservability` custom resource with the following command:

+
[source,bash]
----
oc edit mco observability -o yaml
----

. Add the `advanced` section to the file. Your YAML file might resemble the following contents:

+
[source,yaml]
----
spec:
  advanced:
    retentionConfig:
      blockDuration: 2h
      deleteDelay: 48h
      retentionInLocal: 24h
      retentionResolutionRaw: 365d
      retentionResolution5m: 365d
      retentionResolution1h: 365d
    receive:
      resources:
        limits:
          memory: 4096Gi
      replicas: 3 
----

+
*Notes:*

- For descriptions of all the parameters that can added into the `advanced` configuration, see the _Observability API_ documentation.

- The default retention for all resolution levels, such as `retentionResolutionRaw`, `retentionResolution5m`, or `retentionResolution1h`, is 365 days (`365d`). You must set an explicit value for the resolution retention in your `MultiClusterObservability` `spec.advanced.retentionConfig` parameter.

. If you upgraded from an earlier version and want to keep that version retention configuration, add the configuration previously mentioned. Complete the following steps: 

.. Go to your `MultiClusterObservability` resource by running the following command: 

+
[source,bash]
----
edit mco observability
----

.. In the `spec.advanced.retentionConfig` parameter, apply the following configuration: 

+
[source,bash]
----
spec:
  advanced:
    retentionConfig:
      retentionResolutionRaw: 365d
      retentionResolution5m: 365d
      retentionResolution1h: 365d
----

[#dynamic-metrics-for-sno]
== Dynamic metrics for {sno} clusters

Dynamic metrics collection supports automatic metric collection based on certain conditions. By default, a {sno} cluster does not collect pod and container resource metrics. Once a {sno} cluster reaches a specific level of resource consumption, the defined granular metrics are collected dynamically. When the cluster resource consumption is consistently less than the threshold for a period of time, granular metric collection stops.

The metrics are collected dynamically based on the conditions on the managed cluster specified by a collection rule. Because these metrics are collected dynamically, the following {acm-short} Grafana dashboards do not display any data. When a collection rule is activated and the corresponding metrics are collected, the following panels display data for the duration of the time that the collection rule is initiated:

* Kubernetes/Compute Resources/Namespace (Pods)
* Kubernetes/Compute Resources/Namespace (Workloads)
* Kubernetes/Compute Resources/Nodes (Pods)
* Kubernetes/Compute Resources/Pod
* Kubernetes/Compute Resources/Workload
A collection rule includes the following conditions:
* A set of metrics to collect dynamically.
* Conditions written as a PromQL expression.
* A time interval for the collection, which must be set to `true`.
* A match expression to select clusters where the collect rule must be evaluated.

By default, collection rules are evaluated continuously on managed clusters every 30 seconds, or at a specific time interval. The lowest value between the collection interval and time interval takes precedence. Once the collection rule condition persists for the duration specified by the `for` attribute, the collection rule starts and the metrics specified by the rule are automatically collected on the managed cluster. Metrics collection stops automatically after the collection rule condition no longer exists on the managed cluster, at least 15 minutes after it starts.

The collection rules are grouped together as a parameter section named `collect_rules`, where it can be enabled or disabled as a group. {acm-short} installation includes the collection rule group, `SNOResourceUsage` with two default collection rules: `HighCPUUsage` and `HighMemoryUsage`. The `HighCPUUsage` collection rule begins when the node CPU usage exceeds 70%. The `HighMemoryUsage` collection rule begins if the overall memory utilization of the {sno} cluster exceeds 70% of the available node memory. Currently, the previously mentioned thresholds are fixed and cannot be changed. When a collection rule begins for more than the interval specified by the `for` attribute, the system automatically starts collecting the metrics that are specified in the `dynamic_metrics` section.

View the list of dynamic metrics that from the `collect_rules` section, in the following YAML file:

[source,yaml]
----
collect_rules:
  - group: SNOResourceUsage
    annotations:
      description: >
        By default, a {sno} cluster does not collect pod and container resource metrics. Once a {sno} cluster 
        reaches a level of resource consumption, these granular metrics are collected dynamically. 
        When the cluster resource consumption is consistently less than the threshold for a period of time, 
        collection of the granular metrics stops.
    selector:
      matchExpressions:
        - key: clusterType
          operator: In
          values: ["{sno}"]
    rules:
    - collect: SNOHighCPUUsage
      annotations:
        description: >
          Collects the dynamic metrics specified if the cluster cpu usage is constantly more than 70% for 2 minutes
      expr: (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))) * 100 > 70
      for: 2m
      dynamic_metrics:
        names:
          - container_cpu_cfs_periods_total
          - container_cpu_cfs_throttled_periods_total
          - kube_pod_container_resource_limits 
          - kube_pod_container_resource_requests   
          - namespace_workload_pod:kube_pod_owner:relabel 
          - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate 
          - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate 
    - collect: SNOHighMemoryUsage
      annotations:
        description: >
          Collects the dynamic metrics specified if the cluster memory usage is constantly more than 70% for 2 minutes
      expr: (1 - sum(:node_memory_MemAvailable_bytes:sum) / sum(kube_node_status_allocatable{resource=\"memory\"})) * 100 > 70
      for: 2m
      dynamic_metrics:
        names:
          - kube_pod_container_resource_limits 
          - kube_pod_container_resource_requests 
          - namespace_workload_pod:kube_pod_owner:relabel
        matches:
          - __name__="container_memory_cache",container!=""
          - __name__="container_memory_rss",container!=""
          - __name__="container_memory_swap",container!=""
          - __name__="container_memory_working_set_bytes",container!=""
----

A `collect_rules.group` can be disabled in the `custom-allowlist` as shown in the following example. When a `collect_rules.group` is disabled, metrics collection reverts to the previous behavior. These metrics are collected at regularly, specified intervals:

[source,yaml]
----
collect_rules:
  - group: -SNOResourceUsage
----

The data is only displayed in Grafana when the rule is initiated.

[#updating-replicas]
== Updating the _MultiClusterObservability_ custom resource replicas from the console

If your workload increases, increase the number of replicas of your observability pods. Navigate to the {ocp} console from your hub cluster. Locate the `MultiClusterObservability` custom resource, and update the `replicas` parameter value for the component where you want to change the replicas. Your updated YAML might resemble the following content:

[source,yaml]
----
spec:
   advanced:
      receive:
         replicas: 6
----

For more information about the parameters within the `mco observability` custom resource, see the _Observability API_ documentation.

[#increase-decrease-pv-pvc]
== Increasing and decreasing persistent volumes and persistent volume claims

Increase and decrease the persistent volume and persistent volume claims to change the amount of storage in your storage class. Complete the following steps:

. To increase the size of the persistent volume, update the `MultiClusterObservability` custom resource if the storage class support expanding volumes.        

. To decrease the size of the persistent volumes remove the pods using the persistent volumes, delete the persistent volume and recreate them. You might experience data loss in the persistent volume. Complete the following steps:
+
.. Pause the `MultiClusterObservability` operator by adding the annotation `mco-pause: "true"` to the `MultiClusterObservability` custom resource.

.. Look for the stateful sets or deployments of the desired component. Change their replica count to `0`. This initiates a shutdown, which involves uploading local data when applicable to avoid data loss. For example, the Thanos `Receive` stateful set is named `observability-thanos-receive-default` and has three replicas by default. Therefore, you are looking for the following persistent volume claims:
+
- `data-observability-thanos-receive-default-0`
- `data-observability-thanos-receive-default-1`
- `data-observability-thanos-receive-default-2`

.. Delete the persistent volumes and persistent volume claims used by the desired component. 
.. In the `MultiClusterObservability` custom resource, edit the storage size in the configuration of the component to the desired amount in the storage size field. Prefix with the name of the component.

.. Unpause the `MultiClusterObservability` operator by removing the previously added annotation.

.. To initiate a reconcilation after having the operator paused, delete the `multicluster-observability-operator` and `observatorium-operator` pods. The pods are recreated and reconciled immediately.

. Verify that persistent volume and volume claims are updated by checking the `MultiClusterObservability` custom resource.

[#customizing-route-cert]
== Customizing route certificate

If you want to customize the {ocp-short} route certification, you must add the routes in the `alt_names` section. To ensure your {ocp-short} routes are accessible, add the following information: `alertmanager.apps.<domainname>`, `observatorium-api.apps.<domainname>`, `rbac-query-proxy.apps.<domainname>`.

For more details, see _Replacing certificates for alertmanager route_ in the Governance documentation.

*Note:* Users are responsible for certificate rotations and updates.

[#customizing-certificates-object-store]
== Customizing certificates for accessing the object store

You can configure secure connections with the observability object store by creating a `Secret` resource that contains the certificate authority and configuring the `MultiClusterObservability` custom resource. Complete the following steps:

. To validate the object store connection, create the `Secret` object in the file that contains the certificate authority by using the following command:

+
[source,bash]
----
oc create secret generic <tls_secret_name> --from-file=ca.crt=<path_to_file> -n open-cluster-management-observability
----

.. Alternatively, you can apply the following YAML to create the secret:

+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: <tls_secret_name>
  namespace: open-cluster-management-observability
type: Opaque
data:
  ca.crt: <base64_encoded_ca_certificate>
----
*Optional:* If you want to enable mutual TLS, you need to add the `public.crt` and `private.key` keys in the previous secret.

. Add the TLS secret details to the `metricObjectStorage` section by using the following command:

+
[source,bash]
----
oc edit mco observability -o yaml
----

+
Your file might resemble the following YAML:

+
[source,yaml]
----
metricObjectStorage:
  key: thanos.yaml
  name: thanos-object-storage
  tlsSecretName: tls-certs-secret <1>
  tlsSecretMountPath: /etc/minio/certs <2>
----
<1> The value for `tlsSecretName` is the name of the `Secret` object that you previously created.
<2> The `/etc/minio/certs/` path defined for the `tlsSecretMountPath` parameter specifies where the certificates are mounted in the Observability components. This path is required for the next step.

. Update the `thanos.yaml` definition in the `thanos-object-storage` secret by adding the `http_config.tls_config` section with the certificate details. View the following example:

+
[source,yaml]
----
thanos.yaml: |
   type: s3
   config:
     bucket: "thanos"
     endpoint: "minio:9000"
     insecure: false <1>
     access_key: "minio"
     secret_key: "minio123"
     http_config:
       tls_config:
         ca_file: /etc/minio/certs/ca.crt <2>
         insecure_skip_verify: false
----
<1> Set the `insecure` parameter to `false` to enable HTTPS.
<2> The path for the `ca_file` parameter must match the `tlsSecretMountPath` from the `MultiClusterObservability` custom resource. The `ca.crt` must match the key in the `<tls_secret_name>` `Secret` resource.
+
*Optional:* If you want to enable mutual TLS, you need to add the `cert_file` and `key_file` keys to the `tls_config` section. See the following example:

+
[source,yaml]
----
 thanos.yaml: |
    type: s3
    config:
      bucket: "thanos"
      endpoint: "minio:9000"
      insecure: false
      access_key: "minio"
      secret_key: "minio123"
      http_config:
        tls_config:
          ca_file: /etc/minio/certs/ca.crt <1>
          cert_file: /etc/minio/certs/public.crt
          key_file: /etc/minio/certs/private.key
          insecure_skip_verify: false
----
<1> The path for `ca_file`, `cert_file`, and `key_file` must match the `tlsSecretMountPath` from the `MultiClusterObservability` custom resource. The `ca.crt`, `public.crt`, and `private.crt` must match the respective key in the `tls_secret_name>` `Secret` resource.

. To verify that you can access the object store, check that the pods are deployed. Run the following command:

+
[source,bash]
----
oc -n open-cluster-management-observability get pods -l app.kubernetes.io/name=thanos-store
----

[#configuring-proxy-settings-for-observability-add-ons]
== Configuring proxy settings for observability add-ons

Configure the proxy settings to allow the communications from the managed cluster to access the hub cluster through a HTTP and HTTPS proxy server. Typically, add-ons do not need any special configuration to support HTTP and HTTPS proxy servers between a hub cluster and a managed cluster. But if you enabled the observability add-on, you must complete the proxy configuration. 

=== Prerequisite 

- You have a hub cluster. 
- You have enabled the proxy settings between the hub cluster and managed cluster. 

Complete the following steps to configure the proxy settings for the observability add-on:

. Go to the cluster namespace on your hub cluster. 
. Create an `AddOnDeploymentConfig` resource with the proxy settings by adding a `spec.proxyConfig` parameter. View the following YAML example:

+
[source,yaml]
----
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: AddOnDeploymentConfig
metadata:
  name: <addon-deploy-config-name>
  namespace: <managed-cluster-name>
spec:
  agentInstallNamespace: open-cluster-managment-addon-observability
  proxyConfig:
    httpsProxy: "http://<username>:<password>@<ip>:<port>" <1>
    noProxy: ".cluster.local,.svc,172.30.0.1" <2> 
----
+
<1> For this field, you can specify either a HTTP proxy or a HTTPS proxy.
<2> Include the IP address of the `kube-apiserver`. 

. To get the IP address, run following command on your managed cluster: 

+
[source,bash]
----
oc -n default describe svc kubernetes | grep IP:
----
 
. Go to the `ManagedClusterAddOn` resource and update it by referencing the `AddOnDeploymentConfig` resource that you made. View the following YAML example:

+
[source,yaml]
----
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ManagedClusterAddOn
metadata:
  name: observability-controller
  namespace: <managed-cluster-name>
spec:
  installNamespace: open-cluster-managment-addon-observability
  configs:
  - group: addon.open-cluster-management.io
    resource: AddonDeploymentConfig
    name: <addon-deploy-config-name>
    namespace: <managed-cluster-name> 
----

. Verify the proxy settings. If you successfully configured the proxy settings, the metric collector deployed by the observability add-on agent on the managed cluster sends the data to the hub cluster. Complete the following steps:

.. Go to the hub cluster then the managed cluster on the Grafana dashboard. 
.. View the metrics for the proxy settings. 

[#disabling-proxy-settings-for-observability-add-ons]
== Disabling proxy settings for observability add-ons

If your development needs change, you might need to disable the proxy setting for the observability add-ons you configured for the hub cluster and managed cluster. You can disable the proxy settings for the observability add-on at any time. Complete the following steps:

. Go to the `ManagedClusterAddOn` resource.
. Remove the referenced `AddOnDeploymentConfig` resource.

[#custom-obervatorium-alert-url]
== Customizing the managed cluster Observatorium API and Alertmanager URLs (Technology Preview)

You can customize the Observatorium API and Alertmanager URLs that the managed cluster uses to communicate with the hub cluster to maintain all {acm-short} functions when you use a load balancer or reserve proxy. To customize the URLs, complete the following steps:

. Add your URLs to the `advanced` section of the `MultiClusterObservability` `spec`. See the following example:

+
[source,yaml]
----
spec:
  advanced:
    customObservabilityHubURL: <yourURL>
    customAlertmanagerHubURL: <yourURL>
----
+

*Notes:*

* Only HTTPS URLs are supported. If you do not add `https://` to your URL, the scheme is added automatically.
* You can include the standard path for the Remote Write API, `/api/metrics/v1/default/api/v1/receive` in the `customObservabilityHubURL` `spec`. If you do not include the path, the Observability service automatically adds the path at runtime.
* Any intermediate component you use for the custom Observability hub cluster URL cannot use TLS termination because the component relies on MTLS authentication. The custom Alertmanager hub cluster URL supports intermediate component TLS termination by using the existing bring your own certificate instructions.

. If you are using a `customObservabilityHubURL`, create a route object by using the following template. Replace `<intermediate_component_url>` with the intermediate component URL:

+
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: proxy-observatorium-api
  namespace: open-cluster-management-observability
spec:
  host: <intermediate_component_url>
  port:
    targetPort: public
  tls:
    insecureEdgeTerminationPolicy: None
    termination: passthrough
  to:
    kind: Service
    name: observability-observatorium-api
    weight: 100
  wildcardPolicy: None
----

. If you are using a `customAlertmanagerHubURL`, create a route object by using the following template. Replace `<intermediate_component_url>` with the intermediate component URL:

+
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: alertmanager-proxy
  namespace: open-cluster-management-observability
spec:
  host: <intermediate_component_url>
  path: /api/v2
  port:
    targetPort: oauth-proxy
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: reencrypt
  to:
    kind: Service
    name: alertmanager
    weight: 100
  wildcardPolicy: None
----

[#configure-fine-grain-rbac]
== Configuring fine-grain RBAC (Technology Preview)

To restrict metric access to specific namespaces within the cluster, use fine-grain role-based access control (RBAC). Using fine-grain RBAC, you can allow application teams to only view the metrics for the namespaces that you give them permission to access. 

You must configure metric access control on the hub cluster for the users of that hub cluster. On this hub cluster, a `ManagedCluster` custom resource represents every managed cluster. To configure RBAC and to select the allowed namespaces, use the rules and action verbs specified in the `ManagedCluster` custom resources. 

For example, you have an application named, `my-awesome-app`, and this application is on two different managed clusters, `devcluster1` and `devcluster2`. Both clusters are in the `AwesomeAppNS` namespace. You have an `admin` user group named, `my-awesome-app-admins`, and you want to restrict this user group to only have access to metrics from only these two namespaces on the hub cluster. 

In this example, to use fine-grain RBAC to restrict the user group access, complete the following steps: 

. Define a `ClusterRole` resource with permissions to access metrics. Your resource might resemble the following YAML: 

+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 name: awesome-app-metrics-role
rules:
 - apiGroups:
     - "cluster.open-cluster-management.io"
   resources:
     - managedclusters: <1>
   resourceNames: <2>
     - devcluster1
     - devcluster2
   verbs: <3> 
     - metrics/AwesomeAppNS
----
<1> Represents the parameter values for the managed clusters. 
<2> Represents the list of managed clusters.
<3> Represents the namespace of the managed clusters.

. Define a `ClusterRoleBinding` resource that binds the group, `my-awesome-app-admins`, with the `ClusterRole` resource for the `awesome-app-metrics-role`. Your resource might resemble the following YAML: 

+
[source,yaml]
----
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: awesome-app-metrics-role-binding
subjects:
 - kind: Group
   apiGroup: rbac.authorization.k8s.io
   name: my-awesome-app-admins
roleRef:
 apiGroup: rbac.authorization.k8s.io
 kind: ClusterRole
 name: awesome-app-metrics-role
----

After completing these steps, when the users in the `my-awesome-app-admins` log into the Grafana console, they have the following restrictions: 

* Users see no data for dashboards that summarize fleet level data. 
* Users can only select managed clusters and namespaces specified in the `ClusterRole` resource.   

To set up different types of user access, define separate `ClusterRoles` and `ClusterRoleBindings` resources to represent the different managed clusters in the namespaces. 

[#additional-resource-custom-obs]
== Additional resources

- Refer to link:https://prometheus.io/docs/prometheus/latest/configuration/configuration/[Prometheus configuration] for more information. For more information about recording rules and alerting rules, refer to the recording rules and alerting rules from the link:https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/[Prometheus documentation]. 

- For more information about viewing the dashboard, see xref:../observability/design_grafana.adoc#using-grafana-dashboards[Using Grafana dashboards].

- See xref:../observability/use_observability.adoc#exporting-metrics-to-external-endpoints[Exporting metrics to external endpoints]. 

- See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/monitoring/enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects].

- See the link:../apis/observability.json.adoc#observability-api[Observability API].

- For information about updating the certificate for the alertmanager route, see link:../governance/#replacing-cert-alertmanager[Replacing certificates for alertmanager].

- For more details about observability alerts, see xref:../observability/observability_alerts.adoc#observability-alerts[Observability alerts]

- To learn more about alert forwarding, see the link:https://prometheus.io/docs/alerting/latest/alertmanager/[Prometheus Alertmanager documentation].

- See xref:../observability/observe_environments_intro.adoc#observability-alerts[Observability alerts] for more information.

- For more topics about the observability service, see xref:../observability/observe_environments_intro.adoc#observing-environments-intro[Observability service].

- See link:https://github.com/openshift/enhancements/blob/master/enhancements/workload-partitioning/management-workload-partitioning.md#management-workload-partitioning[Management Workload Partitioning] for more information.


