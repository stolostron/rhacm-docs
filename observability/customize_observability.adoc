[#customizing-observability]
= Customizing observability

Review the following sections to learn more about customizing, managing, and viewing data that is collected by the observability service.

Collect logs about new information that is created for observability resources with the `must-gather` command. For more information, see the _Must-gather_ section in the link:../troubleshooting/troubleshooting_intro.adoc[Troubleshooting documentation].

* <<creating-custom-rules,Creating custom rules>>
* <<configuring-alertmanager,Configuring AlertManager>>
* <<adding-custom-metrics, Adding custom metrics>>
* <<removing-default-metrics,Removing default metrics>>
* <<adding-advanced-config,Adding _advanced_ configuration>>
* <<updating-replicas,Updating the _multiclusterobservability_ CR replicas from the console>>
* <<forward-alerts,Forwarding alerts>>
* <<customizing-route-cert,Customizing route certification>>
* <<customizing-certificates-object-store,Customizing certificates for accessing the object store>>
* <<viewing-and-exploring-data,Viewing and exploring data>>
** <<viewing-etcd-grafana,Viewing the etcd table>>
** <<viewing-cluster-fleet-service-level-overview-on-k8s-api-server-grafana,Viewing the cluster fleet service-level overview for the Kubernetes API server dashboard>>
** <<viewing-cluster-service-level-overview-on-k8s-api-server-grafana,Viewing the cluster service-level overview for the Kubernetes API server dashboard>>
* <<disable-observability,Disable observability>>

[#creating-custom-rules]
== Creating custom rules

Create custom rules for the observability installation by adding Prometheus https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/[recording rules] and https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[alerting rules] to the observability resource. For more information, see https://prometheus.io/docs/prometheus/latest/configuration/configuration/[Prometheus configuration].

** Recording rules provide you the ability to precalculate, or computate expensive expressions as needed. The results are saved as a new set of time series.
** Alerting rules provide you the ability to specify the alert conditions based on how an alert should be sent to an external service.

Define custom rules with Prometheus to create alert conditions, and send notifications to an external messaging service. *Note*: When you update your custom rules, `observability-thanos-rule` pods are restarted automatically.

Create a ConfigMap named `thanos-ruler-custom-rules` in the `open-cluster-management-observability` namespace. The key must be named, `custom_rules.yaml`, as shown in the following example. You can create multiple rules in the configuration.

* By default, the out-of-the-box alert rules are defined in the `thanos-ruler-default-rules` ConfigMap in the `open-cluster-management-observability` namespace. 
+
For example, you can create a custom alert rule that notifies you when your CPU usage passes your defined value. Your YAML might resemble the following content: 
+
[source,yaml]
----
data:
  custom_rules.yaml: |
    groups:
      - name: cluster-health
        rules:
        - alert: ClusterCPUHealth-jb
          annotations:
            summary: Notify when CPU utilization on a cluster is greater than the defined utilization limit
            description: "The cluster has a high CPU usage: {{ $value }} core for {{ $labels.cluster }} {{ $labels.clusterID }}."
          expr: |
            max(cluster:cpu_usage_cores:sum) by (clusterID, cluster, prometheus) > 0
          for: 5s
          labels:
            cluster: "{{ $labels.cluster }}"
            prometheus: "{{ $labels.prometheus }}"
            severity: critical
----

* You can also create a custom recording rule within the `thanos-ruler-custom-rules` ConfigMap.
+
For example, you can create a recording rule that provides you the ability to get the sum of the container memory cache of a pod. Your YAML might resemble the following content:
+
[source,yaml]
----
data:
  custom_rules.yaml: |
    groups:
      - name: container-memory
        rules:
        - record: pod:container_memory_cache:sum
          expr: sum(container_memory_cache{pod!=""}) BY (pod, container)
----
+
*Note*: If this is the first new custom rule, it is created immediately. For changes to the ConfigMap, the configuration is automatically reloaded. The configuration is reloaded due to the `config-reload` within the `observability-thanos-ruler` sidecar.

To verify that the alert rules are functioning appropriately, launch the Grafana dashboard, navigate to the *Explore* page, and query `ALERTS`. The alert is only available in Grafana if the alert is initiated.

[#configuring-alertmanager]
== Configuring AlertManager

Integrate external messaging tools such as email, Slack, and PagerDuty to receive notifications from AlertManager. You must override the `alertmanager-config` secret in the `open-cluster-management-observability` namespace to add integrations, and configure routes for AlertManager. Complete the following steps to update the custom receiver rules:

. Extract the data from the `alertmanager-config` secret. Run the following command:
+
----
oc -n open-cluster-management-observability get secret alertmanager-config --template='{{ index .data "alertmanager.yaml" }}' |base64 -d > alertmanager.yaml
----

. Edit and save the `alertmanager.yaml` file configuration by running the following command:
+
----
oc -n open-cluster-management-observability create secret generic alertmanager-config --from-file=alertmanager.yaml --dry-run -o=yaml |  oc -n open-cluster-management-observability replace secret --filename=-
----
+
Your updated secret might resemble the following content:
+
[source,yaml]
----
global
  smtp_smarthost: 'localhost:25'
  smtp_from: 'alertmanager@example.org'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: 'password'
templates: 
- '/etc/alertmanager/template/*.tmpl'
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 3h 
  receiver: team-X-mails
  routes:
  - match_re:
      service: ^(foo1|foo2|baz)$
    receiver: team-X-mails
----

Your changes are applied immediately after it is modified. For an example of AlertManager, see https://github.com/prometheus/alertmanager/blob/master/doc/examples/simple.yml[prometheus/alertmanager].

[#adding-custom-metrics]
== Adding custom metrics

Add metrics to the `metrics_list.yaml` file, to be collected from managed clusters.

Before you add a custom metric, verify that `mco observability` is enabled with the following command: `oc get mco observability -o yaml`. Check for the following message in the `status.conditions.message` reads: `Observability components are deployed and running`.

Create a file named `observability-metrics-custom-allowlist.yaml` and add the name of the custom metric to the `metrics_list.yaml` parameter. Your YAML for the ConfigMap might resemble the following content:

[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-custom-allowlist
data:
  metrics_list.yaml: |
    names:
      - node_memory_MemTotal_bytes
    rules:
    - record: apiserver_request_duration_seconds:histogram_quantile_90
      expr: histogram_quantile(0.90,sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",
        verb!=\"WATCH\"}[5m])) by (verb,le))
----

** In the `names` section, add the name of the custom metrics that is to be collected from the managed cluster.
** In the `rules` section, enter only one value for the `expr` and `record` parameter pair to define the query expression. The metrics are collected as the name that is defined in the `record` parameter from your managed cluster. The metric value returned are the results after you run the query expression.
** The `names` and `rules` sections are optional. You can use either one or both of the sections.

Create the `observability-metrics-custom-allowlist` ConfigMap in the `open-cluster-management-observability` namespace with the following command: `oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml`.

Verify that data from your custom metric is being collected by querying the metric from the *Explore* page, from the Grafana dashboard. You can also use the custom metrics in your own dashboard. For more information about viewing the dashboard, see xref:../observability/design_grafana.adoc#designing-your-grafana-dashboard[Designing your Grafana dashboard].

[#removing-default-metrics]
== Removing default metrics

If you want data to not be collected in your managed cluster for a specific metric, remove the metric from the `observability-metrics-custom-allowlist.yaml` file. When you remove a metric, the metric data is not collected in your managed clusters. As mentioned previously, first verify that `mco observability` is enabled.

Add the name of the default metric to the `metrics_list.yaml` parameter with a hyphen `-` at the start of the metric name. For example, `-cluster_infrastructure_provider`.

Create the `observability-metrics-custom-allowlist` ConfigMap in the 
`open-cluster-management-observability` namespace with the following command: `oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml`.

Verify that the specific metric is not being collected from your managed clusters. When you query the metric from the Grafana dashboard, the metric is not displayed.

[#adding-advanced-config]
== Adding _advanced_ configuration

Add the `advanced` configuration section to update the retention for each observability component, according to your needs. 

Edit the `MultiClusterObservability` CR and add the `advanced` section with the following command: `oc edit mco observability -o yaml`. Your YAML file might resemble the following contents:

[source,yaml]
----
spec:
  advanced:
    retentionConfig:
      blockDuration: 2h
      deleteDelay: 48h
      retentionInLocal: 24h
      retentionResolutionRaw: 30d
      retentionResolution5m: 180d
      retentionResolution1h: 0d
    receive:
      resources:
        limits:
          memory: 4096Gi
      replicas: 3 
----

For descriptions of all the parameters that can added into the `advanced` configuration, see the link:../apis/observability.json.adoc[Observability API].

[#updating-replicas]
== Updating the _multiclusterobservability_ CR replicas from the console

If your workload increases, increase the number of replicas of your observability pods. Navigate to the {ocp} console from your hub cluster. Locate the `multiclusterobservability` custom resource (CR), and update the `replicas` parameter value for the component where you want to change the replicas. Your updated YAML might resemble the following content:

[source,yaml]
----
spec:
   advanced:
      receive:
         replicas: 6
----

For more information about the parameters within the `mco observability` CR, see the link:../apis/observability.json.adoc#observability-api[Observability API].

[#forward-alerts]
== Forwarding alerts

After you enable observability, alerts from your {ocp-short} managed clusters are automatically sent to the hub cluster. You can use the `alertmanager-config` YAML file to configure alerts with an external notification system. 

View the following example of the `alertmanager-config` YAML file:

[source,yaml]
----
global:
  slack_api_url: '<slack_webhook_url>'

route:
  receiver: 'slack-notifications'
  group_by: [alertname, datacenter, app]

receivers:
- name: 'slack-notifications'
  slack_configs:
  - channel: '#alerts'
    text: 'https://internal.myorg.net/wiki/alerts/{{ .GroupLabels.app }}/{{ .GroupLabels.alertname }}'
----

To learn more, see the https://prometheus.io/docs/alerting/latest/alertmanager/[Prometheus Alertmanager documentation].

[#customizing-route-cert]
== Customizing route certification

If you want to customize the {ocp-short} route certification, you must add the routes in the `alt_names` section. To ensure your {ocp-short} routes are accessible, add the following information: `alertmanager.apps.<domainname>`, `observatorium-api.apps.<domainname>`, `rbac-query-proxy.apps.<domainname>`.

*Note*: Users are responsible for certificate rotations and updates.

[#customizing-certificates-object-store]
=== Customizing certificates for accessing the object store

You can customize certificates for accessing the object store. Edit the `http_config` section by adding the certificate in the object store secret. View the following example:

[source,yaml]
----
 thanos.yaml: |
    type: s3
    config:
      bucket: "thanos"
      endpoint: "minio:9000"
      insecure: false
      access_key: "minio"
      secret_key: "minio123"
      http_config:
        tls_config:
          ca_file: /etc/minio/certs/ca.crt
          cert_file: /etc/minio/certs/public.crt
          key_file: /etc/minio/certs/private.key
          insecure_skip_verify: false
----

You must provide a secret in the `open-cluster-management-observability` namespace. The secret must contain the `ca.crt`, `public.crt`, and `private.key` that you defined in the previous secret example. 

You can also configure the secret name, the `TLSSecretName` parameter in the `MultiClusterObservability` CR. View the following example where the secret name is `tls-certs-secret`:

[source,yaml]
----
metricObjectStorage:
      key: thanos.yaml
      name: thanos-object-storage
      tlsSecretName: tls-certs-secret
----

This secret can be mounted into all components that need to access the object store, and it includes the following components: `receiver`, `store`, `ruler`, `compact`.

[#viewing-and-exploring-data]
== Viewing and exploring data

View the data from your managed clusters by accessing Grafana from the hub cluster. You can query specific alerts and add filters for the query. 

For example, to _cluster_infrastructure_provider_ from a single node cluster, use the following query expression: `cluster_infrastructure_provider{clusterType="SNO"}`

*Note*: Do not set the `ObservabilitySpec.resources.CPU.limits` parameter if observability is enabled on single node managed clusters. When you set the CPU limits, it causes the observability pod to be counted against the capacity for your managed cluster. See https://github.com/openshift/enhancements/blob/master/enhancements/workload-partitioning/management-workload-partitioning.md#management-workload-partitioning[Management Workload Partitioning] for more information.

[#viewing-etcd-grafana]
=== Viewing the etcd table

View the etcd table from the hub cluster dashboard in Grafana to learn the stability of the etcd as a data store. 

Select the Grafana link from your hub cluster to view the _etcd_ table data, which is collected from your hub cluster. The _Leader election changes_ across managed clusters are displayed.

[#viewing-cluster-fleet-service-level-overview-on-k8s-api-server-grafana]
=== Viewing the cluster fleet service-level overview for the Kubernetes API server dashboard

View the cluster fleet Kubernetes API service-level overview from the hub cluster dashboard in Grafana.

After you navigate to the Grafana dashboard, access the managed dashboard menu by selecting *Kubernetes* > *Service-Level Overview* > *API Server*. The _Fleet Overview_ and _Top Cluster_ details are displayed. 

View the total number of clusters that are exceeding or meeting the targeted _service-level objective_ (SLO) value for the past seven or 30-day period, offending and non-offending clusters, and API Server Request Duration.

[#viewing-cluster-service-level-overview-on-k8s-api-server-grafana]
=== Viewing the cluster service-level overview for the Kubernetes API server dashboard

View the Kubernetes API service-level overview table from the hub cluster dashboard in Grafana. 

After you navigate to the Grafana dashboard, access the managed dashboard menu by selecting *Kubernetes* > *Service-Level Overview* > *API Server*. The _Fleet Overview_ and _Top Cluster_ details are displayed.

View the error budget for the past seven or 30-day period, the remaining downtime, and trend.

[#disable-observability]
== Disable observability

You can disable observability, which stops data collection on the {product-title-short} hub cluster.

[#disable-observability-on-all-clusters]
=== Disable observability on all clusters

Disable observability by removing observability components on all managed clusters.

Update the `multicluster-observability-operator` resource by setting `enableMetrics` to `false`. Your updated resource might resemble the following change:

[source,yaml]
----
spec:
  imagePullPolicy: Always
  imagePullSecret: multiclusterhub-operator-pull-secret
  observabilityAddonSpec: # The ObservabilityAddonSpec defines the global settings for all managed clusters which have observability add-on enabled
    enableMetrics: false #indicates the observability addon push metrics to hub server
----

[#disable-observability-on-a-single-cluster]
=== Disable observability on a single cluster

Disable observability by removing observability components on specific managed clusters. Add the `observability: disabled` label to the `managedclusters.cluster.open-cluster-management.io` custom resource.

From the {product-title-short} console _Clusters_ page, add the `observability=disabled` label to the specified cluster.

*Note*: When a managed cluster with the observability component is detached, the `metrics-collector` deployments are removed.

For more information on monitoring data from the console with the observability service, see xref:../observability/observe_environments_intro.adoc#observing-environments-intro[Observing environments introduction].
