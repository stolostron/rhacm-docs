[#customizing-observability]
= Customizing observability configuration

Customize the obseravbility configuration to the specific needs of your environment, after you enable observability.

Read the following sections to learn more about how you want to manage and view cluster fleet data that is collected by the observability service:

*Required access:* Cluster administrator

- <<creating-custom-rules,Creating custom rules>>
- <<adding-custom-metrics, Adding custom metrics>>
- <<adding-advanced-config,Adding _advanced_ configuration>>
- <<updating-replicas,Updating the _MultiClusterObservability_ custom resource replicas from the console>>
- <<customizing-route-cert,Customizing route certification>>
- <<customizing-certificates-object-store,Customizing certificates for accessing the object store>>

[#creating-custom-rules]
== Creating custom rules

Create custom rules for the observability installation by adding Prometheus link:https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/[recording rules] and link:https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[alerting rules] to the observability resource.

** Recording rules provide you the ability to precalculate, or computate expensive expressions as needed. The results are saved as a new set of time series.
** Alerting rules provide you the ability to specify the alert conditions based on how an alert should be sent to an external service.
+
Define custom rules with Prometheus to create alert conditions, and send notifications to an external messaging service. 
+
*Note:* When you update your custom rules, `observability-thanos-rule` pods are restarted automatically.
+
Create a ConfigMap named `thanos-ruler-custom-rules` in the `open-cluster-management-observability` namespace. The key must be named, `custom_rules.yaml`, as shown in the following example. You can create multiple rules in the configuration.
+
* By default, the out-of-the-box alert rules are defined in the `thanos-ruler-default-rules` ConfigMap in the `open-cluster-management-observability` namespace. 
+
For example, you can create a custom alert rule that notifies you when your CPU usage passes your defined value. Your YAML might resemble the following content: 
+
[source,yaml]
----
data:
  custom_rules.yaml: |
    groups:
      - name: cluster-health
        rules:
        - alert: ClusterCPUHealth-jb
          annotations:
            summary: Notify when CPU utilization on a cluster is greater than the defined utilization limit
            description: "The cluster has a high CPU usage: {{ $value }} core for {{ $labels.cluster }} {{ $labels.clusterID }}."
          expr: |
            max(cluster:cpu_usage_cores:sum) by (clusterID, cluster, prometheus) > 0
          for: 5s
          labels:
            cluster: "{{ $labels.cluster }}"
            prometheus: "{{ $labels.prometheus }}"
            severity: critical
----

* You can also create a custom recording rule within the `thanos-ruler-custom-rules` ConfigMap.
+
For example, you can create a recording rule that provides you the ability to get the sum of the container memory cache of a pod. Your YAML might resemble the following content:
+
[source,yaml]
----
data:
  custom_rules.yaml: |
    groups:
      - name: container-memory
        recording_rules:
        - record: pod:container_memory_cache:sum
          expr: sum(container_memory_cache{pod!=""}) BY (pod, container)
----
+
*Note:* If this is the first new custom rule, it is created immediately. For changes to the ConfigMap, the configuration is automatically reloaded. The configuration is reloaded because of the `config-reload` within the `observability-thanos-ruler` sidecar.

To verify that the alert rules are functioning correctly, launch the Grafana dashboard, navigate to the *Explore* page, and query `ALERTS`. The alert is only available in Grafana if the alert is initiated. 

[#adding-custom-metrics]
== Adding custom metrics

Add metrics to the `metrics_list.yaml` file, to be collected from managed clusters. Complete the following steps:

. Before you add a custom metric, verify that `mco observability` is enabled with the following command: 
+
[source,bash]
----
oc get mco observability -o yaml
----

. Check for the following message in the `status.conditions.message` section reads:
+
[source,bash]
----
Observability components are deployed and running
----

. Create the `observability-metrics-custom-allowlist` ConfigMap in the `open-cluster-management-observability` namespace with the following command:
+
[source,bash]
----
oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml
----

. Add the name of the custom metric to the `metrics_list.yaml` parameter. Your YAML for the ConfigMap might resemble the following content:
+
[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-custom-allowlist
data:
  metrics_list.yaml: |
    names: <1>
      - node_memory_MemTotal_bytes
    rules: <2>
    - record: apiserver_request_duration_seconds:histogram_quantile_90
      expr: histogram_quantile(0.90,sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",
        verb!=\"WATCH\"}[5m])) by (verb,le))
----
+
<1> *Optional:* Add the name of the custom metrics that are to be collected from the managed cluster.
<2> *Optional:* Enter only one value for the `expr` and `record` parameter pair to define the query expression. The metrics are collected as the name that is defined in the `record` parameter from your managed cluster. The metric value returned are the results after you run the query expression.
+
You can use either one or both of the sections.
+
For user workload metrics, see the _Adding user workload metrics_ section.

. Verify that data from your custom metric is being collected by querying the metric from the *Explore* page, from the Grafana dashboard. You can also use the custom metrics in your own dashboard.

[#adding-user-workload-metrics]
=== Adding user workload metrics

You can collect {ocp-short} user-defined metrics from workloads in {ocp-short}. You must enable monitoring on your {ocp-short} cluster. See _Enabling monitoring for user-defined projects_.

If you have a managed cluster with monitoring for user-defined workloads enabled, the user workloads are located in the `test` namespace and generate metrics. These metrics are collected by Prometheus from the {ocp-short} user workload.

Add user workload metrics to the `observability-metrics-custom-allowlist` ConfigMap to collect the metrics in the `test` namespace. View the following example:

[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-custom-allowlist
  namespace: test
data:
  uwl_metrics_list.yaml: <1>
    names: <2>
      - sample_metrics
----

<1> Enter the key for the ConfigMap data.
<2> Enter the value of the ConfigMap data in YAML format. The `names` section includes the list of metric names, which you want to collect from the `test` namespace. After you create the ConfigMap, the specified metrics from the target namespace is collected by the observability collector and pushed to the hub cluster.

[#removing-default-metrics]
=== Removing default metrics

If you want data to not be collected in your managed cluster for a specific metric, remove the metric from the `observability-metrics-custom-allowlist.yaml` file. When you remove a metric, the metric data is not collected in your managed clusters. As mentioned previously, first verify that `mco observability` is enabled.

Complete the following steps to remove a default metric:

. Add the name of the default metric to the `metrics_list.yaml` parameter with a hyphen `-` at the start of the metric name. For example, `-cluster_infrastructure_provider`.

. Create the `observability-metrics-custom-allowlist` ConfigMap in the 
`open-cluster-management-observability` namespace with the following command: 
+
[source,bash]
----
oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml
----

. Verify that the specific metric is not being collected from your managed clusters. When you query the metric from the Grafana dashboard, the metric is not displayed.

[#adding-advanced-config]
== Adding advanced configuration for retention

Add the `advanced` configuration section to update the retention for each observability component, according to your needs. 

Edit the `MultiClusterObservability` custom resource and add the `advanced` section with the following command:

[source,bash]
----
oc edit mco observability -o yaml
----

Your YAML file might resemble the following contents:

[source,yaml]
----
spec:
  advanced:
    retentionConfig:
      blockDuration: 2h
      deleteDelay: 48h
      retentionInLocal: 24h
      retentionResolutionRaw: 30d
      retentionResolution5m: 180d
      retentionResolution1h: 0d
    receive:
      resources:
        limits:
          memory: 4096Gi
      replicas: 3 
----

For descriptions of all the parameters that can added into the `advanced` configuration, see the _Observability API_ documentation.

[#dynamic-metrics-for-sno]
== Dynamic metrics for {sno} clusters

Dynamic metrics collection supports automatic metric collection based on certain conditions. By default, a {sno} cluster does not collect pod and container resource metrics. Once a {sno} cluster reaches a specific level of resource consumption, the defined granular metrics are collected dynamically. When the cluster resource consumption is consistently less than the threshold for a period of time, granular metric collection stops.

The metrics are collected dynamically based on the conditions on the managed cluster specified by a collection rule. Because these metrics are collected dynamically, the following {product-title-short} Grafana dashboards do not display any data. When a collection rule is activated and the corresponding metrics are collected, the following panels display data for the duration of the time that the collection rule is initiated:

* Kubernetes/Compute Resources/Namespace (Pods)
* Kubernetes/Compute Resources/Namespace (Workloads)
* Kubernetes/Compute Resources/Nodes (Pods)
* Kubernetes/Compute Resources/Pod
* Kubernetes/Compute Resources/Workload
A collection rule includes the following conditions:
* A set of metrics to collect dynamically.
* Conditions written as a PromQL expression.
* A time interval for the collection, which must be set to `true`.
* A match expression to select clusters where the collect rule must be evaluated.

By default, collection rules are evaluated continuously on managed clusters every 30 seconds, or at a specific time interval. The lowest value between the collection interval and time interval takes precedence. Once the collection rule condition persists for the duration specified by the `for` attribute, the collection rule starts and the metrics specified by the rule are automatically collected on the managed cluster. Metrics collection stops automatically after the collection rule condition no longer exists on the managed cluster, at least 15 minutes after it starts.

The collection rules are grouped together as a parameter section named `collect_rules`, where it can be enabled or disabled as a group. {product-title-short} installation includes the collection rule group, `SNOResourceUsage` with two default collection rules: `HighCPUUsage` and `HighMemoryUsage`. The `HighCPUUsage` collection rule begins when the node CPU usage exceeds 70%. The `HighMemoryUsage` collection rule begins if the overall memory utilization of the {sno} cluster exceeds 70% of the available node memory. Currently, the previously mentioned thresholds are fixed and cannot be changed. When a collection rule begins for more than the interval specified by the `for` attribute, the system automatically starts collecting the metrics that are specified in the `dynamic_metrics` section.

View the list of dynamic metrics that from the `collect_rules` section, in the following YAML file:

[source,yaml]
----
collect_rules:
  - group: SNOResourceUsage
    annotations:
      description: >
        By default, a {sno} cluster does not collect pod and container resource metrics. Once a {sno} cluster 
        reaches a level of resource consumption, these granular metrics are collected dynamically. 
        When the cluster resource consumption is consistently less than the threshold for a period of time, 
        collection of the granular metrics stops.
    selector:
      matchExpressions:
        - key: clusterType
          operator: In
          values: ["{sno}"]
    rules:
    - collect: SNOHighCPUUsage
      annotations:
        description: >
          Collects the dynamic metrics specified if the cluster cpu usage is constantly more than 70% for 2 minutes
      expr: (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))) * 100 > 70
      for: 2m
      dynamic_metrics:
        names:
          - container_cpu_cfs_periods_total
          - container_cpu_cfs_throttled_periods_total
          - kube_pod_container_resource_limits 
          - kube_pod_container_resource_requests   
          - namespace_workload_pod:kube_pod_owner:relabel 
          - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate 
          - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate 
    - collect: SNOHighMemoryUsage
      annotations:
        description: >
          Collects the dynamic metrics specified if the cluster memory usage is constantly more than 70% for 2 minutes
      expr: (1 - sum(:node_memory_MemAvailable_bytes:sum) / sum(kube_node_status_allocatable{resource=\"memory\"})) * 100 > 70
      for: 2m
      dynamic_metrics:
        names:
          - kube_pod_container_resource_limits 
          - kube_pod_container_resource_requests 
          - namespace_workload_pod:kube_pod_owner:relabel
        matches:
          - __name__="container_memory_cache",container!=""
          - __name__="container_memory_rss",container!=""
          - __name__="container_memory_swap",container!=""
          - __name__="container_memory_working_set_bytes",container!=""
----

A `collect_rules.group` can be disabled in the `custom-allowlist` as shown in the following example. When a `collect_rules.group` is disabled, metrics collection reverts to the previous behavior. These metrics are collected at regularly, specified intervals:

[source,yaml]
----
collect_rules:
  - group: -SNOResourceUsage
---- 

The data is only displayed in Grafana when the rule is initiated.

[#updating-replicas]
== Updating the _MultiClusterObservability_ custom resource replicas from the console

If your workload increases, increase the number of replicas of your observability pods. Navigate to the {ocp} console from your hub cluster. Locate the `MultiClusterObservability` custom resource, and update the `replicas` parameter value for the component where you want to change the replicas. Your updated YAML might resemble the following content:

[source,yaml]
----
spec:
   advanced:
      receive:
         replicas: 6
----

For more information about the parameters within the `mco observability` custom resource, see the _Observability API_ documentation.

[#customizing-route-cert]
== Customizing route certificate

If you want to customize the {ocp-short} route certification, you must add the routes in the `alt_names` section. To ensure your {ocp-short} routes are accessible, add the following information: `alertmanager.apps.<domainname>`, `observatorium-api.apps.<domainname>`, `rbac-query-proxy.apps.<domainname>`.

For more details, see _Replacing certificates for alertmanager route_ in the Governance documentation.

*Note:* Users are responsible for certificate rotations and updates.

[#customizing-certificates-object-store]
=== Customizing certificates for accessing the object store

//step by step process instead of a vague description (especially for the 1.3.10.1 section) (request from Felix) | MJ | 06/28/23
You can customize certificates for accessing the object store. Edit the `http_config` section by adding the certificate in the object store secret. View the following example:

[source,yaml]
----
 thanos.yaml: |
    type: s3
    config:
      bucket: "thanos"
      endpoint: "minio:9000"
      insecure: false
      access_key: "minio"
      secret_key: "minio123"
      http_config:
        tls_config:
          ca_file: /etc/minio/certs/ca.crt
          insecure_skip_verify: false
----

You must provide a secret in the `open-cluster-management-observability` namespace. The secret must contain the `ca.crt` that you defined in the previous secret example.
If you want to enable Mutual TLS, you need to provide `public.crt`, and `private.key` in the previous secret. View the following example:

[source,yaml]
----
 thanos.yaml: |
    type: s3
    config:
      ...
      http_config:
        tls_config:
          ca_file: /etc/minio/certs/ca.crt
          cert_file: /etc/minio/certs/public.crt
          key_file: /etc/minio/certs/private.key
          insecure_skip_verify: false
----

You can also configure the secret name, the `TLSSecretName` parameter in the `MultiClusterObservability` custom resource. View the following example where the secret name is `tls-certs-secret`:

[source,yaml]
----
metricObjectStorage:
      key: thanos.yaml
      name: thanos-object-storage
      tlsSecretName: tls-certs-secret
----

This secret can be mounted into all components that need to access the object store, and it includes the following components: `receiver`, `store`, `ruler`, `compact`.

[#additional-resource-custom-obs]
== Additional resources

- Refer to link:https://prometheus.io/docs/prometheus/latest/configuration/configuration/[Prometheus configuration] for more information. For more information about recording rules and alerting rules, refer to the recording rules and alerting rules from the link:https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/[Prometheus documentation]. 

- For more information about viewing the dashboard, see xref:../observability/design_grafana.adoc#using-grafana-dashboards[Using Grafana dashboards].

- See xref:../observability/use_observability.adoc#exporting-metrics-to-external-endpoints[Exporting metrics to external endpoints]. 

- See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/monitoring/enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects].

- See the link:../apis/observability.json.adoc#observability-api[Observability API].

- For information about updating the certificate for the alertmanager route, see link:../governance/#replacing-cert-alertmanager[Replacing certificates for alertmanager].

- For more details about observability alerts, see xref:../observability/observability_alerts.adoc#observability-alerts[Observability alerts]

- To learn more about alert forwarding, see the link:https://prometheus.io/docs/alerting/latest/alertmanager/[Prometheus Alertmanager documentation].

- See xref:../observability/observe_environments_intro.adoc#observability-alerts[Observability alerts] for more information.

- For more topics about the observability service, see xref:../observability/observe_environments_intro.adoc#observing-environments-intro[Observability service introduction].

- See link:https://github.com/openshift/enhancements/blob/master/enhancements/workload-partitioning/management-workload-partitioning.md#management-workload-partitioning[Management Workload Partitioning] for more information.

- Return to the beginning of this topic, <<customizing-observability,Customizing observability>>.

