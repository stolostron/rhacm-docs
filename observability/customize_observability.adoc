[#customizing-observability]
= Customizing observability

Review the following sections to learn more about customizing, managing, and viewing data that is collected by the observability service.

Collect logs about new information that is created for observability resources with the `must-gather` command. For more information, see the _Must-gather_ section in the link:../troubleshooting/troubleshooting_intro.adoc[Troubleshooting documentation].

* <<creating-custom-rules,Creating custom rules>>
* <<configuring-rules-for-alertmanager,Configuring rules for AlertManager>>
* <<adding-custom-metrics, Adding custom metrics>>
* <<removing-default-metrics,Removing default metrics>>
* <<viewing-and-exploring-data,Viewing and exploring data>>
* <<disable-metrics-collector,Disabling _metrics-collector_>>

[#creating-custom-rules]
== Creating custom rules

You can create custom rules for the observability installation by adding Prometheus https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/[recording rules] and https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[alerting rules] to the observability resource. For more information, see https://prometheus.io/docs/prometheus/latest/configuration/configuration/[Prometheus configuration].

Define custom rules with Prometheus to create alert conditions, and send notifications to an external messaging service. *Note*: When you update your custom rules, `observability-observatorium-thanos-rule` pods are restarted automatically.

Complete the following steps to create a custom rule: 

. Log in to your {product-title-short} hub cluster.
. Create a ConfigMap named `thanos-ruler-custom-rules` in the `open-cluster-management-observability` namespace. The key must be named, `custom_rules.yaml`, as shown in the following example. You can create multiple rules in the configuration:
+
By default, the out-of-the-box alert rules are defined in the ConfigMap in the `open-cluster-management-observability` namespace. 
+
For example, you can create a custom alert rule that notifies you when your CPU usage passes your defined value: 
+
----
data:
  custom_rules.yaml: |
    groups:
      - name: cluster-health
        rules:
        - alert: ClusterCPUHealth-jb
          annotations:
            summary: Notify when CPU utilization on a cluster is greater than the defined utilization limit
            description: "The cluster has a high CPU usage: {{ $value }} core for {{ $labels.cluster }} {{ $labels.clusterID }}."
          expr: |
            max(cluster:cpu_usage_cores:sum) by (clusterID, cluster, prometheus) > 0
          for: 5s
          labels:
            cluster: "{{ $labels.cluster }}"
            prometheus: "{{ $labels.prometheus }}"
            severity: critical
----
+
*Note*: If this is the first new custom rule, it is created immediately. For changes to the ConfigMap, you must restart the observability pods with the following command: `kubectl rollout restart statefulset observability-observatorium-thanos-rule -n open-cluster-management-observability`.

. If you want to verify that the alert rules is functioning appropriately, complete the following steps:
.. Access your Grafana dashboard and select the *Explore* icon.
.. In the Metrics exploration bar, type in "ALERTS" and run the query. All the ALERTS that are currently in pending or firing state in the system are displayed.
.. If your alert is not displayed, revisit the rule to see if the expression is accurate.

A custom rule is created.

[#configuring-rules-for-alertmanager]
== Configuring rules for AlertManager

Integrate external messaging tools such as email, Slack, and PagerDuty to receive notifications from AlertManager. You must override the `alertmanager-config` secret in the `open-cluster-management-observability` namespace to add integrations, and configure routes for AlertManager. Complete the following steps to update the custom receiver rules:

. Extract the data from the `alertmanager-config` secret. Run the following command:
+
----
oc -n open-cluster-management-observability get secret alertmanager-config --template='{{ index .data "alertmanager.yaml" }}' |base64 -d > alertmanager.yaml
----

. Edit and save the `alertmanager.yaml` file configuration by running the following command:
+
----
oc -n open-cluster-management-observability create secret generic alertmanager-config --from-file=alertmanager.yaml --dry-run -o=yaml |  oc -n open-cluster-management-observability replace secret --filename=-
----
+
Your updated secret might resemble the following content:
+
----
global
  smtp_smarthost: 'localhost:25'
  smtp_from: 'alertmanager@example.org'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: 'password'
templates: 
- '/etc/alertmanager/template/*.tmpl'
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 3h 
  receiver: team-X-mails
  routes:
  - match_re:
      service: ^(foo1|foo2|baz)$
    receiver: team-X-mails
----

Your changes are applied immediately after it is modified. For an example of AlertManager, see https://github.com/prometheus/alertmanager/blob/master/doc/examples/simple.yml[prometheus/alertmanager].

[#adding-custom-metrics]
== Adding custom metrics

Add metrics to the `metrics_list.yaml` file, to be collected from managed clusters.

Complete the following steps to add custom metrics:

. Log in to your cluster.
. Verify that `mco observability` is enabled. Check for the following message in the `status.conditions.message` reads: `Observability components are deployed and running`. Run the following command:
+
----
oc get mco observability -o yaml
----

. Create a file named `observability-metrics-custom-allowlist.yaml` with the following content. Add the name and recording rule of the custom metric to the `metrics_list.yaml` parameter. For example, collect `node_memory_MemTotal_bytes` and `apiserver_request_duration_seconds:histogram_quantile_99` from your managed cluster. Your YAML for the ConfigMap might resemble the following content:
+
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-custom-allowlist
data:
  metrics_list.yaml: |
    names:
      - node_memory_MemTotal_bytes
    rules:
    - record: apiserver_request_duration_seconds:histogram_quantile_99
      expr: histogram_quantile(0.99,sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",
        verb!=\"WATCH\"}[5m])) by (verb,le))
----
+
** In the `names` section, add the name of the custom metrics which is to be collected from the managed cluster.
** In the `rules` section, enter a value for the `expr` parameter to define the query expression. The metrics are collected as the name that is defined in the `record` parameter from your managed cluster. For more information, see the documentation for https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/#recording-rules[recording rules]. 

. Create the `observability-metrics-custom-allowlist` ConfigMap in the 
`open-cluster-management-observability` namespace by running the following command:
+
----
oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml
----

. Verify that your custom metric is being collected from your managed clusters by viewing the metric on the Grafana dashboard. From your hub cluster, select the **Grafana dashboard** link.

. From the Grafana search bar, enter the metric that you want to view.

Data from your custom metric is collected.

[#removing-default-metrics]
== Removing default metrics

If you want data to not be collected for a specific metric, you can remove the metric from the `observability-metrics-custom-allowlist.yaml` file. When you remove a metric, you are also deleting the metric. You can add the metric name to the `metrics_list.yaml` file and end it with the hyphen `-`.

Complete the following steps to delete default metrics:

. Log in to your cluster.
. Verify that `mco observability` is enabled. Check for the following message in the `status.conditions.message` reads: `Observability components are deployed and running`. Run the following command:
+
----
oc get mco observability -o yaml
----

. In the `observability-metrics-custom-allowlist.yaml` file, add the name of the default metric to the `metrics_list.yaml` parameter with a hyphen `-` at the start of the metric name. For example, add `-rest_client_requests_total` to the metric list. Your YAML for the ConfigMap might resemble the following content:
+
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-custom-allowlist
data:
  metrics_list.yaml: |
    names:
      - node_memory_MemTotal_bytes
      - -rest_client_requests_total
----

. Create the `observability-metrics-custom-allowlist` ConfigMap in the 
`open-cluster-management-observability` namespace by running the following command:
+
----
oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml
----

. Verify that your default metric is not being collected from your managed clusters by viewing the metric on the Grafana dashboard. From your hub cluster, select the **Grafana dashboard** link.

. From the Grafana search bar, enter the metric that you want to check.

Data from your default metric is no longer being collected.

[#viewing-and-exploring-data]
== Viewing and exploring data

View the data from your managed clusters by accessing Grafana. Complete the following steps to view the Grafana dashboards from the console:

. Log in to your {product-title-short} hub cluster. 
. From the navigation menu, select *Observe environments* > *Overview* > *Grafana link*. 
+
You can also  access Grafana dashboards from the _Clusters_ page. From the navigation menu, select *Automate infrastructure* > *Clusters* > *Grafana*.
. Access the Prometheus metric explorer by selecting the *Explore* icon from the Grafana navigation menu.
. View the metrics that are collected from your Single Node OpenShift (SNO):
.. Log in to your {product-title-short} console.
.. From the navigation menu, select **Clusters** > **Grafana** link.
.. Query the metric that you want to check. Results are displayed in the metric table.
.. Metrics that are labled as `SNO` in the _clusterType_ column are collected from your SNO.

[#disable-metrics-collector]
== Disabling _metrics-collector_

You can disable the `metrics-collector`, which stops it from collecting the data and sending the collection data to the observability service. 

[#disable-metrics-collector-on-all-clusters]
=== Disabling _metrics-collector_ on all clusters

Disable the `metrics-collector` pod to stop data from being collected and sent to the observability service on the {product-title-short} hub cluster. 

When you disable the `metrics-collector`, the deployment is scaled to zero and all managed clusters are disabled. View the following options to disable the `metrics-collector`:

Update the `multicluster-observability-operator` resource by setting `enableMetrics` to `false`. Your updated resource might resemble the following change:

----
spec:
  imagePullPolicy: Always
  imagePullSecret: multiclusterhub-operator-pull-secret
  observabilityAddonSpec: # The ObservabilityAddonSpec defines the global settings for all managed clusters which have observability add-on enabled
    enableMetrics: false #indicates the observability addon push metrics to hub server
----

[#disable-metrics-collector-on-a-single-cluster]
=== Disabling _metrics-collector_ on a single cluster

You can disable the `metrics-collector` on specific managed clusters by completing one of the following procedures:

* Add the `observability: disabled` label to the custom resource, `managedclusters.cluster.open-cluster-management.io`.
* From the {product-title-short} console _Clusters_ page, add the `observability: disabled` label by completing the following steps:
+
. In the {product-title-short} console navigation, select *Automate infrastructure* > *Clusters*.
. Select the name of the cluster for which you want to disable data collection that is sent to observability. 
. Select *Labels*.
. Create the label that disables the observability collection by adding the following label:
+
----
observability=disabled
----
. Select *Add* to add the label.
. Select *Done* to close the list of labels. 

*Note*: When a managed cluster with the observability component is detached, the `metrics-collector` deployments are removed.

For more information on monitoring data from the console with the observability service, see xref:../observability/observe_environments_intro.adoc#observing-environments-intro[Observing environments introduction].
