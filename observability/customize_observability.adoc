[#customizing-observability]
= Customizing observability

Review the following sections to learn more about customizing, managing, and viewing data that is collected by the observability service.

Collect logs about new information that is created for observability resources with the `must-gather` command. For more information, see the _Must-gather_ section in the link:../troubleshooting/troubleshooting_intro.adoc[Troubleshooting documentation].

- <<creating-custom-rules,Creating custom rules>>
- <<configuring-alertmanager,Configuring AlertManager>>
- <<forward-alerts,Forwarding alerts>>
* <<disabling-forward-alerts,Disabling forward alerts for managed clusters>>
- <<silence-alerts,Silencing alerts>>
- <<supress-alerts,Suppressing alerts>>
- <<adding-custom-metrics, Adding custom metrics>>
* <<adding-user-workload-metrics,Adding user workload metrics>>
* <<removing-default-metrics,Removing default metrics>>
- <<exporting-metrics-to-external-endpoints,Exporting metrics to external endpoints>>
* <<creating-the-kubernetes-secret-for-external-endpoint,Creating the Kubernetes secret for an external endpoint>>
* <<updating-the-multiclusterobservability-cr,Updating the _MultiClusterObservability_ custom resource>>
* <<viewing-the-status-of-metrics-export,Viewing the status of metric export>>
- <<adding-advanced-config,Adding _advanced_ configuration>>
- <<updating-replicas,Updating the _MultiClusterObservability_ custom resource replicas from the console>>
- <<customizing-route-cert,Customizing route certification>>
- <<customizing-certificates-object-store,Customizing certificates for accessing the object store>>
- <<viewing-and-exploring-data,Viewing and exploring data>>
* <<viewing-etcd-grafana,Viewing the etcd table>>
* <<viewing-cluster-fleet-service-level-overview-on-k8s-api-server-grafana,Viewing the cluster fleet service-level overview for the Kubernetes API server dashboard>>
* <<viewing-cluster-service-level-overview-on-k8s-api-server-grafana,Viewing the cluster service-level overview for the Kubernetes API server dashboard>>
- <<disabling-observability,Disabling observability>>

[#creating-custom-rules]
== Creating custom rules

Create custom rules for the observability installation by adding Prometheus link:https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/[recording rules] and link:https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[alerting rules] to the observability resource. For more information, see link:https://prometheus.io/docs/prometheus/latest/configuration/configuration/[Prometheus configuration].

** Recording rules provide you the ability to precalculate, or computate expensive expressions as needed. The results are saved as a new set of time series.
** Alerting rules provide you the ability to specify the alert conditions based on how an alert should be sent to an external service.
+
Define custom rules with Prometheus to create alert conditions, and send notifications to an external messaging service. 
+
*Note:* When you update your custom rules, `observability-thanos-rule` pods are restarted automatically.
+
Create a ConfigMap named `thanos-ruler-custom-rules` in the `open-cluster-management-observability` namespace. The key must be named, `custom_rules.yaml`, as shown in the following example. You can create multiple rules in the configuration.
+
* By default, the out-of-the-box alert rules are defined in the `thanos-ruler-default-rules` ConfigMap in the `open-cluster-management-observability` namespace. 
+
For example, you can create a custom alert rule that notifies you when your CPU usage passes your defined value. Your YAML might resemble the following content: 
+
[source,yaml]
----
data:
  custom_rules.yaml: |
    groups:
      - name: cluster-health
        rules:
        - alert: ClusterCPUHealth-jb
          annotations:
            summary: Notify when CPU utilization on a cluster is greater than the defined utilization limit
            description: "The cluster has a high CPU usage: {{ $value }} core for {{ $labels.cluster }} {{ $labels.clusterID }}."
          expr: |
            max(cluster:cpu_usage_cores:sum) by (clusterID, cluster, prometheus) > 0
          for: 5s
          labels:
            cluster: "{{ $labels.cluster }}"
            prometheus: "{{ $labels.prometheus }}"
            severity: critical
----

* You can also create a custom recording rule within the `thanos-ruler-custom-rules` ConfigMap.
+
For example, you can create a recording rule that provides you the ability to get the sum of the container memory cache of a pod. Your YAML might resemble the following content:
+
[source,yaml]
----
data:
  custom_rules.yaml: |
    groups:
      - name: container-memory
        recording_rules:
        - record: pod:container_memory_cache:sum
          expr: sum(container_memory_cache{pod!=""}) BY (pod, container)
----
+
*Note:* If this is the first new custom rule, it is created immediately. For changes to the ConfigMap, the configuration is automatically reloaded. The configuration is reloaded because of the `config-reload` within the `observability-thanos-ruler` sidecar.

To verify that the alert rules are functioning appropriately, launch the Grafana dashboard, navigate to the *Explore* page, and query `ALERTS`. The alert is only available in Grafana if the alert is initiated.

[#configuring-alertmanager]
== Configuring AlertManager

Integrate external messaging tools such as email, Slack, and PagerDuty to receive notifications from AlertManager. You must override the `alertmanager-config` secret in the `open-cluster-management-observability` namespace to add integrations, and configure routes for AlertManager. Complete the following steps to update the custom receiver rules:

. Extract the data from the `alertmanager-config` secret. Run the following command:
+
----
oc -n open-cluster-management-observability get secret alertmanager-config --template='{{ index .data "alertmanager.yaml" }}' |base64 -d > alertmanager.yaml
----

. Edit and save the `alertmanager.yaml` file configuration by running the following command:
+
----
oc -n open-cluster-management-observability create secret generic alertmanager-config --from-file=alertmanager.yaml --dry-run -o=yaml |  oc -n open-cluster-management-observability replace secret --filename=-
----
+
Your updated secret might resemble the following content:
+
[source,yaml]
----
global
  smtp_smarthost: 'localhost:25'
  smtp_from: 'alertmanager@example.org'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: 'password'
templates: 
- '/etc/alertmanager/template/*.tmpl'
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 3h 
  receiver: team-X-mails
  routes:
  - match_re:
      service: ^(foo1|foo2|baz)$
    receiver: team-X-mails
----

Your changes are applied immediately after it is modified. For an example of AlertManager, see https://github.com/prometheus/alertmanager/blob/master/doc/examples/simple.yml[prometheus/alertmanager].

[#forward-alerts]
== Forwarding alerts

After you enable observability, alerts from your {ocp-short} managed clusters are automatically sent to the hub cluster. You can use the `alertmanager-config` YAML file to configure alerts with an external notification system. 

View the following example of the `alertmanager-config` YAML file:

[source,yaml]
----
global:
  slack_api_url: '<slack_webhook_url>'

route:
  receiver: 'slack-notifications'
  group_by: [alertname, datacenter, app]

receivers:
- name: 'slack-notifications'
  slack_configs:
  - channel: '#alerts'
    text: 'https://internal.myorg.net/wiki/alerts/{{ .GroupLabels.app }}/{{ .GroupLabels.alertname }}'
----

If you want to configure a proxy for alert forwarding, add the following `global` entry to the `alertmanager-config` YAML file:

[source,yaml]
----
global:
  slack_api_url: '<slack_webhook_url>'
  http_config:
    proxy_url: http://****
----

[#disabling-forward-alerts]
=== Disabling forward alerts for managed clusters

Disable alert forwarding for managed clusters. Add the following annotation to the `MultiClusterObservability` custom resource:

[source,yaml]
----
metadata:
      annotations:
        mco-disable-alerting: "true"
----

When you set the annotation, the alert forwarding configuration on the managed clusters is reverted. Any changes made to the `ocp-monitoring-config` ConfigMap in the `openshift-monitoring` namespace are reverted. Setting the annotation ensures that the `ocp-monitoring-config` ConfigMap is no longer managed or updated by the observability operator endpoint. After you update the configuration, the Prometheus instance on your managed cluster restarts.

*Important:* Metrics on your managed cluster are lost if you have a Prometheus instance with a persistent volume for metrics, and the Prometheus instance restarts. However, metrics from the hub cluster are not affected.

When the changes are reverted, a ConfigMap named `cluster-monitoring-reverted` is createde in the `open-cluster-management-addon-observability` namespace. Any new, manually added alert forward configurations are not reverted from the ConfigMap. 

Verify that the hub cluster alert manager is no longer propagating managed cluster alerts to third-party messaging tools. See the previous section, _Configuring AlertManager_. 

[#silence-alerts]
== Silencing alerts

Add alerts that you do not want to receive. You can silence alerts by the alert name, match label, or time duration. After you add the alert that you want to silence, an ID is created. Your ID for your silenced alert might resemble the following string, `d839aca9-ed46-40be-84c4-dca8773671da`.

Continue reading for ways to silence alerts:

- To silence a {product-title-short} alert, you must have access to the `alertmanager-main` pod in the `open-cluster-management-observability` namespace. For example, enter the following command in the pod terminal to silence `SampleAlert`:
+
----
amtool silence add --alertmanager.url="http://localhost:9093" --author="user" --comment="Silencing sample alert" alertname="SampleAlert"
----
- Silence an alert by using multiple match labels. The following command uses `match-label-1` and `match-label-2`:
+
----
amtool silence add --alertmanager.url="http://localhost:9093" --author="user" --comment="Silencing sample alert" <match-label-1>=<match-value-1> <match-label-2>=<match-value-2>
----
- If you want to silence an alert for a specific period of time, use the `--duration` flag. Run the following command to silence the `SampleAlert` for an hour:
+
----
amtool silence add --alertmanager.url="http://localhost:9093" --author="user" --comment="Silencing sample alert" --duration="1h" alertname="SampleAlert"
----
+
You can also specify a start or end time for the silenced alert. Enter the following command to silence the `SampleAlert` at a specific start time:
+
----
amtool silence add --alertmanager.url="http://localhost:9093" --author="user" --comment="Silencing sample alert" --start="2023-04-14T15:04:05-07:00" alertname="SampleAlert"
----
- To view all silenced alerts that are created, run the following command:
+
----
amtool silence --alertmanager.url="http://localhost:9093"
----
- If you no longer want an alert to be silenced, end the silencing of the alert by running the following command:
+
----
amtool silence expire --alertmanager.url="http://localhost:9093" "d839aca9-ed46-40be-84c4-dca8773671da"
----
+
- To end the silencing of all alerts, run the following command:
+
----
amtool silence expire --alertmanager.url="http://localhost:9093" $(amtool silence query --alertmanager.url="http://localhost:9093" -q) 
----

[#supress-alerts]
== Suppressing alerts

Suppress {product-title-short} alerts across your clusters globally that are less severe. Suppress alerts by defining an inhibition rule in the `alertmanager-config` in the `open-cluster-management-observability` namespace. 

An inhibition rule mutes an alert when there is a set of parameter matches that match another set of existing matchers. In order for the rule to take effect, both the target and source alerts must have the same label values for the label names in the `equal` list. Your `inhibit_rules` might resemble the following:

[source,yaml]
----
global:
  resolve_timeout: 1h
inhibit_rules:<1>
  - equal:
      - namespace
    source_match:<2>
      severity: critical
    target_match_re:
      severity: warning|info
----
<1> The `inhibit_rules` parameter section is defined to look for alerts in the same namespace. When a `critical` alert is initiated within a namespace and if there are any other alerts that contain the severity level `warning` or `info` in that namespace, only the `critical` alerts are routed to the AlertManager receiver. The following alerts might be displayed when there are matches:
+
----
ALERTS{alertname="foo", namespace="ns-1", severity="critical"}
ALERTS{alertname="foo", namespace="ns-1", severity="warning"}
----
+
<2> If the value of the `source_match` and `target_match_re` parameters do not match, the alert is routed to the receiver:
+
----
ALERTS{alertname="foo", namespace="ns-1", severity="critical"}
ALERTS{alertname="foo", namespace="ns-2", severity="warning"}
----

- To view suppressed alerts in {product-title-short}, enter the following command:
+
----
amtool alert --alertmanager.url="http://localhost:9093" --inhibited
----

[#adding-custom-metrics]
== Adding custom metrics

Add metrics to the `metrics_list.yaml` file, to be collected from managed clusters.

Before you add a custom metric, verify that `mco observability` is enabled with the following command: `oc get mco observability -o yaml`. Check for the following message in the `status.conditions.message` reads: `Observability components are deployed and running`.

Create a file named `observability-metrics-custom-allowlist.yaml` and add the name of the custom metric to the `metrics_list.yaml` parameter. Your YAML for the ConfigMap might resemble the following content:

[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-custom-allowlist
data:
  metrics_list.yaml: |
    names:
      - node_memory_MemTotal_bytes
    rules:
    - record: apiserver_request_duration_seconds:histogram_quantile_90
      expr: histogram_quantile(0.90,sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",
        verb!=\"WATCH\"}[5m])) by (verb,le))
----

For user workload metrics, see the _Adding user workload metrics_ section.

** In the `names` section, add the name of the custom metrics that is to be collected from the managed cluster.
** In the `rules` section, enter only one value for the `expr` and `record` parameter pair to define the query expression. The metrics are collected as the name that is defined in the `record` parameter from your managed cluster. The metric value returned are the results after you run the query expression.
** The `names` and `rules` sections are optional. You can use either one or both of the sections.

Create the `observability-metrics-custom-allowlist` ConfigMap in the `open-cluster-management-observability` namespace with the following command: `oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml`.

Verify that data from your custom metric is being collected by querying the metric from the *Explore* page, from the Grafana dashboard. You can also use the custom metrics in your own dashboard. For more information about viewing the dashboard, see xref:../observability/design_grafana.adoc#designing-your-grafana-dashboard[Designing your Grafana dashboard].

[#adding-user-workload-metrics]
=== Adding user workload metrics

You can collect {ocp-short} user-defined metrics from workloads in {ocp-short}. You must enable monitoring, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html/monitoring/enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects].

If you have a managed cluster with monitoring for user-defined workloads enabled, the user workloads are located in the `test` namespace and generate metrics. These metrics are collected by Prometheus from the {ocp-short} user workload.

Collect the metrics from the user workloads by creating a ConfigMap named, `observability-metrics-custom-allowlist` in the `test` namespace. View the following example:

[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-custom-allowlist
  namespace: test
data:
  uwl_metrics_list.yaml: |
    names:
      - sample_metrics
----

- The `uwl_metrics_list.yaml` is the key for the ConfigMap data.

- The value of the ConfigMap data is in YAML format. The `names` section includes the list of metric names, which you want to collect from the `test` namespace. After you create the ConfigMap, the specified metrics from the target namespace is collected by the observability collector and pushed to the hub cluster.

[#removing-default-metrics]
=== Removing default metrics

If you want data to not be collected in your managed cluster for a specific metric, remove the metric from the `observability-metrics-custom-allowlist.yaml` file. When you remove a metric, the metric data is not collected in your managed clusters. As mentioned previously, first verify that `mco observability` is enabled.

Add the name of the default metric to the `metrics_list.yaml` parameter with a hyphen `-` at the start of the metric name. For example, `-cluster_infrastructure_provider`.

Create the `observability-metrics-custom-allowlist` ConfigMap in the 
`open-cluster-management-observability` namespace with the following command: `oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml`.

Verify that the specific metric is not being collected from your managed clusters. When you query the metric from the Grafana dashboard, the metric is not displayed.

[#exporting-metrics-to-external-endpoints]
== Exporting metrics to external endpoints

You can customize observability to export the metrics to external endpoints, which support the Prometheus Remote-Write specification in real time. For more information, see link:https://prometheus.io/docs/concepts/remote_write_spec/[Prometheus Remote-Write specification].

[#creating-the-kubernetes-secret-for-external-endpoint]
=== Creating the Kubernetes secret for an external endpoint

You must create a Kubernetes secret with the access information of the external endpoint in the `open-cluster-management-observability` namespace. View the following example secret:

[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: victoriametrics
  namespace: open-cluster-management-observability
type: Opaque
stringData:
  ep.yaml: |
    url: http://victoriametrics:8428/api/v1/write
    http_client_config:
      basic_auth:
        username: test
        password: test
----

The `ep.yaml` is the key of the content and is used in the `MultiClusterObservability` custom resource in next step. Currently, observability supports exporting metrics to endpoints without any security checks, with basic authentication or with `tls` enablement. View the following tables for a full list of supported parameters:

[options="header", cols=".^2a,.^6a,.^4a"]
|===
|Name|Description|Schema
|**url** +
_required_|URL for the external endpoint.|string
|**http_client_config** +
_optional_|Advanced configuration for the HTTP client.|<<jsonmulticlusterobservability_httpclient,HttpClientConfig>>
|===

[[jsonmulticlusterobservability_httpclient]]
**HttpClientConfig**
[options="header", cols=".^2a,.^3a,.^4a"]
|===
|Name|Description|Schema
|**basic_auth** +
_optional_|HTTP client configuration for basic authentication.|<<jsonmulticlusterobservability_basicauth,BasicAuth>>
|**tls_config** +
_optional_|HTTP client configuration for TLS.|<<jsonmulticlusterobservability_tls,TLSConfig>>
|===

[[jsonmulticlusterobservability_basicauth]]
**BasicAuth**
[options="header", cols=".^2a,.^3a,.^4a"]
|===
|Name|Description|Schema
|**username** +
_optional_|User name for basic authorization.|string
|**password** +
_optional_|Password for basic authorization.|string
|===

[[jsonmulticlusterobservability_tls]]
**TLSConfig**
|===
|Name|Description|Schema
|**secret_name** +
_required_|Name of the secret that contains certificates.|string
|**ca_file_key** +
_optional_|Key of the CA certificate in the secret (only optional if **insecure_skip_verify** is set to true).|string
|**cert_file_key** +
_required_|Key of the client certificate in the secret.|string
|**key_file_key** +
_required_|Key of the client key in the secret.|string
|**insecure_skip_verify** +
_optional_|Parameter to skip the verification for target certificate.|bool
|===

[#updating-the-multiclusterobservability-cr]
=== Updating the _MultiClusterObservability_ custom resource

After you create the Kubernetes secret, you must update the `MultiClusterObservability` custom resource to add `writeStorage` in  the `spec.storageConfig` parameter. View the following example:

[source,yaml]
----
spec:
  storageConfig:
    writeStorage:
    - key: ep.yaml
      name: victoriametrics
----

The value for `writeStorage` is a list. You can add an item to the list when you want to export metrics to one external endpoint. If you add more than one item to the list, then the metrics are exported to multiple external endpoints. Each item contains two attributes: _name_ and _key_. _Name_ is the name of the Kubernetes secret that contains endpoint access information, and _key_ is the key of the content in the secret. View the following description table for the 

[#viewing-the-status-of-metrics-export]
=== Viewing the status of metric export

After the metrics export is enabled, you can view the status of metrics export by checking the `acm_remote_write_requests_total` metric. From the OpenShift console of your hub cluster, navigate to the _Metrics_ page by clicking *Metrics* in the _Observe_ section. 

Then query the `acm_remote_write_requests_total` metric. The value of that metric is the total number of requests with a specific response for one external endpoint, on one observatorium API instance. The `name` label is the name for the external endpoint. The `code` label is the return code of the HTTP request for the metrics export.

[#adding-advanced-config]
== Adding _advanced_ configuration

Add the `advanced` configuration section to update the retention for each observability component, according to your needs. 

Edit the `MultiClusterObservability` custom resource and add the `advanced` section with the following command: `oc edit mco observability -o yaml`. Your YAML file might resemble the following contents:

[source,yaml]
----
spec:
  advanced:
    retentionConfig:
      blockDuration: 2h
      deleteDelay: 48h
      retentionInLocal: 24h
      retentionResolutionRaw: 30d
      retentionResolution5m: 180d
      retentionResolution1h: 0d
    receive:
      resources:
        limits:
          memory: 4096Gi
      replicas: 3 
----

For descriptions of all the parameters that can added into the `advanced` configuration, see the link:../apis/observability.json.adoc[Observability API].

[#updating-replicas]
== Updating the _MultiClusterObservability_ custom resource replicas from the console

If your workload increases, increase the number of replicas of your observability pods. Navigate to the {ocp} console from your hub cluster. Locate the `MultiClusterObservability` custom resource, and update the `replicas` parameter value for the component where you want to change the replicas. Your updated YAML might resemble the following content:

[source,yaml]
----
spec:
   advanced:
      receive:
         replicas: 6
----

For more information about the parameters within the `mco observability` custom resource, see the link:../apis/observability.json.adoc#observability-api[Observability API].

[#customizing-route-cert]
== Customizing route certification

If you want to customize the {ocp-short} route certification, you must add the routes in the `alt_names` section. To ensure your {ocp-short} routes are accessible, add the following information: `alertmanager.apps.<domainname>`, `observatorium-api.apps.<domainname>`, `rbac-query-proxy.apps.<domainname>`.

*Note:* Users are responsible for certificate rotations and updates.

[#customizing-certificates-object-store]
== Customizing certificates for accessing the object store

Complete the following steps to customize certificates for accessing the object store:

. Edit the `http_config` section by adding the certificate in the object store secret. View the following example:
+
[source,yaml]
----
 thanos.yaml: |
    type: s3
    config:
      bucket: "thanos"
      endpoint: "minio:9000"
      insecure: false
      access_key: "minio"
      secret_key: "minio123"
      http_config:
        tls_config:
          ca_file: /etc/minio/certs/ca.crt
          insecure_skip_verify: false
----

. Add the object store secret in the `open-cluster-management-observability` namespace. The secret must contain the `ca.crt` that you defined in the previous secret example. If you want to enable Mutual TLS, you need to provide `public.crt`, and `private.key` in the previous secret. View the following example:
+
[source,yaml]
----
 thanos.yaml: |
    type: s3
    config:
      ...
      http_config:
        tls_config:
          ca_file: /etc/minio/certs/ca.crt <1>
          cert_file: /etc/minio/certs/public.crt
          key_file: /etc/minio/certs/private.key
          insecure_skip_verify: false
----
+
<1> The path to certificates and key values for the `thanos-object-storage` secret.

. Configure the secret name by updating the `TLSSecretName` parameter in the `MultiClusterObservability` custom resource. View the following example where the secret name is `tls-certs-secret`:
+
[source,yaml]
----
metricObjectStorage:
  key: thanos.yaml
  name: thanos-object-storage
  tlsSecretName: tls-certs-secret
----

This secret can be mounted in the `tlsSecretMountPath` resource of all components that need to access the object store, and it includes the following components: `receiver`, `store`, `ruler`, `compact`.

[#viewing-and-exploring-data]
== Viewing and exploring data

View the data from your managed clusters by accessing Grafana from the hub cluster. You can query specific alerts and add filters for the query. 

For example, to _cluster_infrastructure_provider_ from a single node cluster, use the following query expression: `cluster_infrastructure_provider{clusterType="SNO"}`

*Notes:* 

- Do not set the `ObservabilitySpec.resources.CPU.limits` parameter if observability is enabled on single node managed clusters. When you set the CPU limits, it causes the observability pod to be counted against the capacity for your managed cluster. See link:https://github.com/openshift/enhancements/blob/master/enhancements/workload-partitioning/management-workload-partitioning.md#management-workload-partitioning[Management Workload Partitioning] for more information.

[#view-historical-data]
=== Viewing historical data

When you query historical data, manually set your query parameter options to control how much data is displayed from the dashboard. Complete the following steps:

. From your hub cluster, select the *Grafana link* that is in the console header. 

. Edit your cluster dashboard by selecting *Edit Panel*.

. From the Query front-end data source in Grafana, click the _Query_ tab. 

. Select `$datasource`. 

. If you want to see more data, increase the value of the _Step_ parameter section. If the _Step_ parameter section is empty, it is automatically calculated.

. Find the _Custom query parameters_ field and select *`max_source_resolution=auto`*.

. To verify that the data is displayed, refresh your Grafana page.

Your query data appears from the Grafana dashboard.

[#viewing-etcd-grafana]
=== Viewing the etcd table

View the etcd table from the hub cluster dashboard in Grafana to learn the stability of the etcd as a data store. 

Select the Grafana link from your hub cluster to view the _etcd_ table data, which is collected from your hub cluster. The _Leader election changes_ across managed clusters are displayed.

[#viewing-cluster-fleet-service-level-overview-on-k8s-api-server-grafana]
=== Viewing the cluster fleet service-level overview for the Kubernetes API server dashboard

View the cluster fleet Kubernetes API service-level overview from the hub cluster dashboard in Grafana.

After you navigate to the Grafana dashboard, access the managed dashboard menu by selecting *Kubernetes* > *Service-Level Overview* > *API Server*. The _Fleet Overview_ and _Top Cluster_ details are displayed. 

View the total number of clusters that are exceeding or meeting the targeted _service-level objective_ (SLO) value for the past seven or 30-day period, offending and non-offending clusters, and API Server Request Duration.

[#viewing-cluster-service-level-overview-on-k8s-api-server-grafana]
=== Viewing the cluster service-level overview for the Kubernetes API server dashboard

View the Kubernetes API service-level overview table from the hub cluster dashboard in Grafana. 

After you navigate to the Grafana dashboard, access the managed dashboard menu by selecting *Kubernetes* > *Service-Level Overview* > *API Server*. The _Fleet Overview_ and _Top Cluster_ details are displayed.

View the error budget for the past seven or 30-day period, the remaining downtime, and trend.

[#disabling-observability]
== Disabling observability

You can disable observability, which stops data collection on the {product-title-short} hub cluster.

[#disabling-observability-on-all-clusters]
=== Disabling observability on all clusters

Disable observability by removing observability components on all managed clusters.

Update the `multicluster-observability-operator` resource by setting `enableMetrics` to `false`. Your updated resource might resemble the following change:

[source,yaml]
----
spec:
  imagePullPolicy: Always
  imagePullSecret: multiclusterhub-operator-pull-secret
  observabilityAddonSpec: # The ObservabilityAddonSpec defines the global settings for all managed clusters which have observability add-on enabled
    enableMetrics: false #indicates the observability addon push metrics to hub server
----

[#disabling-observability-on-a-single-cluster]
=== Disabling observability on a single cluster

Disable observability by removing observability components on specific managed clusters. Add the `observability: disabled` label to the `managedclusters.cluster.open-cluster-management.io` custom resource.

From the {product-title-short} console _Clusters_ page, add the `observability=disabled` label to the specified cluster.

*Note:* When a managed cluster with the observability component is detached, the `metrics-collector` deployments are removed.

To learn more about alert forwarding, see the link:https://prometheus.io/docs/alerting/latest/alertmanager/[Prometheus AlertManager documentation]. For more information about monitoring data from the console with the observability service, see xref:../observability/observe_environments_intro.adoc#observing-environments-intro[Observing environments introduction].
