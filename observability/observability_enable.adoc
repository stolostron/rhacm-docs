[#enabling-observability-service]
= Enabling observability service
//i need to either rename the title or rename the Enabling observability section on line 9; modular changes handled later | MJ | 07/03/23
Monitor the health of your managed clusters with the observability service (`multicluster-observability-operator`).

*Required access:* Cluster administrator, the `open-cluster-management:cluster-manager-admin` role, or S3 administrator.

* <<prerequisites-observability,Prerequisites>>
* <<enabling-observability,Enabling observability>>
- <<verifying-thanos-version,Verifying the Thanos version>>
* <<creating-mco-cr,Creating the MultiClusterObservability custom resource>>
* <<enabling-observability-ocp,Enabling observability from the {ocp} console>>
* <<external-metric-query,Using the external metric query>>
* <<disabling-observability-resource,Disabling observability>>

[#prerequisites-observability]
== Prerequisites
 
- You must install {product-title}. See link:../install/install_connected.adoc#installing-while-connected-online[Installing while connected online] for more information.
- You must define a storage class in the `MultiClusterObservability` custom resource, if there is no default storage class specified.
- Direct network access to the hub cluster is required. Network access to load balancers and proxies are not supported. For more information, see link:../networking/networking_intro.adoc#networking[Networking].
- You must configure an object store to create a storage solution. {product-title-short} supports the following cloud providers with stable object stores:

* link:https://aws.amazon.com/getting-started/hands-on/lightsail-object-storage/[Amazon Web Services S3 (AWS S3)]
* link:https://www.redhat.com/en/technologies/storage/ceph[Red Hat Ceph (S3 compatible API)]
* link:https://cloud.google.com/storage[Google Cloud Storage]
* link:https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction[Azure storage]
* link:https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation[Red Hat OpenShift Data Foundation (formerly known as Red Hat OpenShift Container Storage)]
* link:https://www.ibm.com/docs/en/baw/20.x?topic=storage-preparing-cloud-public-roks[Red Hat OpenShift on IBM (ROKS)]
+
*Important:* When you configure your object store, ensure that you meet the encryption requirements that are necessary when sensitive data is persisted. The observability service uses Thanos supported, stable object stores.

[#enabling-observability]
== Enabling observability

Enable the observability service by creating a `MultiClusterObservability` custom resource instance. Before you enable observability, see xref:../observability/observe_environments.adoc#observability-pod-capacity-requests[Observability pod capacity requests] for more information. 

*Note:* When observability is enabled or disabled on {ocp-short} managed clusters that are managed by {product-title-short}, the observability endpoint operator updates the `cluster-monitoring-config` ConfigMap by adding additional `alertmanager` configuration that automatically restarts the local Prometheus.

Complete the following steps to enable the observability service: 
 
. Log in to your {product-title-short} hub cluster. 
. Create a namespace for the observability service with the following command:
+
----
oc create namespace open-cluster-management-observability
----

. Generate your pull-secret. If {product-title-short} is installed in the `open-cluster-management` namespace, run the following command:
 
+
----
DOCKER_CONFIG_JSON=`oc extract secret/multiclusterhub-operator-pull-secret -n open-cluster-management --to=-`
----
+
If the `multiclusterhub-operator-pull-secret` is not defined in the namespace, copy the `pull-secret` from the `openshift-config` namespace into the `open-cluster-management-observability` namespace. Run the following command:
+
----
DOCKER_CONFIG_JSON=`oc extract secret/pull-secret -n openshift-config --to=-`
----
+
Then, create the pull-secret in the `open-cluster-management-observability` namespace, run the following command:
+
----
oc create secret generic multiclusterhub-operator-pull-secret \
    -n open-cluster-management-observability \
    --from-literal=.dockerconfigjson="$DOCKER_CONFIG_JSON" \
    --type=kubernetes.io/dockerconfigjson
----

. Create a secret for your object storage for your cloud provider. Your secret must contain the credentials to your storage solution. For example, run the following command:
+
----
oc create -f thanos-object-storage.yaml -n open-cluster-management-observability
----
+
View the following examples of secrets for the supported object stores:

** For Amazon S3 or S3 compatible, your secret might resemble the following file:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: thanos-object-storage
  namespace: open-cluster-management-observability
type: Opaque
stringData:
  thanos.yaml: |
    type: s3
    config:
      bucket: YOUR_S3_BUCKET
      endpoint: YOUR_S3_ENDPOINT
      insecure: true
      access_key: YOUR_ACCESS_KEY
      secret_key: YOUR_SECRET_KEY
----
+
For more details, see link:https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html[Amazon Simple Storage Service user guide].

** For Google, your secret might resemble the following file: 
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: thanos-object-storage
  namespace: open-cluster-management-observability
type: Opaque
stringData:
  thanos.yaml: |
    type: GCS
    config:
      bucket: YOUR_GCS_BUCKET
      service_account: YOUR_SERVICE_ACCOUNT
----
+
For more details, see link:https://cloud.google.com/storage/docs/introduction[Google Cloud Storage].

** For Azure your secret might resemble the following file:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: thanos-object-storage
  namespace: open-cluster-management-observability
type: Opaque
stringData:
  thanos.yaml: |
    type: AZURE
    config:
      storage_account: YOUR_STORAGE_ACCT
      storage_account_key: YOUR_STORAGE_KEY
      container: YOUR_CONTAINER
      endpoint: blob.core.windows.net
      max_retries: 0
----
+
For more details, see link:https://docs.microsoft.com/en-us/azure/storage/[Azure Storage documentation].
+
*Note:* If you use Azure as an object storage for a {ocp} cluster, the storage account associated with the cluster is not supported. You must create a new storage account.

** For Red Hat OpenShift Data Foundation, your secret might resemble the following file:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: thanos-object-storage
  namespace: open-cluster-management-observability
type: Opaque
stringData:
  thanos.yaml: |
    type: s3
    config:
      bucket: YOUR_RH_DATA_FOUNDATION_BUCKET
      endpoint: YOUR_RH_DATA_FOUNDATION_ENDPOINT
      insecure: false
      access_key: YOUR_RH_DATA_FOUNDATION_ACCESS_KEY
      secret_key: YOUR_RH_DATA_FOUNDATION_SECRET_KEY
----
+
For more details, see link:https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation[Red Hat OpenShift Data Foundation]. For Red Hat OpenShift on IBM (ROKS), your secret might resemble the following file:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: thanos-object-storage
  namespace: open-cluster-management-observability
type: Opaque
stringData:
  thanos.yaml: |
    type: s3
    config:
      bucket: YOUR_ROKS_S3_BUCKET
      endpoint: YOUR_ROKS_S3_ENDPOINT
      insecure: true
      access_key: YOUR_ROKS_ACCESS_KEY
      secret_key: YOUR_ROKS_SECRET_KEY
----
+
For more details, follow the IBM Cloud documentation, link:https://cloud.ibm.com/objectstorage/create[Cloud Object Storage]. Be sure to use the service credentials to connect with the object storage. For more details, follow the IBM Cloud documentation, link:https://cloud.ibm.com/objectstorage/create%5BCloud[Cloud Object Store] and link:https://cloud.ibm.com/docs/cloud-object-storage/iam?topic=cloud-object-storage-service-credentials%5BService[Service Credentials].

** For Amazon S3 or S3 compatible storage, you can also use short term, limited-privilege credentials generated with AWS Security Token Service (AWS STS). Refer to link:https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html[AWS Security Token Service documentation] for more details.
+
Generating access keys using AWS Security Service require the following additional steps:

- Create an IAM policy that limits access to an S3 bucket
- Create an IAM role with a trust policy to generate JWT tokens for {ocp-short} service accounts
- Specify annotations for the observability service accounts that requires access to the S3 bucket. You can find an example of how observability on Red Hat OpenShift Service on AWS (ROSA) cluster can be configured to work with AWS STS tokens in the _Set environment_ step. See link:https://www.rosaworkshop.io/[Red Hat OpenShift Service on AWS (ROSA)] for more details, along with link:https://www.rosaworkshop.io/rosa/15-sts_explained/[ROSA with STS explained] for an in-depth description of the requirements and setup to use STS tokens.

Complete the following steps to generate access keys using the AWS Security Service:

. Set up the AWS environment. Run the following commands:
+
[source,bash]
----
export POLICY_VERSION=$(date +"%m-%d-%y")
export TRUST_POLICY_VERSION=$(date +"%m-%d-%y") 
export CLUSTER_NAME=<my-cluster>
export S3_BUCKET=$CLUSTER_NAME-acm-observability
export REGION=us-east-2
export NAMESPACE=open-cluster-management-observability
export SA=tbd
export SCRATCH_DIR=/tmp/scratch
export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e "s/^https:\/\///")
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
export AWS_PAGER=""
rm -rf $SCRATCH_DIR
mkdir -p $SCRATCH_DIR
----

. Create an S3 bucket with the following command:
+
[source,bash]
----
aws s3 mb s3://$S3_BUCKET
----

. Create a `s3-policy` JSON file for access to your S3 bucket. Run the following command:
+
[source,json]
----
{
    "Version": "$POLICY_VERSION",
    "Statement": [
        {
            "Sid": "Statement",
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:PutObject",
                "s3:PutObjectAcl",
                "s3:CreateBucket",
                "s3:DeleteBucket"
            ],
            "Resource": [
                "arn:aws:s3:::$S3_BUCKET/*",
                "arn:aws:s3:::$S3_BUCKET"
            ]
        }
    ]
 }
----

. Apply the policy with the following command:
+
----
S3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-acm-obs \
--policy-document file://$SCRATCH_DIR/s3-policy.json \
--query 'Policy.Arn' --output text)
echo $S3_POLICY
----

. Create a `TrustPolicy` JSON file. Run the following command:
+
[source,json]
----
{
 "Version": "$TRUST_POLICY_VERSION",
 "Statement": [
   {
     "Effect": "Allow",
     "Principal": {
       "Federated": "arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}"
     },
     "Action": "sts:AssumeRoleWithWebIdentity",
     "Condition": {
       "StringEquals": {
         "${OIDC_PROVIDER}:sub": [
           "system:serviceaccount:${NAMESPACE}:observability-thanos-query",
           "system:serviceaccount:${NAMESPACE}:observability-thanos-store-shard",
           "system:serviceaccount:${NAMESPACE}:observability-thanos-compact"
           "system:serviceaccount:${NAMESPACE}:observability-thanos-rule",
           "system:serviceaccount:${NAMESPACE}:observability-thanos-receive",
         ]
       }
     }
   }
 ]
}
----

. Create a role for AWS Prometheus and CloudWatch with the following command:
+
----
S3_ROLE=$(aws iam create-role \
  --role-name "$CLUSTER_NAME-acm-obs-s3" \
  --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \
  --query "Role.Arn" --output text)
echo $S3_ROLE
----

. Attach the policies to the role. Run the following command:
+
----
aws iam attach-role-policy \
  --role-name "$CLUSTER_NAME-acm-obs-s3" \
  --policy-arn $S3_POLICY
----
+
Your secret might resemble the following file. The `config` section specifies `signature_version2: false` and does not specify `access_key` and `secret_key`:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: thanos-object-storage
  namespace: open-cluster-management-observability
type: Opaque
stringData:
  thanos.yaml: |
 type: s3
 config:
   bucket: $S3_BUCKET
   endpoint: s3.$REGION.amazonaws.com
   signature_version2: false
----

. Specify service account annotations when you the `MultiClusterObservability` custom resource as described in _Creating the MultiClusterObservability custom resource_ section. 

. You can retrieve the S3 access key and secret key for your cloud providers with the following commands. You must decode, edit, and encode your `base64` string in the secret:
+
----
YOUR_CLOUD_PROVIDER_ACCESS_KEY=$(oc -n open-cluster-management-observability get secret <object-storage-secret> -o jsonpath="{.data.thanos\.yaml}" | base64 --decode | grep access_key | awk '{print $2}')

echo $ACCESS_KEY

YOUR_CLOUD_PROVIDER_SECRET_KEY=$(oc -n open-cluster-management-observability get secret <object-storage-secret> -o jsonpath="{.data.thanos\.yaml}" | base64 --decode | grep secret_key | awk '{print $2}')

echo $SECRET_KEY
----

. Verify that observability is enabled by checking the pods for the following deployments and stateful sets. You might receive the following information:
+
----
observability-thanos-query (deployment)
observability-thanos-compact (statefulset)
observability-thanos-receive-default  (statefulset)
observability-thanos-rule   (statefulset)
observability-thanos-store-shard-x  (statefulsets)
----

[#verifying-thanos-version]
=== Verifying the Thanos version

Verify the Thanos version from the command line interface (CLI). 

After you log in to your hub cluster, run the following command in the observability pods to receive the Thanos version:

----
thanos --version
----

The Thanos version is displayed.

[#creating-mco-cr]
=== Creating the MultiClusterObservability custom resource

Complete the following steps to create the `MultiClusterObservability` custom resource on your hub cluster:

. Create the `MultiClusterObservability` custom resource YAML file named `_multiclusterobservability_cr.yaml_`. 
+
View the following default YAML file for observability:
+
[source,yaml]
----
apiVersion: observability.open-cluster-management.io/v1beta2
kind: MultiClusterObservability
metadata:
  name: observability
spec:
  observabilityAddonSpec: {}
  storageConfig:
    metricObjectStorage:
      name: thanos-object-storage
      key: thanos.yaml
----
+
You might want to modify the value for the `retentionConfig` parameter in the `advanced` section. For more information, see link:https://thanos.io/v0.8/components/compact/#downsampling-resolution-and-retention[Thanos Downsampling resolution and retention]. Depending on the number of managed clusters, you might want to update the amount of storage for stateful sets. If your S3 bucket is configured to use STS tokens, annotate the service accounts to use STS with S3 role. View the following configuration:
+
[source,yaml]
----
spec:
  advanced:
    compact:
      eks.amazonaws.com/role-arn=$S3_ROLE
    store:
      eks.amazonaws.com/role-arn=$S3_ROLE
    rule:
      eks.amazonaws.com/role-arn=$S3_ROLE
    receive:
      eks.amazonaws.com/role-arn=$S3_ROLE
    query:
      eks.amazonaws.com/role-arn=$S3_ROLE
----
+
See link:../apis/observability.json.adoc#observability-api[Observability API] for more information.
+
. To deploy on infrastructure machine sets, you must set a label for your set by updating the `nodeSelector` in the `MultiClusterObservability` YAML. Your YAML might resemble the following content:

+
----
  nodeSelector:
    node-role.kubernetes.io/infra: 
----
+
For more information, see link:https://docs.openshift.com/container-platform/4.11/machine_management/creating-infrastructure-machinesets.html[Creating infrastructure machine sets].

. Apply the observability YAML to your cluster by running the following command:
+
----
oc apply -f multiclusterobservability_cr.yaml
----
+
All the pods in `open-cluster-management-observability` namespace for Thanos, Grafana and Alertmanager are created. All the managed clusters connected to the {product-title-short} hub cluster are enabled to send metrics back to the {product-title-short} Observability service.

. Validate that the observability service is enabled and the data is populated by launching the Grafana dashboards. Click the **Grafana link** that is near the console header, from either the console _Overview_ page or the _Clusters_ page.
+
*Note:* If you want to exclude specific managed clusters from collecting the observability data, add the following cluster label to your clusters: `observability: disabled`.

The observability service is enabled. After you enable the observability service, the following functions are initiated:

- All the alert managers from the managed clusters are forwarded to the {product-title-short} hub cluster.
- All the managed clusters that are connected to the {product-title-short} hub cluster are enabled to send alerts back to the {product-title-short} observability service. You can configure the {product-title-short} Alertmanager to take care of deduplicating, grouping, and routing the alerts to the correct receiver integration such as email, PagerDuty, or OpsGenie. You can also handle silencing and inhibition of the alerts.
+
*Note:* Alert forwarding to the {product-title-short} hub cluster feature is only supported by managed clusters with {ocp} version 4.8 or later. After you install {product-title-short} with observability enabled, alerts from {ocp-short} v4.8 and later are automatically forwarded to the hub cluster. See xref:../observability/customize_observability.adoc#forward-alerts[Forwarding alerts] to learn more.

* Access the {ocp-short} 3.11 Grafana dashboards with the following URL: `https://$ACM_URL/grafana/dashboards`. Select the folder named _OCP 3.11_ to view the {ocp-short} 3.11 dashboards.

[#enabling-observability-ocp]
== Enabling observability from the {ocp} console

Optionally, you can enable observability from the {ocp} console, create a project named `open-cluster-management-observability`. Be sure to create an image pull-secret named, `multiclusterhub-operator-pull-secret` in the `open-cluster-management-observability` project.

Create your object storage secret named, `thanos-object-storage` in the `open-cluster-management-observability` project. Enter the object storage secret details, then click *Create*. See step four of the _Enabling observability_ section to view an example of a secret.

Create the `MultiClusterObservability` custom resource instance. When you receive the following message, the obseravbility service is enabled successfully from {ocp-short}: `Observability components are deployed and running`.

[#external-metric-query]
=== Using the external metric query

Observability provides an external API for metrics to be queried through the OpenShift route, `rbac-query-proxy`. View the following tasks to use `rbac-query-proxy` route:

* You can get the details of the route with the following command:
+
----
oc get route rbac-query-proxy -n open-cluster-management-observability
----

* To access the `rbac-query-proxy` route, you must have an OpenShift OAuth access token. The token should be associated with a user or service account, which has permission to get namespaces. For more information, see link:https://docs.openshift.com/container-platform/4.11/authentication/managing-oauth-access-tokens.html[Managing user-owned OAuth access tokens].

* Get the default CA certificate and store the content of the key `tls.crt` in a local file. Run the following command:
+
----
oc -n openshift-ingress get secret router-certs-default -o jsonpath="{.data.tls\.crt}" | base64 -d > ca.crt
----

* Run the following command to query metrics:
+
----
curl --cacert ./ca.crt -H "Authorization: Bearer {TOKEN}" https://{PROXY_ROUTE_URL}/api/v1/query?query={QUERY_EXPRESSION}
----
+
*Note:* The `QUERY_EXPRESSION` is the standard Prometheus query expression. For example, query the metrics `cluster_infrastructure_provider` by replacing the URL in the previously mentioned command with the following URL: `https://{PROXY_ROUTE_URL}/api/v1/query?query=cluster_infrastructure_provider`. For more details, see link:https://prometheus.io/docs/prometheus/latest/querying/basics/[Querying Prometheus].
//IS THIS ACCURATE? THIS FILE WAS REMOVED
//* You can also replace certificates for the `rbac-query-proxy` route. See link:../governance/cert_mgmt_ingress.adoc#openssl-commands-for-generating-a-certificate[OpenSSL commands for generating a certificate] to create certificates. When you customize the `csr.cnf`, update the `DNS.1` to the hostname for the `rbac-query-proxy` route.

** Run the following command to create `proxy-byo-ca` and `proxy-byo-cert` secrets using the generated certificates:
+
----
oc -n open-cluster-management-observability create secret tls proxy-byo-ca --cert ./ca.crt --key ./ca.key

oc -n open-cluster-management-observability create secret tls proxy-byo-cert --cert ./ingress.crt --key ./ingress.key
----

[#dynamic-metrics-for-sno]
=== Dynamic metrics for single-node OpenShift clusters

Dynamic metrics collection supports automatic metric collection based on certain conditions. By default, a SNO cluster does not collect pod and container resource metrics. Once a SNO cluster reaches a specific level of resource consumption, the defined granular metrics are collected dynamically. When the cluster resource consumption is consistently less than the threshold for a period of time, granular metric collection stops.

The metrics are collected dynamically based on the conditions on the managed cluster specified by a collection rule. Because these metrics are collected dynamically, the following {product-title-short} Grafana dashboards do not display any data. When a collection rule is activated and the corresponding metrics are collected, the following panels display data for the duration of the time that the collection rule is initiated:

* Kubernetes/Compute Resources/Namespace (Pods)
* Kubernetes/Compute Resources/Namespace (Workloads)
* Kubernetes/Compute Resources/Nodes (Pods)
* Kubernetes/Compute Resources/Pod
* Kubernetes/Compute Resources/Workload

A collection rule includes the following conditions:

* A set of metrics to collect dynamically.
* Conditions written as a PromQL expression.
* A time interval for the collection, which must be set to `true`.
* A match expression to select clusters where the collect rule must be evaluated.

By default, collection rules are evaluated continuously on managed clusters every 30 seconds, or at a specific time interval. The lowest value between the collection interval and time interval takes precedence. Once the collection rule condition persists for the duration specified by the `for` attribute, the collection rule starts and the metrics specified by the rule are automatically collected on the managed cluster. Metrics collection stops automatically after the collection rule condition no longer exists on the managed cluster, at least 15 minutes after it starts.

The collection rules are grouped together as a parameter section named `collect_rules`, where it can be enabled or disabled as a group. {product-title-short} installation includes the collection rule group, `SNOResourceUsage` with two default collection rules: `HighCPUUsage` and `HighMemoryUsage`. The `HighCPUUsage` collection rule begins when the node CPU usage exceeds 70%. The `HighMemoryUsage` collection rule begins if the overall memory utilization of the SNO cluster exceeds 70% of the available node memory. Currently, the previously mentioned thresholds are fixed and cannot be changed. When a collection rule begins for more than the interval specified by the `for` attribute, the system automatically starts collecting the metrics that are specified in the `dynamic_metrics` section.

View the list of dynamic metrics that from the `collect_rules` section, in the following YAML file:

[source,yaml]
----
collect_rules:
  - group: SNOResourceUsage
    annotations:
      description: >
        By default, a SNO cluster does not collect pod and container resource metrics. Once a SNO cluster 
        reaches a level of resource consumption, these granular metrics are collected dynamically. 
        When the cluster resource consumption is consistently less than the threshold for a period of time, 
        collection of the granular metrics stops.
    selector:
      matchExpressions:
        - key: clusterType
          operator: In
          values: ["SNO"]
    rules:
    - collect: SNOHighCPUUsage
      annotations:
        description: >
          Collects the dynamic metrics specified if the cluster cpu usage is constantly more than 70% for 2 minutes
      expr: (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))) * 100 > 70
      for: 2m
      dynamic_metrics:
        names:
          - container_cpu_cfs_periods_total
          - container_cpu_cfs_throttled_periods_total
          - kube_pod_container_resource_limits 
          - kube_pod_container_resource_requests   
          - namespace_workload_pod:kube_pod_owner:relabel 
          - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate 
          - node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate 
    - collect: SNOHighMemoryUsage
      annotations:
        description: >
          Collects the dynamic metrics specified if the cluster memory usage is constantly more than 70% for 2 minutes
      expr: (1 - sum(:node_memory_MemAvailable_bytes:sum) / sum(kube_node_status_allocatable{resource=\"memory\"})) * 100 > 70
      for: 2m
      dynamic_metrics:
        names:
          - kube_pod_container_resource_limits 
          - kube_pod_container_resource_requests 
          - namespace_workload_pod:kube_pod_owner:relabel
        matches:
          - __name__="container_memory_cache",container!=""
          - __name__="container_memory_rss",container!=""
          - __name__="container_memory_swap",container!=""
          - __name__="container_memory_working_set_bytes",container!=""
----

A `collect_rules.group` can be disabled in the `custom-allowlist` as shown in the following example. When a `collect_rules.group` is disabled, metrics collection reverts to the previous behavior. These metrics are collected at regularly, specified intervals:

[source,yaml]
----
collect_rules:
  - group: -SNOResourceUsage
---- 

The data is only displayed in Grafana when the rule is initiated.

[#disabling-observability-resource]
== Disabling observability

To disable the observability service, uninstall the `observability` resource. From the {ocp-short} console navigation, select *Operators* > *Installed Operators* > *Advanced Cluster Manager for Kubernetes*. Remove the `MultiClusterObservability` custom resource.

To learn more about customizing the observability service, see xref:../observability/customize_observability.adoc#customizing-observability[Customizing observability].


