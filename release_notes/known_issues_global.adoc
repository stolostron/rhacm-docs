[#known-issues-global-hub]
= Multicluster global hub Operator known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for the {global-hub} Operator. The following list contains known issues for this release, or known issues that continued from the previous release. For your {ocp-short} cluster, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/release_notes/ocp-4-12-release-notes#ocp-4-12-known-issues[{ocp-short} known issues].

[#kafka-topics-after-upgrade]
== The {global-hub} operand does not use your Kafka topics

There are three topics named `spec`, `status` and `event` in your own Kafka that you brought. After upgrading {global-hub} to 1.3 or later, only the `spec` and `status` topics remain. Configure these topics through the {global-hub} operand. The default topis for your transport that your brought are `gh-spec` and `gh-status`. 

The {global-hub} operand is the API or resource of your system, and the {global-hub} operator is the controller. After upgrading {global-hub}, align the topics between your own Kafka clusters and the {global-hub} operand. You can either create the `gh-spec` and `gh-status` topic in your Kafka cluster, or you can change the {global-hub} operand to use the existing `spec` and `status` topics.

Use the following YAML sample to update your {global-hub} operand to use your own Kafka topics:

[source,yaml]
----
apiVersion: operator.open-cluster-management.io/v1alpha4
kind: MulticlusterGlobalHub
spec:
  availabilityConfig: Basic
  dataLayer:
    kafka:
      topics:
        specTopic: spec
        statusTopic: status
    postgres:
      retention: 18m
  enableMetrics: false
----

[#granfa-cannot-open]
== The {global-hub} Grafana console cannot open in a FIPS-enabled environment

If {global-hub} is running on the latest {ocp-short} environment that is FIPS-enabled, you cannot access the Grafana console due to the wrong `oauth-proxy` image. {acm-short} 2.10.x supports the latest {ocp-short} version, enabling you to get the `oauth-proxy` image from {acm-short} 2.10.x. 

To access your Grafana console, manually update the `oauth-proxy` image with the {acm-short} bundle image. To update the `oauth-proxy` image, complete the following steps:

. Get the correct `oauth-proxy` image from the `mch-image-manifest-xxx` by running the following command. Replace `<.data.oauth_proxy_latestocp>` with your {ocp-short} version:

+
[source,bash]
----
oc get cm -n open-cluster-management mch-image-manifest-xxx -ojsonpath=<.data.oauth_proxy_latestocp> 
----

. Update the deployment image in the {global-hub} `clusterserviceversion` (CSV) with the correct image by running the following command: 

+
[source,bash]
----
oc edit csv multicluster-global-hub-operator-rh.v1.1.x -n multicluster-global-hub 
----

. Find the value for `RELATED_IMAGE_OAUTH_PROXY` and replace it with the output you received from Step 1. 

[#kafka-operator-keeps-restarting]
== Kafka operator keeps restarting 

In the Federal Information Processing Standard (FIPS) environment, the Kafka operator keeps restarting because of the out-of-memory (OOM) state. To fix this issue, set the resource limit to at least `512M`. For detailed steps on how to set this limit, see link:https://access.redhat.com/documentation/en-us/red_hat_amq_streams/2.6/html/deploying_and_managing_amq_streams_on_openshift/deploy-intro_str#assembly-fips-support-str[amq stream doc].

[#backup-and-restore-known-issues]
== Backup and restore known issues 

If your original {global-hub} cluster crashes, the {global-hub} loses its generated events and `cron` jobs. Even if you restore the new {global-hub} cluster, the events and `cron` jobs are not restored. To workaround this issue, you can manually run the `cron` job, see link:https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.9/html/multicluster_global_hub/multicluster-global-hub#global-hub-compliance-manual[Running the summarization process manually].

[#managed-cluster-not-counted]
== Managed cluster displays but is not counted

A managed cluster that is not created successfully, meaning `clusterclaim id.k8s.io` does not exist in the managed cluster, is not counted in the policy compliance dashboards, but shows in the policy console. 

[#operator-hyperlink]
== The {global-hub} is installed on {ocp-short} 4.13 hyperlinks might redirect home

If the {global-hub} Operator is installed on {ocp-short} 4.13, all hyperlinks that link to the managed clusters list and detail pages in dashboards might redirect to the {acm-short} home page. 

You need to manually go to your target page.

[#no-new-page-group-filter]
== The standard group filter cannot pass to the new page

In the *Global Hub Policy Group Compliancy Overview* hub dashboards, you can check one data point by clicking **View Offending Policies for standard group**, but after you click this link to go to the offending page, the standard group filter cannot pass to the new page. 

This is also an issue for the **Cluster Group Compliancy Overview**.

[#cannot-redirect-ocp-cluster-obs]
== Cannot redirect to {ocp-short} 3.11 cluster _Observability_ page

If a managed hub cluster imports an {ocp-short} 3.11 cluster (deprecated) as managed cluster, it cannot redirect to the _Observability_ page in the *Global Hub* > *Overview* dashboard.

You need to manually navigate to your target page.
