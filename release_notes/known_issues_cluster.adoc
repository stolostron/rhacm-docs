[#known-issues-cluster]
= Cluster management known issues and limitations

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for cluster management with {acm-short}. The following list contains known issues and limitations for this release, or known issues that continued from the previous release.

For cluster lifecycle known issues, see link:../clusters/release_notes/known_issues.adoc#known-issues-mce[Cluser lifecycle known issues and limitations] in the {mce-short} documentation.

[#hub-cluster-one-way-limits]
== Hub cluster communication limitations
//2.9:ACM-6292

The following limitations occur if the hub cluster is not able to reach or communicate with the managed cluster:

- You cannot create a new managed cluster by using the console. You are still able to import a managed cluster manually by using the command line interface or by using the *Run import commands manually* option in the console.
- If you deploy an Application or ApplicationSet by using the console, or if you import a managed cluster into ArgoCD, the hub cluster ArgoCD controller calls the managed cluster API server. You can use AppSub or the ArgoCD pull model to work around the issue.
- The console page for pod logs does not work, and an error message that resembles the following appears:
+
----
Error querying resource logs:
Service unavailable
----

[#local-cluster-auto]
== The local-cluster might not be automatically recreated
//2.4:17790

If the local-cluster is deleted while `disableHubSelfManagement` is set to `false`, the local-cluster is recreated by the `MulticlusterHub` operator. After you detach a local-cluster, the local-cluster might not be automatically recreated. 

- To resolve this issue, modify a resource that is watched by the `MulticlusterHub` operator. See the following example:

+
----
oc delete deployment multiclusterhub-repo -n <namespace>
----

- To properly detach the local-cluster, set the `disableHubSelfManagement` to true in the `MultiClusterHub`.


[#cluster-local-offline-reimport]
== Local-cluster status offline after reimporting with a different name
//2.4:16977

When you accidentally try to reimport the cluster named `local-cluster` as a cluster with a different name, the status for `local-cluster` and for the reimported cluster display `offline`.

To recover from this case, complete the following steps:

. Run the following command on the hub cluster to edit the setting for self-management of the hub cluster temporarily:
+
----
oc edit mch -n open-cluster-management multiclusterhub
----

. Add the setting `spec.disableSelfManagement=true`.

. Run the following command on the hub cluster to delete and redeploy the local-cluster:
+
----
oc delete managedcluster local-cluster
----

. Enter the following command to remove the `local-cluster` management setting: 
+
----
oc edit mch -n open-cluster-management multiclusterhub
----

. Remove `spec.disableSelfManagement=true` that you previously added.

[#hub-managed-clusters-clock]
== Hub cluster and managed clusters clock not synced
// 2.1:5636

Hub cluster and manage cluster time might become out-of-sync, displaying in the console `unknown` and eventually `available` within a few minutes. Ensure that the {ocp-short} hub cluster time is configured correctly. See link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html/installation_configuration/installing-customizing[Customizing nodes].

