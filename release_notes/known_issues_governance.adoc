[#known-issues-governance]
= Governance known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for Governance. The following list contains known issues for this release, or known issues that continued from the previous release. 

For your {ocp} cluster, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/release_notes/ocp-4-12-release-notes#ocp-4-12-known-issues[{ocp-short} known issues]. 

For more about deprecations and removals, see xref:../release_notes/deprecate_remove.adoc#deprecations-removals[Deprecations and removals].

[#grc-resources-not-cleaned-up]
== Governance resources not cleaned up properly when the component is disabled 
//2.9:8550

Governance resources are not cleaned up properly. When the component is set to `false` or is disabled in the `MultiClusterHub` operator, the governance component is removed before it can clean up the add-ons that it manages. 

[#unable-to-log-out]
== Unable to log out from Red Hat Advanced Cluster Management

When you use an external identity provider to log in to {product-title-short}, you might not be able to log out of {product-title-short}. This occurs when you use {product-title-short}, installed with IBM Cloud and Keycloak as the identity providers.

You must log out of the external identity provider before you attempt to log out of {product-title-short}. 

[#gatekeeper-upgrade]
== Gatekeeper operator installation fails
//2.4:16673

When you install the gatekeeper operator on {ocp} version 4.9, the installation fails. Before you upgrade {ocp-short} to version 4.9.0., you must upgrade the gatekeeper operator to version 0.2.0. See link:../governance/create_gatekeeper.adoc#upgrading-gatekeeper-gatekeeper-operator[Upgrading gatekeeper and the gatekeeper operator] for more information.

[#config-policy-stuck]
== Configuration policy listed complaint when namespace is stuck in _Terminating_ state
//2.2:20715

When you have a configuration policy that is configured with `mustnothave` for the `complianceType` parameter and `enforce` for the `remediationAction` parameter, the policy is listed as compliant after a deletion request is made to the Kubernetes API. Therefore, the Kubernetes object can be stuck in a `Terminating` state while the policy is listed as compliant.

[#operators-deployed-with-policies]
== Operators deployed with policies do not support ARM

While installation into an ARM environment is supported, operators that are deployed with policies might not support ARM environments. The following policies that install operators do not support ARM environments:

* link:https://github.com/stolostron/policy-collection/blob/main/stable/SI-System-and-Information-Integrity/policy-imagemanifestvuln.yaml[{product-title-short} policy for the Quay Container Security Operator]
* link:https://github.com/stolostron/policy-collection/blob/main/stable/CA-Security-Assessment-and-Authorization/policy-compliance-operator-install.yaml[{product-title-short} policy for the Compliance Operator]

[#configurationpolicy-crd-terminating]
== ConfigurationPolicy CRD is stuck in terminating

When you remove the `config-policy-controller` add-on from a managed cluster by disabling the policy controller in the `KlusterletAddonConfig` or by detaching the cluster, the `ConfigurationPolicy` CRD might get stuck in a terminating state. If the `ConfigurationPolicy` CRD is stuck in a terminating state, new policies might not be added to the cluster if the add-on is reinstalled later. You can also receive the following error:

----
template-error; Failed to create policy template: create not allowed while custom resource definition is terminating
----

Use the following command to check if the CRD is stuck: 

----
oc get crd configurationpolicies.policy.open-cluster-management.io -o=jsonpath='{.metadata.deletionTimestamp}'
----

If a deletion timestamp is on the resource, the CRD is stuck. To resolve the issue, remove all finalizers from configuration policies that remain on the cluster. Use the following command on the managed cluster and replace `<cluster-namespace>` with the managed cluster namespace:

----
oc get configurationpolicy -n <cluster-namespace> -o name | xargs oc patch -n <cluster-namespace> --type=merge -p '{"metadata":{"finalizers": []}}'
----

The configuration policy resources are automatically removed from the cluster and the CRD exits its terminating state. If the add-on has already been reinstalled, the CRD is recreated automatically without a deletion timestamp.

[#pruneobjbeh-not-working-existing-config-policy]
== _pruneObjectBehavior_ does not work when modifying existing configuration policy
//2.6:25261
//2.7+2.8:5939

When you modify an existing configuration policy, `pruneObjectBehavior` does not work. View the following reasons why `pruneObjectBehavior` might not work:

- If you set `pruneObjectBehavior` to `DeleteAll` or `DeleteIfCreated` in a configuration policy, old resources that were created before modifying are not cleaned correctly. Only new resources from policy creations and policy updates are tracked and deleted when you delete the configuration policy.

- If you set `pruneObjectBehavior` to `None` or do not set the parameter value, old objects might be unintentionally deleted on the managed cluster. Specifically, this occurs when a user changes the `name`, `namespace`, `kind`, or `apiversion` in the template. The parameter fields can dynamically change when the `object-templates-raw` or `namespaceSelector` parameters change. 

[#policy-status-repeated-updates]
== Policy status shows repeated updates when enforced

If a policy is set to `remediationAction: enforce` and is repeatedly updated, the {product-title-short} console shows repeated violations with successful updates. See the following two possible causes and solutions for the error:

- Another controller or process is also updating the object with different values.
+
To resolve the issue, disable the policy and compare the differences between `objectDefinition` in the policy and the object on the managed cluster. If the values are different, another controller or process might be updating them. Check the `metadata` of the object to help identify why the values are different.

- The `objectDefinition` in the `ConfigurationPolicy` does not match because of Kubernetes processing the object when the policy is applied.
+
To resolve the issue, disable the policy and compare the differences between `objectDefinition` in the policy and the object on the managed cluster. If the keys are different or missing, Kubernetes might have processed the keys before applying them to the object, such as removing keys containing default or empty values.
+
*Note:* If `pruneObjectBehavior` is set to something other than `None`, disabling the policy causes the objects to be cleaned up. In this case, set `pruneObjectBehavior` to `None` so that the objects exist after the policy is disabled.

For example, the `stringData` map in a `Secret` resource is converted by Kubernetes to `data` with `base64` encoded values. Instead of using `stringData`, use `data` directly with `base64` encoded values instead of strings.

[#psp-not-supported-ocp]
== Pod security policies not supported on {ocp-short} 4.12 and later

The support of pod security policies is removed from {ocp-short} 4.12 and later, and from Kubernetes v1.25 and later. If you apply a `PodSecurityPolicy` resource, you might receive the following non-compliant message:

----
violation - couldn't find mapping resource with kind PodSecurityPolicy, please check if you have CRD deployed
----

[#duplicate-policy-template-name-in-the-same-policy]
== Duplicate policy template names create inconstistent results
//2.8:5754

When you create a policy with identical policy template names, you receive inconsistent results that are not detected, but you might not know the cause. For example, defining a policy with multiple configuration policies named `create-pod` causes inconsistent results. *Best practice:* Avoid using duplicate names for policy templates.

[#disabling-grc]
== Governance deployments do not shut down without errors when disabled

When you disable governance deployments in the `MultiClusterHub` object, the deployments are not cleaned without errors. Complete the following steps to disable governance so that the deployments also get cleaned up:

. Disable the `policyController` in the `KlusterletAddonConfig` for the managed cluster. If you do this for all managed clusters, run the following command:
+
[source,bash]
----
for CLUSTER in $(oc get managedclusters -o jsonpath='{.items[].metadata.name}'); do
  oc patch -n ${CLUSTER} klusterletaddonconfig ${CLUSTER} --type=merge --patch='{"spec":{"policyController":{"enabled":false}}}'
done
----

. For local clusters only: Delete the `ManifestWork` for the local cluster and remove the finalizer on the `ManagedClusterAddon` if the `governance-policy-framework-uninstall` pod of a local cluster is in `CrashLoopBackOff`. Run the following commands:
+
[source,bash]
----
oc delete manifestwork -n local-cluster -l open-cluster-management.io/addon-name=governance-policy-framework
oc patch managedclusteraddon -n local-cluster governance-policy-framework --type=merge --patch='{"metadata":{"finalizers":[]}}'
----

. Disable governance globally, if required, by setting the `grc` element in the `spec.overrides` section to `false` in the `MultiClusterHub` object. Run the following command:
+
[source,bash]
----
oc edit multiclusterhub <name> -n <namespace>
----

. For local clusters only: If there are any local cluster policies, you can delete the policies by running the following command:
+
[source,bash]
----
oc delete policies -n local-cluster --all
----

. To re-enable governance in the `KlusterletAddonConfig`, re-enable the `grc` element of the `spec.overrides` section in the `MultiClusterHub`. Run the following command:
+
[source,bash]
----
for CLUSTER in $(oc get managedclusters -o jsonpath='{.items[].metadata.name}'); do
  oc patch -n ${CLUSTER} klusterletaddonconfig ${CLUSTER} --type=merge --patch='{"spec":{"policyController":{"enabled":true}}}'
done
----

. If the deployments are unsuccessful, the `governance-policy-addon-controller` might have a stale lease. Delete the lease by using the following command:
+
[source,bash]
----
oc delete lease governance-policy-addon-controller-lock -n <namespace> 
----

[#templating-errors]
== Objects are deleted due to templating errors
//2.8:ACM-5855

When there are templating errors, such as incorrect syntax in a configuration policy, objects are deleted. Recreate your deleted object with the correct syntax. 


[#duplicate-ansible-jobs]
== Duplicate Ansible jobs are created for policy automations  
//2.7+2.8:ACM-5644

If you have a `PolicyAutomation` that is set to _Run once_ mode and disabled, an extra Ansible job is created. You can delete the extra Ansible job. Complete the following steps:

. Run the following command to view the Ansible job list:
+
[source,bash]
----
oc get ansiblejob -n {namespace}
----

. Delete the duplicate Ansible job by using the following command:
+
[source,bash]
----
oc delete ansiblejob {ansiblejob name} -n {namespace}
----
