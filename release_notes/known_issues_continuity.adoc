[#known-issues-continuity]
= Business continuity known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for {product-title}. The following list contains known issues for this release, or known issues that continued from the previous release. 

For your {ocp} cluster, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/release_notes/ocp-4-12-release-notes#ocp-4-12-known-issues[{ocp-short} known issues]. 

For more about deprecations and removals, see xref:../release_notes/deprecate_remove.adoc#deprecations-removals[Deprecations and removals].

[#known-issues-backup-restore]
== Backup and restore known issues

Backup and restore known issues and limitations are listed here, along with workarounds if they are available.

[#bare-metal-reintall]
=== Bare metal managed clusters deployed with the Infrastructure Operator by using the ZTP flow perform reinstall nodes 
//2.9:ACM-8627

If the resources for the bare metal cluster are backed up and restored to a secondary hub cluster by using the {product-title-short} back up and restore feature, the managed cluster reinstalls on the nodes, which destroys the existing managed cluster. 

*Note:* This only affects bare metal clusters that were deployed by using zero touch provisioning, meaning that they have `BareMetalHost` resources that manage powering on and off bare metal nodes and attaching virtual media for booting.

If a `BareMetalHost` resource was not used in the deployment of the managed cluster, there is no negative impact.

To work around this issue, exclude managed `BareMetalHost` resources on the primary hub cluster from performing back up and restore to the secondary hub cluster. 

Add the following label to the `BareMetalHost` resources on the primary hub cluster: `velero.io/exclude-from-backup: "true"`.

This label excludes any resource from the back up procedure. 

When you exclude the `BareMetalHost` resource from restore, removing a cluster by using zero touch provisioning does not fully function because `BareMetalHost` manages power for the bare metal nodes. 

[#restore-ocp-not-supported]
=== Restoring {ocp-short} 3.11 managed cluster is not supported
//2.9:ACM-8472

If you have {ocp-short} 3.11 managed clusters and you restore the cluster, the managed cluster is not connected to the new hub cluster automatically. 

This occurs because the managed service account add-on is not supported on {ocp-short} 3.11 managed clusters.

[#backup-oadp-failed-validation]
=== _BackupSchedule_ shows a _FailedValidation_ status when using OADP 1.1.2, or later
//2.8:OADP-1511

After you enable the {product-title-short} backup and restore component and successfully create a `DataProtectionApplication` resource, a `BackupStorageLocation` resource is created with a status of `Available`. When you are using OADP version 1.1.2 or later, you might receive the following message after you create a `BackupSchedule` resource and the status is `FailedValidation`:

----
oc get backupschedule -n open-cluster-management-backup
NAME PHASE MESSAGE
rosa-backup-schedule FailedValidation Backup storage location is not available. Check velero.io.BackupStorageLocation and validate storage credentials.
----

The error is caused by a missing value for `ownerReference` in the `BackupStorageLocation` resource. The value of the `DataProtectionApplication` resource should be used as the value of the `ownerReference`.

To work around the problem, manually add the `ownerReference` to the `BackupStorageLocation`:

. Open the `oadp-operator.v1.1.2` file by running the following command:

+
----
oc edit csv -n open-cluster-management-backup oadp-operator.v1.1.2
----

. Edit the value of `spec.deployments.label.spec.replicas` by replacing the `1` with a `0` in the OADP operator CSV.

. Patch the `ownerReference` annotations in the YAML script as shown in the following example:

+
[source,yaml]
----
metadata:
resourceVersion: '273482'
name: dpa-sample-1
uid: 4701599a-cdf5-48ac-9264-695a95b935a0
namespace: open-cluster-management-backup
ownerReferences: <<

apiVersion: oadp.openshift.io/v1alpha1
blockOwnerDeletion: true
controller: true
kind: DataProtectionApplication
name: dpa-sample
uid: 52acd151-52fd-440a-a846-95a0d7368ff7
----

. Change the value of `spec.deployments.label.spec.replicas` back to `1` to start the data protection application process with the new settings. 

[#restore-limitations]
=== Velero restore limitations

A new hub cluster can have a different configuration than the active hub cluster if the new hub cluster, where the data is restored, has user-created resources. For example, this can include an existing policy that was created on the new hub cluster before the backup data is restored on the new hub cluster.

Velero skips existing resources if they are not part of the restored backup, so the policy on the new hub cluster remains unchanged, resulting in a different configuration between the new hub cluster and active hub cluster.

To address this limitation, the cluster backup and restore operator runs a post restore operation to clean up the resources created by the user or a different restore operation when a `restore.cluster.open-cluster-management.io` resource is created.

For more information, see the link:../business_continuity/backup_restore/backup_install.adoc#dr4hub-install-backup-and-restore[Installing the backup and restore operator] topic. 

[#imported-clusters-not-displayed]
=== Passive configurations do not display managed clusters

Managed clusters are only displayed when the activation data is restored on the passive hub cluster.

[#upgrade-limitation]
=== Cluster backup and restore upgrade limitation

If you upgrade your cluster from {product-version-prev} to {product-version} with the `enableClusterBackup` parameter set to `true`, the following message appears:

----
When upgrading from version 2.4 to 2.5, cluster backup must be disabled
----

Before you upgrade your cluster, disable cluster backup and restore by setting the `enableClusterBackup` parameter to `false`. The `components` section in your `MultiClusterHub` resource might resemble the following YAML file:

You can reenable the backup and restore component when the upgrade is complete. View the following sample:

[source,yaml]
----
overrides:
      components:
        - enabled: true
          name: multiclusterhub-repo
        - enabled: true
          name: search
        - enabled: true
          name: management-ingress
        - enabled: true
          name: console
        - enabled: true
          name: insights
        - enabled: true
          name: grc
        - enabled: true
          name: cluster-lifecycle
        - enabled: true
          name: volsync
        - enabled: true
          name: multicluster-engine
        - enabled: false
          name: cluster-proxy-addon
        - enabled: true <<<<<<<< 
          name: cluster-backup
    separateCertificateManagement: false
----

If you have manually installed OADP, you must manually uninstall OADP before you upgrade. After the upgrade is successful and backup and restore is reenabled, OADP is installed automatically.

[#managed-cluster-resources-not-restored]
=== Managed cluster resource not restored
//2.5:22402

When you restore the settings for the `local-cluster` managed cluster resource and overwrite the `local-cluster` data on a new hub cluster, the settings are misconfigured. Content from the previous hub cluster `local-cluster` is not backed up because the resource contains `local-cluster` specific information, such as the cluster URL details.

You must manually apply any configuration changes that are related to the `local-cluster` resource on the restored cluster. See _Prepare the new hub cluster_ in the link:../business_continuity/backup_restore/backup_install.adoc#dr4hub-install-backup-and-restore[Installing the backup and restore operator] topic.

[#restored-hive-managed-clusters-unable-new-hub]
=== Restored Hive managed clusters might not be able to connect with the new hub cluster
//2.6:23930

When you restore the backup of the changed or rotated certificate of authority (CA) for the Hive managed cluster, on a new hub cluster, the managed cluster fails to connect to the new hub cluster. The connection fails because the `admin` `kubeconfig` secret for this managed cluster, available with the backup, is no longer valid. 

You must manually update the restored `admin` `kubeconfig` secret of the managed cluster on the new hub cluster.

[#imported-managed-clusters-pending-import]
=== Imported managed clusters show a _Pending Import_ status
//2.7:26797

Managed clusters that are manually imported on the primary hub cluster show a `Pending Import` status when the activation data is restored on the passive hub cluster. For more information, see link:../business_continuity/backup_restore/backup_msa.adoc#auto-connect-clusters-msa[Connecting clusters by using a Managed Service Account].

[#appliedmanifestwork-not-removed]
=== The _appliedmanifestwork_ is not removed from managed clusters after restoring the hub cluster
//2.7:27129

When the hub cluster data is restored on the new hub cluster, the `appliedmanifestwork` is not removed from managed clusters that have a placement rule for an application subscription that is not a fixed cluster set.

See the following example of a placement rule for an application subscription that is not a fixed cluster set:

[source,yaml]
----
spec:
  clusterReplicas: 1
  clusterSelector:
    matchLabels:
      environment: dev
----

As a result, the application is orphaned when the managed cluster is detached from the restored hub cluster.

To avoid the issue, specify a fixed cluster set in the placement rule. See the following example:

[source,yaml]
----
spec:
  clusterSelector:
    matchLabels:
      environment: dev
----

You can also delete the remaining `appliedmanifestwork` manually by running the folowing command:

----
oc delete appliedmanifestwork <the-left-appliedmanifestwork-name>
----

[#backup-placement-clusterset]
=== The _appliedmanifestwork_ is not removed and hub cluster placement rule does not have a fixed cluster set
//2.7+:ACM-7588

When the hub cluster data is restored on the new hub cluster, the `appliedmanifestwork` is not removed from managed clusters that have a placement rule for an application subscription that is not a fixed cluster set. As a result, the application is orphaned when the managed cluster is detached from the restored hub cluster.

See the following example of a placement rule for an application subscription that is not a fixed cluster set:

[source,yaml]
----
spec:
  clusterReplicas: 1
  clusterSelector:
    matchLabels:
      environment: dev 
----

To avoid the issue, specify a fixed cluster set in the placement rule. See the following example:

[source,yaml]
----
spec:
  clusterSelector:
    matchLabels:
      environment: dev 
----

You can also delete the remaining `appliedmanifestwork` manually by running the following command:

----
oc delete appliedmanifestwork <the-left-appliedmanifestwork-name>
----

[#appliedmanifest-agentid-missing]
=== The _appliedmanifestwork_ not removed and _agentID_ is missing in the specification
//2.7+:ACM-7588

When you are using {product-title-short} 2.6 as your primary hub cluster, but your restore hub cluster is on version 2.7 or later, the `agentID` is missing in the specification of `appliedmanifestworks` because the field is introduced in the 2.7 release. This results in the extra `appliedmanifestworks` for the primary hub on the managed cluster.

To avoid the issue, upgrade the primary hub cluster to {product-title-short} 2.7, then restore the backup on a new hub cluster.

Fix the managed clusters by setting the `spec.agentID` manually for each `appliedmanifestwork`.

. Run the following command to get the `agentID`: 
+
----
oc get klusterlet klusterlet -o jsonpath='{.metadata.uid}'
----

. Run the following command to set the `spec.agentID` for each `appliedmanifestwork`:
+
----
oc patch appliedmanifestwork <appliedmanifestwork_name> --type=merge -p '{"spec":{"agentID": "'$AGENT_ID'"}}'  
----

[#msa-status-unknown]
=== The _managed-serviceaccount_ add-on status shows _Unknown_
//2.8:ACM-5887

The managed cluster `appliedmanifestwork` `addon-managed-serviceaccount-deploy` is removed from the imported managed cluster if you are using the Managed Service Account without enabling it on the {mce} resource of the new hub cluster.

The managed cluster is still imported to the new hub cluster, but 
the `managed-serviceaccount` add-on status shows `Unknown`.
 
You can recover the `managed-serviceaccount` add-on after enabling the Managed Service Account in the {mce-short} resource. See link:../business_continuity/backup_restore/backup_msa.adoc#enabling-auto-import[Enabling automatic import] to learn how to enable the Managed Service Account.

//[#known-issues-volsync]
//== Volsync known issues
