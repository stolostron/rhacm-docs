[#known-issues-continuity]
= Business continuity known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for {product-title}. The following list contains known issues for this release, or known issues that continued from the previous release. 

For your {ocp} cluster, see https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html/release_notes/ocp-4-13-release-notes#ocp-4-13-known-issues[{ocp-short} known issues][{ocp-short} known issues]. 

For more about deprecations and removals, see xref:../release_notes/deprecate_remove.adoc#deprecations-removals[Deprecations and removals].

[#known-issues-backup-restore]
== Backup and restore known issues

[#backup-oadp-failed-validation]
=== _BackupSchedule_ shows a _FailedValidation_ status when using OADP 1.1.2, or later
//2.8:OADP-1511

After you enable the {product-title-short} backup and restore component and successfully create a `DataProtectionApplication` resource, a `BackupStorageLocation` resource is created with a status of `Available`. When you are using OADP version 1.1.2 or later, you might receive the following message after you create a `BackupSchedule` resource and the status is `FailedValidation`:

----
oc get backupschedule -n open-cluster-management-backup
NAME PHASE MESSAGE
rosa-backup-schedule FailedValidation Backup storage location is not available. Check velero.io.BackupStorageLocation and validate storage credentials.
----

The error is caused by a missing value for `ownerReference` in the `BackupStorageLocation` resource. The value of the `DataProtectionApplication` resource should be used as the value of the `ownerReference`.

To work around the problem, manually add the `ownerReference` to the `BackupStorageLocation`:

. Open the `oadp-operator.v1.1.2` file by running the following command:

+
----
oc edit csv -n open-cluster-management-backup oadp-operator.v1.1.2
----

. Edit the value of `spec.deployments.label.spec.replicas` by replacing the `1` with a `0` in the OADP operator CSV.

. Patch the `ownerReference` annotations in the YAML script as shown in the following example:

+
[source,yaml]
----
metadata:
resourceVersion: '273482'
name: dpa-sample-1
uid: 4701599a-cdf5-48ac-9264-695a95b935a0
namespace: open-cluster-management-backup
ownerReferences: <<

apiVersion: oadp.openshift.io/v1alpha1
blockOwnerDeletion: true
controller: true
kind: DataProtectionApplication
name: dpa-sample
uid: 52acd151-52fd-440a-a846-95a0d7368ff7
----

. Change the value of `spec.deployments.label.spec.replicas` back to `1` to start the data protection application process with the new settings. 

[#backup-collision]
=== Avoid backup collision
//2.5:19469

As hub clusters change from passive to primary clusters and back, different clusters can backup data at the same storage location. This can result in backup collisions, which means that the latest backups are generated by a passive hub cluster. 

The passive hub cluster produces backups because the `BackupSchedule.cluster.open-cluster-management.io` resource is enabled on the hub cluster, but it should no longer write backup data since the hub cluster is no longer a primary hub cluster. Run the following command to check if there is a backup collision:

----
oc get backupschedule -A
----

You might receive the following status:

----
NAMESPACE       NAME               PHASE             MESSAGE
openshift-adp   schedule-hub-1   BackupCollision   Backup acm-resources-schedule-20220301234625, from cluster with id [be97a9eb-60b8-4511-805c-298e7c0898b3] is using the same storage location. This is a backup collision with current cluster [1f30bfe5-0588-441c-889e-eaf0ae55f941] backup. Review and resolve the collision then create a new BackupSchedule resource to  resume backups from this cluster.
----

The controller avoids backup collisions by setting the `BackupSchedule.cluster.open-cluster-management.io` resource `status` to `BackupCollision`. The `Schedule.velero.io` resources that are created by the `BackupSchedule` resource are automatically deleted. 

The backup collision is reported by the link:https://github.com/stolostron/cluster-backup-chart/blob/main/stable/cluster-backup-chart/templates/hub-backup-pod.yaml[`hub-backup-pod`] policy. The administrator must verify which hub cluster writes data to the storage location. Then remove the `BackupSchedule.cluster.open-cluster-management.io` resource from the passive hub cluster, and recreate a new `BackupSchedule.cluster.open-cluster-management.io` resource on the primary hub cluster to resume the backup.

See link:../business_continuity/backup_restore/backup_intro.adoc#backup-intro[Backup and restore] for more information. 

[#restore-limitations]
=== Velero restore limitations
A new hub cluster can have a different configuration than the active hub cluster if the new hub cluster, where the data is restored, has user-created resources. For example, this can include an existing policy that was created on the new hub cluster before the backup data is restored on the new hub cluster.

Velero skips existing resources if they are not part of the restored backup, so the policy on the new hub cluster remains unchanged, resulting in a different configuration between the new hub cluster and active hub cluster.

To address this limitation, the cluster backup and restore operator runs a post restore operation to clean up the resources created by the user or a different restore operation when a `restore.cluster.open-cluster-management.io` resource is created.

For more information, see the link:../business_continuity/backup_restore/backup_install.adoc#install-backup-and-restore[Installing the backup and restore operator] topic. 

[#imported-clusters-not-displayed]
=== Passive configurations do not display managed clusters

Managed clusters are only displayed when the activation data is restored on the passive hub cluster.

[#upgrade-limitation]
=== Cluster backup and restore upgrade limitation

If you upgrade your cluster from {product-version-prev} to {product-version} with the `enableClusterBackup` parameter set to `true`, the following message appears:

----
When upgrading from version 2.4 to 2.5, cluster backup must be disabled
----

Before you upgrade your cluster, disable cluster backup and restore by setting the `enableClusterBackup` parameter to `false`. The `components` section in your `MultiClusterHub` resource might resemble the following YAML file:

You can reenable the backup and restore component when the upgrade is complete. View the following sample:

[source,yaml]
----
overrides:
      components:
        - enabled: true
          name: multiclusterhub-repo
        - enabled: true
          name: search
        - enabled: true
          name: management-ingress
        - enabled: true
          name: console
        - enabled: true
          name: insights
        - enabled: true
          name: grc
        - enabled: true
          name: cluster-lifecycle
        - enabled: true
          name: volsync
        - enabled: true
          name: multicluster-engine
        - enabled: false
          name: cluster-proxy-addon
        - enabled: true <<<<<<<< 
          name: cluster-backup
    separateCertificateManagement: false
----

If you have manually installed OADP, you must manually uninstall OADP before you upgrade. After the upgrade is successful and backup and restore is reenabled, OADP is installed automatically.

[#managed-cluster-resources-not-restored]
=== Managed cluster resource not restored
//2.5:22402

When you restore the settings for the `local-cluster` managed cluster resource and overwrite the `local-cluster` data on a new hub cluster, the settings are misconfigured. Content from the previous hub cluster `local-cluster` is not backed up because the resource contains `local-cluster` specific information, such as the cluster URL details.

You must manually apply any configuration changes that are related to the `local-cluster` resource on the restored cluster. See _Prepare the new hub cluster_ in the link:../business_continuity/backup_restore/backup_install.adoc#install-backup-and-restore[Installing the backup and restore operator] topic.

[#restored-hive-managed-clusters-unable-new-hub]
=== Restored Hive managed clusters might not be able to connect with the new hub cluster
//2.6:23930

When you restore the backup of the changed or rotated certificate of authority (CA) for the Hive managed cluster, on a new hub cluster, the managed cluster fails to connect to the new hub cluster. The connection fails because the `admin` `kubeconfig` secret for this managed cluster, available with the backup, is no longer valid. 

You must manually update the restored `admin` `kubeconfig` secret of the managed cluster on the new hub cluster.

[#imported-managed-clusters-pending-import]
=== Imported managed clusters show a _Pending Import_ status
//2.7:26797

Managed clusters that are manually imported on the primary hub cluster show a `Pending Import` status when the activation data is restored on the passive hub cluster. For more information, see link:../business_continuity/backup_restore/backup_msa.adoc#auto-connect-clusters-msa[Connecting clusters by using a Managed Service Account].

[#appliedmanifestwork-not-removed]
=== The _appliedmanifestwork_ is not removed from managed clusters after restoring the hub cluster
//2.7:27129

When the hub cluster data is restored on the new hub cluster, the `appliedmanifestwork` is not removed from managed clusters that have a placement rule for an application subscription that is not a fixed cluster set.

See the following example of a placement rule for an application subscription that is not a fixed cluster set:

[source,yaml]
----
spec:
  clusterReplicas: 1
  clusterSelector:
    matchLabels:
      environment: dev
----

As a result, the application is orphaned when the managed cluster is detached from the restored hub cluster.

To avoid the issue, specify a fixed cluster set in the placement rule. See the following example:

[source,yaml]
----
spec:
  clusterSelector:
    matchLabels:
      environment: dev
----

You can also delete the remaining `appliedmanifestwork` manually by running the following command:

----
oc delete appliedmanifestwork <the-left-appliedmanifestwork-name>
----

//[#known-issues-volsync]
//== Volsync known issues
