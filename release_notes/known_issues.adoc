[#known-issues]
= Known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue with workaround if:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)

Or consider a troubleshooting topic.
////

Review the known issues for {product-title}. The following list contains known issues for this release, or known issues that continued from the previous release. For your {ocp} cluster, see https://docs.openshift.com/container-platform/4.3/release_notes/ocp-4-3-release-notes.html#ocp-4-3-known-issues[{ocp-short} known issues].

* <<installation-known-issues,Installation known issues>>
* <<web-console-known-issues,Web console known issues>>
* <<cluster-management-issues,Cluster management known issues>>
* <<application-management-known-issues,Application management known issues>>
//* <<security-known-issues,Security known issues>>

[#installation-known-issues]
== Installation known issues

[#openshift-container-platform-cluster-upgrade-failed-status]
=== OpenShift Container Platform cluster upgrade failed status

// 2.0.0:3442

When an {ocp-short} cluster is in the upgrade stage, the cluster pods are restarted and the cluster might remain in `upgrade failed` status for a variation of 1-5 minutes. This behavior is expected and resolves after a few minutes.

[#certificate-manager-must-not-exist-during-an-installation]
=== Certificate manager must not exist during an installation

// 1.0.0:678 be sure this is an install doc prereq

Certificate manager must not exist on a cluster when you install {product-title}.

When certificate manager already exists on the cluster, {product-title} installation fails.

To resolve this issue, verify if the certificate manager is present in your cluster by running the following command:

----
kubectl get crd | grep certificates.certmanager
----

[#web-console-known-issues]
== Web console known issues

[#search-result-node]
=== Node discrepancy between Cluster page and search results
// 2.0, 2.1, 2.2:9987

You might see a discrepancy between the nodes dispalyed on the _Cluster_ page and the _Search_ results.

[#ldap-user-names-are-case-sensitive]
=== LDAP user names are case-sensitive
// 1.0.0:before 1.0.0.1

LDAP user names are case-sensitive.
You must use the name exactly the way it is configured in your LDAP directory.

[#console-features-might-not-display-in-firefox-earlier-versions]
=== Console features might not display in Firefox earlier versions

// 1.0.0:before 1.0.0.1

The product supports Mozilla Firefox 74.0 or the latest version that is available for Linux, macOS, and Windows.
Upgrade to the latest version for the best console compatibility.

[#unable-to-search-using-values-with-empty-spaces]
=== Unable to search using values with empty spaces

// 1.0.0:1726

From the console and Visual Web Terminal, users are unable to search for values that contain an empty space.

[#at-logout-user-kubeadmin-gets-extra-browser-tab-with-blank-page]
=== At logout user kubeadmin gets extra browser tab with blank page

// 1.0.0:2191

When you are logged in as `kubeadmin` and you click the *Log out* option in the drop-down menu, the console returns to the login screen, but a browser tab opens with a `/logout` URL.
The page is blank and you can close the tab without impact to your console.

[#secret-content-is-no-longer-displayed]
=== Secret content is no longer displayed

// 2.1.0:6108

For security reasons, search does not display the contents of secrets found on managed clusters. When you search for a secret from the console, you might receive the following error message:

----
Unable to load resource data - Check to make sure the cluster hosting this resource is online
----

[#restrictions-for-storage-size-in-searchcustomization]
=== Restrictions for storage size in searchcustomization
//2.2:8501

When you update the storage size in the `searchcustomization` CR, the PVC configuration does not change. If you need to update the storage size, update the PVC (`_<storageclassname>-search-redisgraph-0_`) with the following command:
----
oc edit pvc <storageclassname>-search-redisgraph-0
----

[#search-yaml-safari]
=== YAML file is not displayed from the _Search_ page
//2.2.3:11994

When you use the Safari v14.0.3 web browser, the YAML file is not displayed from the _Search_ page. See link:../install/requirements.adoc#supported-browser[Supported browsers] for other supported browsers or upgrade your Safari version.

[#restart-redisgraph]
=== Restart _redisgraph_ StatefulSet and pod
//2.2.5:13679

The `redisgraph` StatefulSet and pod are not refreshed when you upgrade from 2.2.z to 2.2.5. You must delete the `redisgraph` StatefulSet manually, so that the changes are picked up. Complete the following steps to fix this issue:

. Install {product-title-short} 2.2.4. 
. Upgrade to {product-title-short} 2.2.5.
. Log in as an administrator and check the `redisgraph` StatefulSet by running the following command:
+
----
oc get statefulset search-redisgraph -n open-cluster-management
----
. Notice that the StatefulSet and pod did not restart.
. Delete the `redisgraph` StatefulSet with the following command:
+
----
oc delete statefulset search-redisgraph -n open-cluster-management
----
. Verifiy if the `redisgraph` pod is running successfully with the following command:
+
----
oc get pod search-redisgraph-0 -n open-cluster-management
----

The `redisgraph` StatefulSet and pod have restarted. 

[#observability-endpoint-operator-fails-to-pull-image]
=== Observability endpoint operator fails to pull image
//2.2:9259

The observability endpoint operator fails if you create a pull-secret to deploy to the MultiClusterObservability CustomResource (CR) and there is no pull-secret in the `open-cluster-management-observability` namespace. When you import a new cluster, or import a Hive cluster that is created with {product-title-short}, you need to manually create a pull-image secret on the managed cluster.

For more information, see link:../observing_environments/observability_enable.adoc#enabling-observability[Enabling observability].

[#observability-add-on-stuck-in-terminating]
=== Observability add-on stuck in terminating
//2.2:9928

When the managed cluster is detached forcefully, the `ObservabilityAddon` resource (`_observability-addon_`) in the cluster namespace is stuck in the _Terminating_ status and cannot be removed. Also, the cluster namespace cannot be deleted.

To fix this problem, you can update the `ObervabilityAddon` resource in the cluster namespace. Update the resource by deleting the `_finalizers_` parameter in the metadata. Run the following command:

----
kubectl edit observabilityaddon observability-addon -n <CLUSTER_NAMESPACE>
----

`_CLUSTER_NAMESPACE_` is the cluster namespace for the detached cluster.

After the `_finalizers_` parameter is removed, the `ObervabilityAddon` resource is removed.

[#missing-data-roks]
=== There is no data from ROKS cluster
//2.2.3:12114

{product-title-short} observability does not display data from an ROKS cluster on some panels within built-in dashboards. This is because ROKS does not expose any API Server metrics from servers they manage. The following Grafana dashboards contain panels that do not support ROKS clusters: `Kubernetes/API server`, `Kubernetes/Compute Resources/Workload`, `Kubernetes/Compute Resources/Namespace(Workload)`

[#metrics-not-collected]
=== Metrics data no longer collected after upgrade

After you upgrade {product-title-short} from 2.2.3 to 2.2.4, you might notice that the `metrics-collector` in the `open-cluster-management-addon-observability` namespace stops collecting data from your managed clusters. This is because the image manifests ConfigMap was upgraded after the multicluster observability operator was upgraded.

You might see the following Quay.io images being used in the `endpoint-observabililty-operator` YAML file: `quay.io/open-cluster-management/endpoint-monitoring-operator:2.2.0-6a5ea47fc39d51fb4fade6157843f2977442996e` and `quay.io/open-cluster-management/metrics-collector:2.2.0-ff79e6ec8783756b942a77f08b3ab763dfd2dc15`. 

To fix this issue, delete `multicluster-observability-operator` pods that are in the `open-cluster-management` namespace in the hub cluster. After the new pod is created, a new `endpoint-observability-operator` deployment is created in your managed cluster with the correct images.

[#mco-incorrect-status]
=== _MultiClusterObservability_ CR displays incorrect status

// 2.2.2:11235

After you upgrade {product-title-short} from {product-version-prev} to {product-version}.x, the `MultiClusterObservability` custom resource (CR) continues to display `Installing` from the {ocp-short} console. You can confirm that `MultiClusterObservability` is deployed successfully by ensuring that the metrics data is displayed from the Grafana console.

[#metric-gap]
=== Observability service metric gap

// 2.2.8:14821

Some metric data is not displayed on the Grafana dashboard for {ocp} verson 4.8 clusters. You must upgrade to version 2.3. For more information, see link:../install/upgrade_hub.adoc#upgrading-by-using-the-operator[Upgrading by using the operator].

[#cluster-management-issues]
== Cluster management known issues

[#delay-upgrade-cluster]
=== Cluster version does not immediately update in the console when the cluster is upgraded
// 2.2.2:11389

When you upgrade a managed cluster or a local cluster on the *Cluster details* page, it might take up to 10 minutes after the upgrade process completes for the updated version number to display on the *Cluster details* page.

[#no-create-bm-47]
=== Cannot create bare metal managed clusters on {ocp-short} version 4.7
// 2.2:10581

You cannot create bare metal managed clusters by using the {product-title-short} hub cluster when the hub cluster is hosted on {ocp-short} version 4.7.

[#create-resource-dropdown-error]
=== Create resource dropdown error
// 2.1:6299 Remove after 2.1.1????

When you detach a managed cluster, the _Create resources_ page might temporarily break and display the following error:

----
Error occurred while retrieving clusters info. Not found.
----

Wait until the namespace automatically gets removed, which takes 5-10 minutes after you detach the cluster. Or, if the namespace is stuck in a terminating state, you need to manually delete the namespace. Return to the page to see if the error resolved.

[#hub-managed-clusters-clock]
=== Hub cluster and managed clusters clock not synced
// 2.1:5636

Hub cluster and manage cluster time might become out-of-sync, displaying in the console `unknown` and eventually `available` within a few minutes. Ensure that the {ocp} hub cluster time is configured correctly. See https://docs.openshift.com/container-platform/4.6/installing/install_config/installing-customizing.html[Customizing nodes].

[#console-managed-cluster-inconsistency]
=== Console might report managed cluster policy inconsistency
// 2.0.0:3850

After a cluster is imported, log in to the imported cluster and make sure all pods that are deployed by the Klusterlet are running. Otherwise, you might see inconsistent data in the console.

For example, if a policy controller is not running, you might not get the same results of violations on the _Governance and risk_ page and the _Cluster status_. 

For instance, you might see 0 violations listed in the _Overview_ status, but you might have 12 violations reported on the _Governance and risk_ page. 

In this case, inconsistency between the pages represents a disconnection between the `policy-controller-addon` on managed clusters and the policy controller on the hub cluster. Additionally, the managed cluster might not have enough resources to run all the Klusterlet components. 

As a result, the policy was not propagated to managed cluster, or the violation was not reported back from managed clusters.

[#importing-clusters-might-require-two-attempts]
=== Importing clusters might require two attempts

// 2.0.0:3596

When you import a cluster that was previously managed and detached by a {product-title-short} hub cluster, the import process might fail the first time. The cluster status is `pending import`. Run the command again, and the import should be successful. 

[#importing-certain-versions-of-ibm-red-hat-openshift-kubernetes-service-clusters-is-not-supported]
=== Importing certain versions of IBM Red Hat OpenShift Kubernetes Service clusters is not supported

// 1.0.0:2179

You cannot import IBM Red Hat OpenShift Kubernetes Service version 3.11 clusters.
Later versions of IBM OpenShift Kubernetes Service are supported.

[#detaching-openshift-container-platform-3.11-does-not-remove-the-open-cluster-management-agent]
=== Detaching {ocp-short} 3.11 does not remove the _open-cluster-management-agent_

// 2.0.0:3847

When you detach managed clusters on {ocp-short} 3.11, the `open-cluster-management-agent` namespace is not automatically deleted. Manually remove the namespace by running the following command:

----
oc delete ns open-cluster-management-agent
----

[#automatic-secret-updates-for-provisioned-clusters-is-not-supported]
=== Automatic secret updates for provisioned clusters is not supported

// 2.0.0:3702

When you change your cloud provider access key, the provisioned cluster access key is not updated in the namespace. This is required when your credentials expire on the cloud provider where the managed cluster is hosted and you try delete the managed cluster. If something like this occurs, run the following command for your cloud provider to update the access key: 

* Amazon Web Services (AWS)

+
----
oc patch secret {CLUSTER-NAME}-aws-creds -n {CLUSTER-NAME} --type json -p='[{"op": "add", "path": "/stringData", "value":{"aws_access_key_id": "{YOUR-NEW-ACCESS-KEY-ID}","aws_secret_access_key":"{YOUR-NEW-aws_secret_access_key}"} }]'
----

* Google Cloud Platform (GCP)

+
You can identify this issue by a repeating log error message that reads, `Invalid JWT Signature` when you attempt to destroy the cluster. If your log contains this message, obtain a new Google Cloud Provider service account JSON key and enter the following command:

+
----
oc set data secret/<CLUSTER-NAME>-gcp-creds -n <CLUSTER-NAME> --from-file=osServiceAccount.json=$HOME/.gcp/osServiceAccount.json
----
+
Replace `_CLUSTER-NAME_` with the name of your cluster.
+
Replace the path to the file `$HOME/.gcp/osServiceAccount.json` with the path to the file that contains your new Google Cloud Provider service account JSON key. 


* Microsoft Azure 

+
----
oc set data secret/{CLUSTER-NAME}-azure-creds -n {CLUSTER-NAME} --from-file=osServiceAccount.json=$HOME/.azure/osServiceAccount.json
----

* VMware vSphere

+
----
oc patch secret {CLUSTER-NAME}-vsphere-creds -n {CLUSTER-NAME} --type json -p='[{"op": "add", "path": "/stringData", "value":{"username": "{YOUR-NEW-VMware-username}","password":"{YOUR-NEW-VMware-password}"} }]'
----

[#no-run-mgt-ingress-non-root]
=== Cannot run management ingress as non-root user
//2.0:35532

You must be logged in as `root` to run the `management-ingress` service. 

[#node-information-from-the-managed-cluster-cannot-be-viewed-in-search]
=== Node information from the managed cluster cannot be viewed in search
// 2.0.2:4598

Search maps RBAC for resources in the hub cluster. Depending on user RBAC settings for {product-title-short}, users might not see node data from the managed cluster. Results from search might be different from what is displayed on the _Nodes_ page for a cluster.

[#cluster-might-not-be-destroyed]
=== Process to destroy a cluster does not complete

// 2.1.0:4748

When you destroy a managed cluster, the status continues to display `Destroying` after one hour, and the cluster is not destroyed. To resolve this issue complete the following steps:

. Manually ensure that there are no orphaned resources on your cloud, and that all of the provider resources that are associated with the managed cluster are cleaned up.

. Open the `ClusterDeployment` information for the managed cluster that is being removed by entering the following command:
+
----
oc edit clusterdeployment/<mycluster> -n <namespace>
----
+
Replace `_mycluster_` with the name of the managed cluster that you are destroying.
+
Replace `_namespace_` with the namespace of the managed cluster.

. Remove the `hive.openshift.io/deprovision` finalizer to forcefully stop the process that is trying to clean up the cluster resources in the cloud.

. Save your changes and verify that `ClusterDeployment` is gone.

. Manually remove the namespace of the managed cluster by running the following command:
+
----
oc delete ns <namespace>
----
+
Replace `_namespace_` with the namespace of the managed cluster.

[#no-upgrade-os-on-osd]
=== Cannot upgrade {ocp-short} managed clusters on Red Hat OpenShift Dedicated with the console

// 2.2.0:8922

You cannot use the {product-title-short} console to upgrade {ocp-short} managed clusters that are in the Red Hat OpenShift Dedicated environment.

[#observability-annotation-query-failed]
=== Metrics are unavailable in the Grafana console

* Annotation query failed in the Grafana console: 
// 2.1.0:5625
+
When you search for a specific annotation in the Grafana console, you might receive the following error message due to an expired token: 
+
`"Annotation Query Failed"`
+
Refresh your browser and verify you are logged into your hub cluster.

* Error in _rbac-query-proxy_ pod:
+
Due to unauthorized access to the `managedcluster` resource, you might receive the following error when you query a cluster or project:
+
`no project or cluster found`
+
Check the role permissions and update appropriately. See link:../security/rbac.adoc#role-based-access-control[Role-based access control] for more information. 

[#bm-assets-remain-after-destroy]
=== Related bare metal assets not destroyed after bare metal cluster is destroyed

// 2.2.0:9943

Your bare metal asset might remain as an orphaned asset after you destroy the cluster that was associated with it. This happens when you have the required permissions to destroy a cluster, but not to destroy the bare metal asset. To ensure that you do not experience this issue, add a finalizer to the `ClusterDeployment` resource when you create a bare metal asset with {product-title-short} that references a cluster deployment:

----
kubectl patch clusterdeployments <name> -n <namespace> -p '{"metadata":{"finalizers":["baremetalasset.inventory.open-cluster-management.io"]}}'
----
Replace `_name_` with the name of your cluster deployment. 

Replace `_namespace_` with the namespace of your cluster resource.

If you delete the cluster deployment, you must remove the finalizer manually by entering the following command:
----
kubectl patch clusterdeployments <name> -n <namespace> -p '{"metadata":{"finalizers":[]}}'
----
Replace `_name_` with the name of your cluster deployment. 

Replace `_namespace_` with the namespace of your cluster resource.

[#application-management-known-issues]
== Application management known issues

[#application-deployment-window-error]
=== Application deployment window error
// 2.2.0:15743

When you create an application with a deployment window that is set to `Active within specified interval`, the deployment window might not be calculated correctly, resulting in the application being deployed in undefined times.

[#application-name]
=== Application name requirements
// 2.3/2.2:#14310

An application name cannot exceed 37 characters. The application deployment displays the following error if the characters exceed this amount:

[source,yaml]
----
status:
  phase: PropagationFailed
----

[#work-manager-addon-search]
=== Work manager add-on search details
//2.2.0: 13715

The search details page for a certain resource on a certain managed cluster might fail. You must ensure that the work-manager add-on in the managed cluster is in `Available` status before you can search.

[#deployable-resource-empty-spec] 
=== Deployable resource with empty specification does not work
// 2.2:9953 app team plans to fix in 2.3 to block user from creating

Applying a Deployable resource with no specification crashes the pod `multicluster-operators-application` container `multicluster-operators-deployable`. A deployable needs to contain specifications. 

If you accidentally create the resource without a specification, delete the unnecessary deployable and restart the `multicluster-operators-application` pod.

See the following example of a Deployable that is empty and crashes the pod:

----
apiVersion: apps.open-cluster-management.io/v1
kind: Deployable
metadata:
  labels:
    app: simple-app-tester
  name: simple-app-tester-deployable
  namespace: grp-proof-of-concept-acm
----

[#replication-controller-replica-set-resource]
=== Topology ReplicationController or ReplicaSet resources missing
// 2.2:9198

When you deploy an application that directly creates a `ReplicationController` or `ReplicaSet` resource, the Pod resources are not displayed in the _Application topology_. You can use the `Deployment` or `DeploymentConfig` resources instead for creating Pod resources.

[#application-topology-displays-incorrect-job-status]
=== Application topology displays incorrect Ansible job status
// 2.2:9505

Ansible tests run as hooks for the subscription and not as regular tasks. You need to store Ansible tasks in a prehook and posthook folder. You can choose to deploy the Ansible tasks as regular tasks and not as hooks, but the Application topology Ansible job status is not reported correctly in this case.

[#application-ansible-standalone]
=== Application Ansible hook stand-alone mode
// 2.2:8036

Ansible hook stand-alone mode is not supported. To deploy Ansible hook on the hub cluster with a subscription, you might use the following subscription YAML:

[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: Subscription
metadata:
  name: sub-rhacm-gitops-demo
  namespace: hello-openshift
annotations:
  apps.open-cluster-management.io/github-path: myapp
  apps.open-cluster-management.io/github-branch: master
spec:
  hooksecretref:
      name: toweraccess
  channel: rhacm-gitops-demo/ch-rhacm-gitops-demo
  placement:
     local: true
----

However, this configuration might never create the Ansible instance, since the `spec.placement.local:true` has the subscription running on `standalone` mode. You need to create the subscription in hub mode. 

. Create a placement rule that deploys to `local-cluster`. See the following sample:

+
[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata: 
  name: <towhichcluster>
  namespace: hello-openshift
spec:
  clusterSelector:
    matchLabels:
      local-cluster: "true" #this points to your hub cluster
----

. Reference that placement rule in your subscription. See the following:

+
[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: Subscription
metadata:
  name: sub-rhacm-gitops-demo
  namespace: hello-openshift
annotations:
  apps.open-cluster-management.io/github-path: myapp
  apps.open-cluster-management.io/github-branch: master
spec:
  hooksecretref:
      name: toweraccess
  channel: rhacm-gitops-demo/ch-rhacm-gitops-demo
  placement:
     placementRef:
        name: <towhichcluster>
        kind: PlacementRule
----

After applying both, you should see the Ansible instance created in your hub cluster.

[#application-local-cluster-limitation]
=== Application Deploy on local cluster limitation
// 2.1.0:6418

If you select *Deploy on local cluster* when you create or edit an application, the application Topology does not display correctly. *Deploy on local cluster* is the option to deploy resources on your hub cluster so that you can manage it as the `local cluster`, but this is not best practice for this release.

To resolve the issue, see the following procedure:

. Deselect the *Deploy on local cluster* option in the console.
. Select the *Deploy application resources only on clusters matching specified labels* option.
. Create the following label: `local-cluster : 'true'`.

[#namespace-channel-subscription-remains-in-failed-state]
=== Namespace channel subscription remains in failed state
// 2.0.0:3581

When you subscribe to a namespace channel and the subscription remains in `FAILED` state after you fixed other associated resources such as channel, secret, ConfigMap, or placement rule, the namespace subscription is not continuously reconciled. 

To force the subscription reconcile again to get out of `FAILED` state, complete the following steps:

. Log in to your hub cluster.
. Manually add a label to the subscription using the following command:

----
oc label subscriptions.apps.open-cluster-management.io the_subscription_name reconcile=true
----

[#edit-role-for-application-error]
=== Edit role for application error

// 2.0.0:1681

A user performing in an `Editor` role should only have `read` or `update` authority on an application, but erroneously editor can also `create` and `delete` an application. Red Hat OpenShift Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole applications.app.k8s.io-v1beta1-edit -o yaml` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#edit-role-for-placement-rule-error]
=== Edit role for placement rule error

// 2.0.0:3693

A user performing in an `Editor` role should only have `read` or `update` authority on an placement rule, but erroneously editor can also `create` and `delete`, as well. Red Hat OpenShift Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole placementrules.apps.open-cluster-management.io-v1-edit` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#application-not-deployed-after-an-updated-placement-rule]
=== Application not deployed after an updated placement rule

// 1.0.0:1449

If applications are not deploying after an update to a placement rule, verify that the `klusterlet-addon-appmgr` pod is running.
The `klusterlet-addon-appmgr` is the subscription container that needs to run on endpoint clusters.

You can run `oc get pods -n open-cluster-management-agent-addon` to verify.

You can also search for `kind:pod cluster:yourcluster` in the console and see if the `klusterlet-addon-appmgr` is running.

If you cannot verify, attempt to import the cluster again and verify again.

[#subscription-operator-does-not-create-an-scc]
=== Subscription operator does not create an SCC

// 1.0.0:1764

Learn about {ocp} SCC at https://docs.openshift.com/container-platform/4.7/authentication/managing-security-context-constraints.html#security-context-constraints-about_configuring-internal-oauth[Managing Security Context Constraints (SCC)], which is an additional configuration required on the managed cluster.

Different deployments have different security context and different service accounts.
The subscription operator cannot create an SCC automatically.
Administrators control permissions for pods.
A Security Context Constraints (SCC) CR is required to enable appropriate permissions for the relative service accounts to create pods in the non-default namespace:

To manually create an SCC CR in your namespace, complete the following:

. Find the service account that is defined in the deployments.
For example, see the following `nginx` deployments:
+
----
 nginx-ingress-52edb
 nginx-ingress-52edb-backend
----

. Create an SCC CR in your namespace to assign the required permissions to the service account or accounts.
See the following example where `kind: SecurityContextConstraints` is added:
+
----
 apiVersion: security.openshift.io/v1
 defaultAddCapabilities:
 kind: SecurityContextConstraints
 metadata:
   name: ingress-nginx
   namespace: ns-sub-1
 priority: null
 readOnlyRootFilesystem: false
 requiredDropCapabilities:
 fsGroup:
   type: RunAsAny
 runAsUser:
   type: RunAsAny
 seLinuxContext:
   type: RunAsAny
 users:
 - system:serviceaccount:my-operator:nginx-ingress-52edb
 - system:serviceaccount:my-operator:nginx-ingress-52edb-backend
----

[#application-channels-require-unique-namespaces]
=== Application channels require unique namespaces

// 1.0.0:2311

Creating more than one channel in the same namespace can cause errors with the hub cluster.

For instance, namespace `charts-v1` is used by the installer as a Helm type channel, so do not create any additional channels in `charts-v1`. Ensure that you create your channel in a unique namespace. All channels need an individual namespace, except GitHub channels, which can share a namespace with another GitHub channel.

[#application-management-limitation]
== Application management limitations

[#application-tables]
=== Application console tables

See the following limitations to various _Application_ tables in the console:

- From the _Applications_ table on the _Overview_ page, the _Clusters_ column on each table displays a count of clusters where application resources are deployed. Since applications are defined by Application, Subscription, PlacementRule, and Channel objects on the local cluster, the local cluster is included in the search results, whether actual application resources are deployed on the local cluster or not.

- From the _Advanced configuration_ table for _Subscriptions_, the _Applications_ column displays the total number of applications that use that subscription, but if the subscription deploys child applications, those are included in the search result, as well.

- From the _Advanced configuration_ table for _Channels_, the _Subscriptions_ column displays the total number of subscriptions on the local cluster that use that channel, but this does not include subscriptions that are deployed by other subscriptions, which are included in the search result.

//[#security-known-issues]
//== Security known issues
