[#known-issues]
= Known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for {product-title}. The following list contains known issues for this release, or known issues that continued from the previous release. 

For your {ocp} cluster, see https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/release_notes/ocp-4-12-release-notes#ocp-4-12-known-issues[{ocp-short} known issues]. 

For more about deprecations and removals, see xref:../release_notes/deprecate_remove.adoc#deprecations-removals[Deprecations and removals] in the release notes.


* <<documentation-known-issues,Documentation known issues>>
* <<installation-known-issues,Installation known issues>>
* <<web-console-known-issues,Web console known issues>>
** <<observability-known-issues,Observability known issues>>
* <<cluster-management-issues,Cluster management known issues>>
* <<application-management-known-issues,Application management known issues>>
* <<governance-known-issues,Governance known issues>>
* <<backup-known-issues,Backup and restore known issues>>
* <<submariner-known-issues,Submariner known issues>>

[#documentation-known-issues]
== Documentation known issues

[#links-to-higher-level]
=== Documentation links in the Customer Portal might link to a higher-level section
// 2.4:19417

In some cases, the internal links to other sections of the {product-title-short} documentation in the Customer Portal do not link directly to the named section. In some instances, the links resolve to the highest-level section. 

If this happens, you can either find the specified section manually or complete the following steps to resolve:

. Copy the link that is not resolving to the correct section and paste it in your browser address bar. For example, it might be: `https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.7/html/add-ons/index#volsync`.

. In the link, replace `html` with `html-single`. The new URL should read: `https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.7/html-single/add-ons/index#volsync`

. Link to the new URL to find the specified section in the documentation.

[#installation-known-issues]
== Installation known issues

[#upgrade-user-permissions-set]
=== RBAC user requires additional roles and role bindings to view deployed resources after upgrade
//2.7:ACM-3367

After you upgrade to {product-title-short} version 2.7, the user permissions for resources in the `apps.open-cluster-management.io` group are not available. Starting with {product-title-short} version 2.7, these custom resource definitions are no longer deployed by OLM and results in the following changes:

. The resource type is no longer available in the {product-title-short} subscription console view as a card that you can select to create a resource.
. The `clusterroles` that have aggregation rules assigned to default roles are not applied for the API resource kind.

If an RBAC user requires access to these resources, you must grant the correct permissions.

[#upgrade-remaining-resource]
=== Deprecated resources remain after upgrade to Errata releases
//2.6X: 26987

After you upgrade from 2.4.x to 2.5.x, and then to 2.6.x, deprecated resources in the managed cluster namespace might remain. You need to manually delete these deprecated resources if version 2.6.x was upgraded from 2.4.x:

*Note:* You need to wait 30 minutes or more before you upgrade from version 2.5.x to version 2.6.x.

You can delete from the console, or you can run a command similar to the following example for the resources you want to delete:

----
oc delete -n <managed cluster namespace> managedclusteraddons.addon.open-cluster-management.io <resource-name> 
----
 
See the list of deprecated resources that might remain:

----
managedclusteraddons.addon.open-cluster-management.io:
policy-controller
manifestworks.work.open-cluster-management.io:
-klusterlet-addon-appmgr
-klusterlet-addon-certpolicyctrl
-klusterlet-addon-crds
-klusterlet-addon-iampolicyctrl
-klusterlet-addon-operator
-klusterlet-addon-policyctrl
-klusterlet-addon-workmgr
----

[#upgrade-pod-not-up]
=== Pods might not come back up after upgrading {product-title-short}
// 2.5, 2.4: 23730

After upgrading {product-title-short} to a new version, a few pods that belong to a `StatefulSet` might remain in a `failed` state. This infrequent event is caused by a known https://github.com/kubernetes/kubernetes/issues/60164[Kubernetes issue].

As a workaround for this problem, delete the failed pod. Kubernetes automatically relaunches it with the correct settings.

[#openshift-container-platform-cluster-upgrade-failed-status]
=== OpenShift Container Platform cluster upgrade failed status
// 2.0.0:3442

When an {ocp-short} cluster is in the upgrade stage, the cluster pods are restarted and the cluster might remain in `upgrade failed` status for a variation of 1-5 minutes. This behavior is expected and resolves after a few minutes.

[#create-multiclusterengine-button-not-working]
=== Create MultiClusterEngine button not working
//2.6:25641

After installing {product-title} in the {ocp} console, a pop-up window with the following message appears:

`MultiClusterEngine required`

`Create a MultiClusterEngine instance to use this Operator.`

The *Create MultiClusterEngine* button in the pop-up window message might not work. To work around the issue, select *Create instance* in the MultiClusterEngine tile in the Provided APIs section.

[#web-console-known-issues]
== Web console known issues

[#ldap-user-names-are-case-sensitive]
=== LDAP user names are case-sensitive
// 1.0.0:before 1.0.0.1

LDAP user names are case-sensitive.
You must use the name exactly the way it is configured in your LDAP directory.

[#console-features-might-not-display-in-firefox-earlier-versions]
=== Console features might not display in Firefox earlier version
// 1.0.0:before 1.0.0.1

There are known issues with dark theme styling for older versions of Firefox. Upgrade to the latest version for the best console compatibility.

For more information, see link:../install/requirements.adoc#supported-browsers[Supported browsers].

[#restrictions-for-storage-size-in-searchcustomization]
=== Restrictions for storage size in search customization
//2.2:8501

When you update the storage size in the `searchcustomization` CR, the PVC configuration does not change. If you need to update the storage size, update the PVC (`_<storageclassname>-search-redisgraph-0_`) with the following command:
----
oc edit pvc <storageclassname>-search-redisgraph-0
----

[#search-query-issue]
=== Search query parsing error
//2.5:22391 

If an environment becomes large and requires more tests for scaling, the search queries can timeout which results in a parsing error message being displayed. This error is displayed after 30 seconds of waiting for a search query.

Extend the timeout time with the following command:

----
kubectl annotate route multicloud-console haproxy.router.openshift.io/timeout=Xs
----

[#cannot-edit-namespace-bindings-for-cluster-set]
=== Cannot edit namespace bindings for cluster set
//2.6:25389

When you edit namespace bindings for a cluster set with the `admin` role or `bind` role, you might encounter an error that resembles the following message:

`ResourceError: managedclustersetbindings.cluster.open-cluster-management.io "<cluster-set>" is forbidden: User "<user>" cannot create/delete resource "managedclustersetbindings" in API group "cluster.open-cluster-management.io" in the namespace "<namespace>".`

To resolve the issue, make sure you also have permission to create or delete a `ManagedClusterSetBinding` resource in the namespace you want to bind. The role bindings only allow you to bind the cluster set to the namespace.

[#scrolling-hosted]
=== Horizontal scrolling does not work after provisioning hosted control plane cluster
//2.7:27107

After provisioning a hosted control plane cluster, you might not be able to scroll horizontally in the cluster overview of the {product-title-short} console if the `ClusterVersionUpgradeable` parameter is too long. You cannot view the hidden data as a result.

To work around the issue, zoom out by using your browser zoom controls, increase your {product-title-short} console window size, or copy and paste the text to a different location.

[#error-integrations-aap]
=== Error when using integrations with the {aap} Operator
//2.7:27113

If you use integrations that depend on the {aap-short} Operator and do not have permissions to view installed Operators on the {ocp} cluster, you might see an error message that resembles the following:

`The Ansible Automation Platform Operator is required to use automation templates. Version 2.2.1 or greater is required to use workflow job templates in automation templates.`

You can safely ignore the error message if you confirm with your system administrator that the Operator is installed.

[#observability-known-issues]
== Observability known issues

[#duplicate-local-clusters-in-kubernetes-service-level-overview-api-server-dashboard]
=== Duplicate local-clusters on Service-level Overview dashboard
//2.4:16885

When various hub clusters deploy {product-title-short} observability using the same S3 storage, _duplicate_ `local-clusters` can be detected and displayed within the _Kubernetes/Service-Level Overview/API Server_ dashboard. The duplicate clusters affect the results within the following panels: _Top Clusters_, _Number of clusters that has exceeded the SLO_, and _Number of clusters that are meeting the SLO_. The `local-clusters` are unique clusters associated with the shared S3 storage. To prevent multiple `local-clusters` from displaying within the dashboard, it is recommended for each unique hub cluster to deploy observability with a S3 bucket specifically for the hub cluster.

[#observability-endpoint-operator-fails-to-pull-image]
=== Observability endpoint operator fails to pull image
//2.2:9259

The observability endpoint operator fails if you create a pull-secret to deploy to the MultiClusterObservability CustomResource (CR) and there is no pull-secret in the `open-cluster-management-observability` namespace. When you import a new cluster, or import a Hive cluster that is created with {product-title-short}, you need to manually create a pull-image secret on the managed cluster.

For more information, see link:../observability/observability_enable.adoc#enabling-observability[Enabling observability].

[#missing-data-roks]
=== There is no data from ROKS clusters
//2.2.3:12114

{product-title-short} observability does not display data from a ROKS cluster on some panels within built-in dashboards. This is because ROKS does not expose any API server metrics from servers they manage. The following Grafana dashboards contain panels that do not support ROKS clusters: `Kubernetes/API server`, `Kubernetes/Compute Resources/Workload`, `Kubernetes/Compute Resources/Namespace(Workload)`

[#missing-etcd-data-roks]
=== There is no etcd data from ROKS clusters
//2.2.3:12114

For ROKS clusters, {product-title-short} observability does not display data in the _etcd_ panel of the dashboard.

[#observability-annotation-query-failed]
=== Metrics are unavailable in the Grafana console

* Annotation query failed in the Grafana console: 
// 2.1.0:5625
+
When you search for a specific annotation in the Grafana console, you might receive the following error message due to an expired token: 
+
`"Annotation Query Failed"`
+
Refresh your browser and verify you are logged into your hub cluster.

* Error in _rbac-query-proxy_ pod:
+
Due to unauthorized access to the `managedcluster` resource, you might receive the following error when you query a cluster or project:
+
`no project or cluster found`
+
Check the role permissions and update appropriately. See link:../access_control/rbac.adoc#role-based-access-control[Role-based access control] for more information. 

[#prometheus-data-loss]
=== Prometheus data loss on managed clusters
//2.4:17137

By default, Prometheus on OpenShift uses ephemeral storage. Prometheus loses all metrics data whenever it is restarted.

When observability is enabled or disabled on {ocp-short} managed clusters that are managed by {product-title-short}, the observability endpoint operator updates the `cluster-monitoring-config` `ConfigMap` by adding additional alertmanager configuration that restarts the local Prometheus automatically. 

[#error-ingesting-out-of-order-samples]
=== Error ingesting out-of-order samples
//2.4:15666

Observability `receive` pods report the following error message:

----
Error on ingesting out-of-order samples
----

The error message means that the time series data sent by a managed cluster, during a metrics collection interval is older than the time series data it sent in the previous collection interval. When this problem happens, data is discarded by the Thanos receivers and this might create a gap in the data shown in Grafana dashboards. If the error is seen frequently, it is recommended to increase the metrics collection interval to a higher value. For example, you can increase the interval to 60 seconds.

The problem is only noticed when the time series interval is set to a lower value, such as 30 seconds. Note, this problem is not seen when the metrics collection interval is set to the default value of 300 seconds.

[#observability-add-on-fails]
=== Grafana deployment fails on managed clusters
//2.6:24512

The Grafana instance does not deploy to the managed cluster if the size of the manifest exceeds 50 thousand bytes. Only the `local-cluster` appears in Grafana after you deploy observability.

[#grafana-dev-fails-upgrade]
=== Grafana deployment fails after upgrade
//2.6:25815

If you have a `grafana-dev` instance deployed in earlier versions before 2.6, and you upgrade the environment to 2.6, the `grafana-dev` does not work. You must delete the existing `grafana-dev` instance by running the following command:

----
./setup-grafana-dev.sh --clean
----

Recreate the instance with the following command:

----
./setup-grafana-dev.sh --deploy
----

[#klusterlet-addon-search-crashing]
=== _klusterlet-addon-search_ pod fails
//2.5:27173

The `klusterlet-addon-search` pod fails because the memory limit is reached. You must update the memory request and limit by customizing the `klusterlet-addon-search` deployment on your managed cluster. Edit the `ManagedclusterAddon` custom resource named `search-collector`, on your hub cluster. Add the following annotations to the `search-collector` and update the memory, `addon.open-cluster-management.io/search_memory_request=512Mi` and `addon.open-cluster-management.io/search_memory_limit=1024Mi`.

For example, if you have a managed cluster named `foobar`, run the following command to change the memory request to `512Mi` and the memory limit to `1024Mi`:

----
oc annotate managedclusteraddon search-collector -n foobar \
addon.open-cluster-management.io/search_memory_request=512Mi \
addon.open-cluster-management.io/search_memory_limit=1024Mi
----

[#hub-self-management-list-grafana]
=== Enabling _disableHubSelfManagement_ causes empty list in Grafana dashboard
//2.8:ACM-4942

The Grafana dashboard shows an empty label list if the `disableHubSelfManagement` parameter is set to `true` in the `mulitclusterengine` custom resource. You must set the parameter to `false` or remove the parameter to see the label list. See link:../install/adv_config_install.adoc#disable-hub-self-management[disableHubSelfManagement] for more details.

[fqdn-not-supported]
=== Endpoint URL cannot have fully qualified domain names (FQDN)
//2.7:ACM-4806

When you use the FQDN or protocol for the `endpoint` parameter, your observability pods are not enabled. The following error message is displayed:

[source,bash]
----
Endpoint url cannot have fully qualified paths
----

Enter the URL without the protocol. Your `endpoint` value must resemble the following URL for your secrets:

[source,bash]
----
endpoint: example.com:443
----

[#cluster-management-issues]
== Cluster management known issues

Cluster management or _Cluster lifecycle_ is provided by the {mce-short} with or without {product-title-short}. See the following known issues and limitations for Cluster management that apply to {product-title-short} only. Most cluster management known issues are located in the Cluster lifecycle documentation at link:../clusters/release_notes/known_issues.adoc#known-issues-cluster[Cluster lifecycle known issues]. 

[#no-ansible-power-hub]
=== Cannot use {aap-short} integration with an IBM Power or IBM Z system hub cluster
// 2.3:13523

You cannot use the {aap-short} integration when the {product-title} hub cluster is running on IBM Power or IBM Z systems because the link:https://catalog.redhat.com/software/containers/ansible-automation-platform/platform-resource-rhel7-operator/5f6a0f22592d9a52663ccab6[{aap-short} Resource Operator] does not provide `ppc64le` or `s390x` images.

[#application-management-known-issues]
== Application management known issues

See the following known issues for the application lifecycle component.

[#application-blocked-state]
=== Application in blocked state
//2.7.0: 27139

If an application is in a `blocked` state, the subscription displays that the cluster is offline, but the cluster is in `healthy` and `ready` status.

[#object-bucket-subscription-admin]
=== Application ObjectBucket channel type cannot use allow and deny lists
//2.5.0: 22807

You cannot specify allow and deny lists with ObjectBucket channel type in the `subscription-admin` role. In other channel types, the allow and deny lists in the subscription indicates which Kubernetes resources can be deployed, and which Kubernetes resources should not be deployed.
[#argo-app-set-version]
=== Argo Application cannot be deployed on 3.x {ocp-short} managed clusters

Argo `ApplicationSet` from the console cannot be deployed on 3.x {ocp-short} managed clusters because the `Infrastructure.config.openshift.io` API is not available on  on 3.x.

[#changes-not-automatic]
=== Changes to the multicluster_operators_subscription image do not take effect automatically
//2.5.0: 21446

The `application-manager` add-on that is running on the managed clusters is now handled by the subscription operator, when it was previously handled by the klusterlet operator. The subscription operator is not managed the `multicluster-hub`, so changes to the `multicluster_operators_subscription` image in the `multicluster-hub` image manifest ConfigMap do not take effect automatically.

If the image that is used by the subscription operator is overrided by changing the `multicluster_operators_subscription` image in the `multicluster-hub` image manifest ConfigMap, the `application-manager` add-on on the managed clusters does not use the new image until the subscription operator pod is restarted. You need to restart the pod.

[#policy-needs-subscription-admin]
=== Policy resource not deployed unless by subscription administrator
//2.4.0: 17819

The `policy.open-cluster-management.io/v1` resources are no longer deployed by an application subscription by default for {product-title-short} version 2.4.

A subscription administrator needs to deploy the application subscription to change this default behavior.

See link:../applications/allow_deny.adoc[Creating an allow and deny list as subscription administrator] for information. `policy.open-cluster-management.io/v1` resources that were deployed by existing application subscriptions in previous {product-title-short} versions remain, but are no longer reconciled with the source repository unless the application subscriptions are deployed by a subscription administrator.

[#application-ansible-standalone]
=== Application Ansible hook stand-alone mode
// 2.2:8036

Ansible hook stand-alone mode is not supported. To deploy Ansible hook on the hub cluster with a subscription, you might use the following subscription YAML:

[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: Subscription
metadata:
  name: sub-rhacm-gitops-demo
  namespace: hello-openshift
annotations:
  apps.open-cluster-management.io/github-path: myapp
  apps.open-cluster-management.io/github-branch: master
spec:
  hooksecretref:
      name: toweraccess
  channel: rhacm-gitops-demo/ch-rhacm-gitops-demo
  placement:
     local: true
----

However, this configuration might never create the Ansible instance, since the `spec.placement.local:true` has the subscription running on `standalone` mode. You need to create the subscription in hub mode. 

. Create a placement rule that deploys to `local-cluster`. See the following sample:
+
[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata: 
  name: <towhichcluster>
  namespace: hello-openshift
spec:
  clusterSelector:
    matchLabels:
      local-cluster: "true" #this points to your hub cluster
----

. Reference that placement rule in your subscription. See the following:
+
[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: Subscription
metadata:
  name: sub-rhacm-gitops-demo
  namespace: hello-openshift
annotations:
  apps.open-cluster-management.io/github-path: myapp
  apps.open-cluster-management.io/github-branch: master
spec:
  hooksecretref:
      name: toweraccess
  channel: rhacm-gitops-demo/ch-rhacm-gitops-demo
  placement:
     placementRef:
        name: <towhichcluster>
        kind: PlacementRule
----

After applying both, you should see the Ansible instance created in your hub cluster.

[#edit-role-for-application-error]
=== Edit role for application error
// 2.0.0:1681

A user performing in an `Editor` role should only have `read` or `update` authority on an application, but erroneously editor can also `create` and `delete` an application. {ocp-short} Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole applications.app.k8s.io-v1beta2-edit -o yaml` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#edit-role-for-placement-rule-error]
=== Edit role for placement rule error
// 2.0.0:3693

A user performing in an `Editor` role should only have `read` or `update` authority on an placement rule, but erroneously editor can also `create` and `delete`, as well. {ocp-short} Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole placementrules.apps.open-cluster-management.io-v1-edit` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#application-not-deployed-after-an-updated-placement-rule]
=== Application not deployed after an updated placement rule
// 1.0.0:1449

If applications are not deploying after an update to a placement rule, verify that the `application-manager` pod is running.
The `application-manager` is the subscription container that needs to run on managed clusters.

You can run `oc get pods -n open-cluster-management-agent-addon |grep application-manager` to verify.

You can also search for `kind:pod cluster:yourcluster` in the console and see if the `application-manager` is running.

If you cannot verify, attempt to import the cluster again and verify again.

[#subscription-operator-does-not-create-an-scc]
=== Subscription operator does not create an SCC
// 1.0.0:1764

Learn about {ocp} SCC at https://docs.openshift.com/container-platform/4.8/authentication/managing-security-context-constraints.html#security-context-constraints-about_configuring-internal-oauth[Managing Security Context Constraints (SCC)], which is an additional configuration required on the managed cluster.

Different deployments have different security context and different service accounts. The subscription operator cannot create an SCC CR automatically.. Administrators control permissions for pods. A Security Context Constraints (SCC) CR is required to enable appropriate permissions for the relative service accounts to create pods in the non-default namespace. To manually create an SCC CR in your namespace, complete the following steps:

. Find the service account that is defined in the deployments. For example, see the following `nginx` deployments:
+
----
nginx-ingress-52edb
nginx-ingress-52edb-backend
----
+
. Create an SCC CR in your namespace to assign the required permissions to the service account or accounts. See the following example, where `kind: SecurityContextConstraints` is added:
+
[source,yaml]
----
apiVersion: security.openshift.io/v1
 defaultAddCapabilities:
 kind: SecurityContextConstraints
 metadata:
   name: ingress-nginx
   namespace: ns-sub-1
 priority: null
 readOnlyRootFilesystem: false
 requiredDropCapabilities:
 fsGroup:
   type: RunAsAny
 runAsUser:
   type: RunAsAny
 seLinuxContext:
   type: RunAsAny
 users:
 - system:serviceaccount:my-operator:nginx-ingress-52edb
 - system:serviceaccount:my-operator:nginx-ingress-52edb-backend
----

[#application-channels-require-unique-namespaces]
=== Application channels require unique namespaces
// 1.0.0:2311

Creating more than one channel in the same namespace can cause errors with the hub cluster.

For instance, namespace `charts-v1` is used by the installer as a Helm type channel, so do not create any additional channels in `charts-v1`. Ensure that you create your channel in a unique namespace. All channels need an individual namespace, except GitHub channels, which can share a namespace with another GitHub channel.

[#ansible-automation-platform-fail]
=== {aap-short} job fail

Ansible jobs fail to run when you select an incompatible option. {aap-short} only works when the `-cluster-scoped` channel options are chosen. This affects all components that need to perform Ansible jobs.

[#ansible-automation-operator-access]
=== {aap-short} operator access {aap-short} outside of a proxy

The {aap} operator cannot access {aap-short} outside of a proxy-enabled {ocp-short} cluster. To resolve, you can install the {aap-short} within the proxy. See install steps that are provided by {aap-short}.

[#application-name]
=== Application name requirements
// 2.3:#14310

An application name cannot exceed 37 characters. The application deployment displays the following error if the characters exceed this amount.

[source,yaml]
----
status:
  phase: PropagationFailed
  reason: 'Deployable.apps.open-cluster-management.io "_long_lengthy_name_" is invalid: metadata.labels: Invalid value: "_long_lengthy_name_": must be no more than 63 characters/n'
----

[#application-tables]
=== Application console table limitations
// 2.3:12410

See the following limitations to various _Application_ tables in the console:

- From the _Applications_ table on the _Overview_ page and the _Subscriptions_ table on the _Advanced configuration_ page, the _Clusters_ column displays a count of clusters where application resources are deployed. Since applications are defined by resources on the local cluster, the local cluster is included in the search results, whether actual application resources are deployed on the local cluster or not.

- From the _Advanced configuration_ table for _Subscriptions_, the _Applications_ column displays the total number of applications that use that subscription, but if the subscription deploys child applications, those are included in the search result, as well.

- From the _Advanced configuration_ table for _Channels_, the _Subscriptions_ column displays the total number of subscriptions on the local cluster that use that channel, but this does not include subscriptions that are deployed by other subscriptions, which are included in the search result.

[#app-topology]
=== No Application console topology filtering

The _Console_ and _Topology_ for _Application_ changes for the {product-version}. There is no filtering capability from the console Topology page.

[#allow-deny-list-not-working-objectstorage-app]
=== Allow and deny list does not work in Object storage applications
// 2.6:25445

The `allow` and `deny` list feature does not work in Object storage application subscriptions.

[#governance-known-issues]
== Governance known issues

== The _ignorePending_ flag is ignored by the policy generator
//2.7:6348

The `ignorePending` flag is ignored if you set `consolidateManifests: true` in your policy generator.

You can set `consolidateManifests: false` if you need to implement the `ignorePending` function.

[#unable-to-log-out]
=== Unable to log out from Red Hat Advanced Cluster Management

When you use an external identity provider to log in to {product-title-short}, you might not be able to log out of {product-title-short}. This occurs when you use {product-title-short}, installed with IBM Cloud and Keycloak as the identity providers.

You must log out of the external identity provider before you attempt to log out of {product-title-short}. 

[#gatekeeper-upgrade]
=== Gatekeeper operator installation fails
//2.4:16673

When you install the gatekeeper operator on {ocp} version 4.9, the installation fails. Before you upgrade {ocp-short} to version 4.9.0., you must upgrade the gatekeeper operator to version 0.2.0. See link:../governance/create_gatekeeper.adoc#upgrading-gatekeeper-gatekeeper-operator[Upgrading gatekeeper and the gatekeeper operator] for more information.

[#config-policy-stuck]
=== Configuration policy listed complaint when namespace is stuck in _Terminating_ state
//2.2:20715

When you have a configuration policy that is configured with `mustnothave` for the `complianceType` parameter and `enforce` for the `remediationAction` parameter, the policy is listed as compliant after a deletion request is made to the Kubernetes API. Therefore, the Kubernetes object can be stuck in a `Terminating` state while the policy is listed as compliant.

[#operators-deployed-with-policies]
=== Operators deployed with policies do not support ARM

While installation into an ARM environment is supported, operators that are deployed with policies might not support ARM environments. The following policies that install operators do not support ARM environments:

* link:https://github.com/stolostron/policy-collection/blob/main/stable/SI-System-and-Information-Integrity/policy-imagemanifestvuln.yaml[{product-title-short} policy for the Quay Container Security Operator]
* link:https://github.com/stolostron/policy-collection/blob/main/stable/CA-Security-Assessment-and-Authorization/policy-compliance-operator-install.yaml[{product-title-short} policy for the Compliance Operator]

[#configurationpolicy-crd-terminating]
=== ConfigurationPolicy CRD is stuck in terminating

When you remove the `config-policy-controller` add-on from a managed cluster by disabling the policy controller in the `KlusterletAddonConfig` or by detaching the cluster, the `ConfigurationPolicy` CRD might get stuck in a terminating state. If the `ConfigurationPolicy` CRD is stuck in a terminating state, new policies might not be added to the cluster if the add-on is reinstalled later. You can also receive the following error:

----
template-error; Failed to create policy template: create not allowed while custom resource definition is terminating
----

Use the following command to check if the CRD is stuck: 

----
oc get crd configurationpolicies.policy.open-cluster-management.io -o=jsonpath='{.metadata.deletionTimestamp}'
----

If a deletion timestamp is on the resource, the CRD is stuck. To resolve the issue, remove all finalizers from configuration policies that remain on the cluster. Use the following command on the managed cluster and replace `<cluster-namespace>` with the managed cluster namespace:

----
oc get configurationpolicy -n <cluster-namespace> -o name | xargs oc patch -n <cluster-namespace> --type=merge -p '{"metadata":{"finalizers": []}}'
----

The configuration policy resources are automatically removed from the cluster and the CRD exits its terminating state. If the add-on has already been reinstalled, the CRD is recreated automatically without a deletion timestamp.

[#pruneobjbeh-not-working-existing-config-policy]
== _pruneObjectBehavior_ does not work when modifying existing configuration policy
//2.6:25261
//2.7+2.8:5939

When you modify an existing configuration policy, `pruneObjectBehavior` does not work. View the following reasons why `pruneObjectBehavior` might not work:

- If you set `pruneObjectBehavior` to `DeleteAll` or `DeleteIfCreated` in a configuration policy, old resources that were created before modifying are not cleaned correctly. Only new resources from policy creations and policy updates are tracked and deleted when you delete the configuration policy.

- If you set `pruneObjectBehavior` to `None` or do not set the parameter value, old objects might be unintentionally deleted on the managed cluster. Specifically, this occurs when a user changes the `name`, `namespace`, `kind`, or `apiversion` in the template. The parameter fields can dynamically change when the `object-templates-raw` or `namespaceSelector` parameters change.

[#policy-status-repeated-updates]
=== Policy status shows repeated updates when enforced

If a policy is set to `remediationAction: enforce` and is repeatedly updated, the {product-title-short} console shows repeated violations with successful updates. This might happen in the following two cases:

- Another controller or process is also updating the object with different values.
+
To resolve the issue, disable the policy and compare the differences between `objectDefinition` in the policy and the object on the managed cluster. If the values are different, another controller or process might be updating them. Check the `metadata` of the object to help identify why the values are different.

- The `objectDefinition` in the `ConfigurationPolicy` does not match because Kubernetes processing the object when the policy is applied.
+
To resolve the issue, disable the policy and compare the differences between `objectDefinition` in the policy and the object on the managed cluster. If the keys are different or missing, Kubernetes might have processed the keys before applying them to the object, such as removing keys containing default or empty values.
+
Known examples:
+
|===
| Kind | Issue description

|`PodSecurityPolicy`
| Kubernetes removes keys with values set to `false`, which you can see in the resulting object on the managed cluster. In this case, remove the keys from the `objectDefinition` in the policy.

|`Secret`
| The `stringData` map is processed by Kubernetes to `data` with `base64` encoded values. Instead of using `stringData`, use `data` directly with `base64` encoded values instead of strings.

|===

[#policy-templates-not-removed]
=== Policy template issues

You might encounter the following issues when you edit policy templates for configuration policies:

- When you rename your configuration policy to a new name, a copy of the configuration policy with the older name remains. 

- If you remove a configuration policy from a policy on your hub cluster, the configuration policy remains on your managed cluster but its status is not provided.

To resolve this, disable your policy and reenable it. You can also delete the entire policy.

[#psp-not-supported-ocp]
=== Pod security policies not supported on OpenShift 4.12 and later

The support of pod security policies is removed from {ocp-short} 4.12 and later, and from Kubernetes v1.25 and later. If you apply a `PodSecurityPolicy` resource, you might receive the following non-compliant message:

----
violation - couldn't find mapping resource with kind PodSecurityPolicy, please check if you have CRD deployed
----

[#duplicate-ansible-jobs]
== Duplicate Ansible jobs are created for policy automations  
//2.7+2.8:ACM-5644

If you have a `PolicyAutomation` that is set to _Run once_ mode and disabled, an extra Ansible job is created. You can delete the extra Ansible job. Complete the following steps:

. Run the following command to view the Ansible job list:
+
[source,bash]
----
oc get ansiblejob -n {namespace}
----

. Delete the duplicate Ansible job by using the following command:
+
[source,bash]
----
oc delete ansiblejob {ansiblejob name} -n {namespace}
----

[#backup-known-issues]
== Backup and restore known issues

[#backup-oadp-failed-validation]
=== _BackupSchedule_ shows a _FailedValidation_ status when using OADP 1.1.2, or later
//2.8:OADP-1511

After you enable the {product-title-short} backup and restore component and successfully create a `DataProtectionApplication` resource, a `BackupStorageLocation` resource is created with a status of `Available`. When you are using OADP version 1.1.2 or later, you might receive the following message after you create a `BackupSchedule` resource and the status is `FailedValidation`:

----
oc get backupschedule -n open-cluster-management-backup
NAME PHASE MESSAGE
rosa-backup-schedule FailedValidation Backup storage location is not available. Check velero.io.BackupStorageLocation and validate storage credentials.
----

The error is caused by a missing value for `ownerReference` in the `BackupStorageLocation` resource. The value of the `DataProtectionApplication` resource should be used as the value of the `ownerReference`.

To work around the problem, manually add the `ownerReference` to the `BackupStorageLocation`:

. Open the `oadp-operator.v1.1.2` file by running the following command:
+
----
oc edit csv -n open-cluster-management-backup oadp-operator.v1.1.2
----

. Edit the value of `spec.deployments.label.spec.replicas` by replacing the `1` with a `0` in the OADP operator CSV.

. Patch the `ownerReference` annotations in the YAML script as shown in the following example:
+
[source,yaml]
----
metadata:
resourceVersion: '273482'
name: dpa-sample-1
uid: 4701599a-cdf5-48ac-9264-695a95b935a0
namespace: open-cluster-management-backup
ownerReferences: <<

apiVersion: oadp.openshift.io/v1alpha1
blockOwnerDeletion: true
controller: true
kind: DataProtectionApplication
name: dpa-sample
uid: 52acd151-52fd-440a-a846-95a0d7368ff7
----

. Change the value of `spec.deployments.label.spec.replicas` back to `1` to start the data protection application process with the new settings. 

[#restore-limitations]
=== Velero restore limitations
A new hub cluster can have a different configuration than the active hub cluster if the new hub cluster, where the data is restored, has user-created resources. For example, this can include an existing policy that was created on the new hub cluster before the backup data is restored on the new hub cluster.

Velero skips existing resources if they are not part of the restored backup, so the policy on the new hub cluster remains unchanged, resulting in a different configuration between the new hub cluster and active hub cluster.

To address this limitation, the cluster backup and restore operator runs a post restore operation to clean up the resources created by the user or a different restore operation when a `restore.cluster.open-cluster-management.io` resource is created.

For more information, see _Cleaning the hub cluster before restore_ in the link:../backup_restore/manage_backup_restore.adoc#manage-backup-and-restore[Managing the backup and restore operator] topic. 

[#imported-clusters-not-displayed]
=== Passive configurations do not display managed clusters

Managed clusters are only displayed when the activation data is restored on the passive hub cluster.

[#upgrade-limitation]
=== Cluster backup and restore upgrade limitation

If you upgrade your cluster from {product-version-prev} to {product-version} with the `enableClusterBackup` parameter set to `true`, the following message appears:

----
When upgrading from version 2.4 to 2.5, cluster backup must be disabled
----

Before you upgrade your cluster, disable cluster backup and restore by setting the `enableClusterBackup` parameter to `false`. The `components` section in your `MultiClusterHub` resource might resemble the following YAML file:

You can reenable the backup and restore component when the upgrade is complete. View the following sample:

[source,yaml]
----
overrides:
      components:
        - enabled: true
          name: multiclusterhub-repo
        - enabled: true
          name: search
        - enabled: true
          name: management-ingress
        - enabled: true
          name: console
        - enabled: true
          name: insights
        - enabled: true
          name: grc
        - enabled: true
          name: cluster-lifecycle
        - enabled: true
          name: volsync
        - enabled: true
          name: multicluster-engine
        - enabled: false
          name: cluster-proxy-addon
        - enabled: true <<<<<<<< 
          name: cluster-backup
    separateCertificateManagement: false
----

If you have manually installed OADP, you must manually uninstall OADP before you upgrade. After the upgrade is successful and backup and restore is reenabled, OADP is installed automatically.

[#managed-cluster-resources-not-restored]
=== Managed cluster resource not restored
//2.5:22402

When you restore the settings for the `local-cluster` managed cluster resource and overwrite the `local-cluster` data on a new hub cluster, the settings are misconfigured. Content from the previous hub cluster `local-cluster` is not backed up because the resource contains `local-cluster` specific information, such as the cluster URL details.

You must manually apply any configuration changes that are related to the `local-cluster` resource on the restored cluster. See _Prepare the new hub cluster_ in the link:../backup_restore/manage_backup_restore.adoc#manage-backup-and-restore[Managing the backup and restore operator] topic.

[#restored-hive-managed-clusters-unable-new-hub]
=== Restored Hive managed clusters might not be able to connect with the new hub cluster
//2.6:23930

When you restore the backup of the changed or rotated certificate of authority (CA) for the Hive managed cluster, on a new hub cluster, the managed cluster fails to connect to the new hub cluster. The connection fails because the `admin` `kubeconfig` secret for this managed cluster, available with the backup, is no longer valid. 

You must manually update the restored `admin` `kubeconfig` secret of the managed cluster on the new hub cluster.

[#imported-managed-clusters-pending-import]
=== Imported managed clusters show a _Pending Import_ status
//2.7:26797

Managed clusters that are manually imported on the primary hub cluster show a `Pending Import` status when the activation data is restored on the passive hub cluster. For more information, see link:../backup_restore/manage_backup_restore.adoc#auto-connect-clusters-msa[Automatically connecting clusters by using a Managed Service Account].

[#appliedmanifestwork-not-removed]
=== The _appliedmanifestwork_ is not removed from managed clusters after restoring the hub cluster
//2.7:27129

When the hub cluster data is restored on the new hub cluster, the `appliedmanifestwork` is not removed from managed clusters that have a placement rule for an application subscription that is not a fixed cluster set.

See the following example of a placement rule for an application subscription that is not a fixed cluster set:

[source,yaml]
----
spec:
  clusterReplicas: 1
  clusterSelector:
    matchLabels:
      environment: dev
----

As a result, the application is orphaned when the managed cluster is detached from the restored hub cluster.

To avoid the issue, specify a fixed cluster set in the placement rule. See the following example:

[source,yaml]
----
spec:
  clusterSelector:
    matchLabels:
      environment: dev
----

You can also delete the remaining `appliedmanifestwork` manually by running the folowing command:

----
oc delete appliedmanifestwork <the-left-appliedmanifestwork-name>
----

[#backup-placement-clusterset]
=== The _appliedmanifestwork_ is not removed and hub cluster placement rule does not have a fixed cluster set
//2.7+:ACM-7588

When the hub cluster data is restored on the new hub cluster, the `appliedmanifestwork` is not removed from managed clusters that have a placement rule for an application subscription that is not a fixed cluster set. As a result, the application is orphaned when the managed cluster is detached from the restored hub cluster.

See the following example of a placement rule for an application subscription that is not a fixed cluster set:

+
[source,yaml]
----
spec:
  clusterReplicas: 1
  clusterSelector:
    matchLabels:
      environment: dev 
----

To avoid the issue, specify a fixed cluster set in the placement rule. See the following example:

+
[source,yaml]
----
spec:
  clusterSelector:
    matchLabels:
      environment: dev 
----

You can also delete the remaining `appliedmanifestwork` manually by running the following command:

----
oc delete appliedmanifestwork <the-left-appliedmanifestwork-name>
----

[#appliedmanifest-agentid-missing]
=== _appliedmanifestwork_ not removed and _agentID_ is missing in the specification
//2.7+:ACM-7588

When you are using {product-title-short} 2.6 as your primary hub cluster, but your restore hub cluster is on version 2.7 or later, the `agentID` is missing in the specification of `appliedmanifestworks` because the field is introduced in the 2.7 release. This results in the extra `appliedmanifestworks` for the primary hub on the managed cluster.

To avoid the issue, upgrade the primary hub cluster to {product-title-short} 2.7, then restore the backup on a new hub cluster.

Fix the managed clusters by setting the `spec.agentID` manually for each `appliedmanifestwork`.

. Run the following command to get the `agentID`: 
+
----
oc get klusterlet klusterlet -o jsonpath='{.metadata.uid}'
----

. Run the following command to set the `spec.agentID` for each `appliedmanifestwork`:
+
----
oc patch appliedmanifestwork <appliedmanifestwork_name> --type=merge -p '{"spec":{"agentID": "'$AGENT_ID'"}}'  
----
 
[#msa-status-unknown]
=== The _managed-serviceaccount_ add-on status shows _Unknown_
//2.8:ACM-5887

The managed cluster `appliedmanifestwork` `addon-managed-serviceaccount-deploy` is removed from the imported managed cluster if you are using the Managed Service Account without enabling it on the {mce} resource of the new hub cluster.

The managed cluster is still imported to the new hub cluster, but the `managed-serviceaccount` add-on status shows `Unknown`.

You can recover the `managed-serviceaccount` add-on after enabling the Managed Service Account in the {mce-short} resource. See link:../backup_restore/manage_backup_restore.adoc#enabling-auto-import[Enabling automatic import] to learn how to enable the Managed Service Account.

[#submariner-known-issues]
== Submariner known issues

[#not-all-infrastructure]
=== Not all of the infrastructure providers that {product-title-short} can manage are supported

Submariner is not supported with all of the infrastructure providers that {product-title-short} can manage. Refer to the https://access.redhat.com/articles/6978968[{product-title-short} support matrix] for a list of supported providers.

[#headless-services-globalnet]
=== Limited headless services support
//2.5:24159

Service discovery is not supported for headless services without selectors when using Globalnet.

[#submariner-vxlan]
=== Deployments that use VXLAN when NAT is enabled are not supported
//2.5:24258

Only non-NAT deployments support Submariner deployments with the VXLAN cable driver.

[#submariner-ovn-k8]
=== OVN Kubernetes requires OCP 4.11 and later
//2.6:25275

If you are using the OVN Kubernetes CNI network, you need Red Hat OpenShift 4.11 or later.

[#globalnet-limitations]
=== Globalnet limitations
//2.5:26901

Globalnet is not supported with Red Hat OpenShift Data Foundation disaster recovery solutions. Make sure to use a non-overlapping range of private IP addresses for the cluster and service networks in each cluster for regional disaster recovery scenarios.

[#certificates-prevent-connection-broker]
=== Self-signed certificates might prevent connection to broker
//2.7:27008

Self-signed certificates on the broker might prevent joined clusters from connecting to the broker. The connection fails with certificate validation errors. You can disable broker certificate validation by setting `InsecureBrokerConnection` to `true` in the relevant `SubmarinerConfig` object. See the following example:

[source,yaml]
----
apiVersion: submarineraddon.open-cluster-management.io/v1alpha1
kind: SubmarinerConfig
metadata:
   name: submariner
   namespace: <managed-cluster-namespace>
spec:
   insecureBrokerConnection: true
----

[#submariner-sdn-cni]
=== Submariner only supports OpenShift SDN or OVN Kubernetes
//2.8:ACM-5306
Submariner only supports {ocp} clusters that use the OpenShift SDN or the OVN-Kubernetes Container Network Interface (CNI) network provider.

[#submariner-diagnose-azure]
=== Command limitation on Microsoft Azure clusters
//2.8:ACM-5327

The `subctl diagnose firewall inter-cluster` command does not work on Microsoft Azure clusters.

[#editapplicationset-expand-feature-repeats]
== _EditApplicationSet_ expand feature repeats
When you add multiple label expressions or attempt to enter your cluster selector for your `ApplicationSet`, you might receive the following message repeatedly,  "Expand to enter expression". You can enter your cluster selection despite this issue.

[#submariner-upgrade-limit]
=== Automatic upgrade not working with custom _CatalogSource_ or _Subscription_

Submariner is automatically upgraded when {product-title} is upgraded. The automatic upgrade might fail if you are using a custom `CatalogSource` or `Subscription`.

To make sure automatic upgrades work when installing Submariner on managed clusters, you must set the `spec.subscriptionConfig.channel` field to `stable-0.14` in the `SubmarinerConfig` custom resource for each managed cluster.
