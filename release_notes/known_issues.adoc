[#known-issues]
= Known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue with workaround if:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)

Or consider a troubleshooting topic.
////

Review the known issues for {product-title}. The following list contains known issues for this release, or known issues that continued from the previous release. For your {ocp} cluster, see https://docs.openshift.com/container-platform/4.3/release_notes/ocp-4-3-release-notes.html#ocp-4-3-known-issues[{ocp-short} known issues].

* <<installation-known-issues,Installation known issues>>
* <<web-console-known-issues,Web console known issues>>
** <<observability-known-issues,Observability known issues>>
* <<cluster-management-issues,Cluster management known issues>>
* <<application-management-known-issues,Application management known issues>>
* <<governance-known-issues,Governance known issues>>

[#installation-known-issues]
== Installation known issues

[#ocp-49-compat-2.3]
=== Compatibility issues with {ocp-short} version 4.9

The functional testing to determine the compatibility between {ocp-short} version 4.9 and {product-title-short} version 2.3 is still in progress. Until the compatibility between them is verified, there might be compatibility issues when using {product-title-short} version 2.3 on {ocp-short} version 4.9. 

[#upgrade-231-error-chart]
=== Upgrade from version 2.2.x to 2.3.1 upgrade fails to progress
// 2.3.1: 15425

When you upgrade your {product-title-short} from version 2.2.x to 2.3.1, the upgrade fails. The `Multiclusterhub` status displays: `failed to download chart from helm repo` in the component error messages. You may also see errors that reference a problem with `no endpoints available for service "ocm-webhook"`.

On your hub cluster, run the following command in the namespace where {product-title-short} is installed to restart deployments and upgrade to version 2.3.1:

----
oc delete deploy ocm-proxyserver ocm-controller ocm-webhook multiclusterhub-repo
----

*Note:* The errors resolve, but the reconciliation process might not start immediately. This can be accelerated by restarting the `multicluster-operators-standalone-subscription` in the same namespace that the product is installed.

[#upgrade-230-231-error]
=== Upgrade from version 2.3.0 to 2.3.1 fails with a ImagePullBackOff error
// 2.3.1: 15059

When you upgrade your {product-title-short} from version 2.3.0 to 2.3.1, the `klusterlet-addon-operator` pod in the `open-cluster-management-agent-addon` namespace on your managed clusters returns an `ImagePullBackOff` error.

Complete the following steps on your hub cluster to fix this issue and upgrade to version 2.3.1:

. Run the following commands to delete the `ConfigMap` for the `MultiClusterHub` manifest:
+
----
oc delete cm -n open-cluster-management mch-image-manifest-2.3.0
----

. Run the following command to restart the pod for the controller:
+
----
oc delete po -n open-cluster-management -lapp=klusterlet-addon-controller-v2
----

. If the Grafana pod on the `open-cluster-management-observability` namespace of the hub also returns an `ImagePullBackOff` error after upgrading from 2.3.0 to 2.3.1, run the following command to restart the pod so it uses the correct images:
+
----
oc delete po -n open-cluster-management -lname=multicluster-observability-operator
----

You are running {product-title-short} version 2.3.1. 

[#openshift-container-platform-cluster-upgrade-failed-status]
=== OpenShift Container Platform cluster upgrade failed status
// 2.0.0:3442

When an {ocp-short} cluster is in the upgrade stage, the cluster pods are restarted and the cluster might remain in `upgrade failed` status for a variation of 1-5 minutes. This behavior is expected and resolves after a few minutes.

[#web-console-known-issues]
== Web console known issues

[#search-result-node]
=== Node discrepancy between Cluster page and search results
// 2.0, 2.1, 2.2:9987

You might see a discrepancy between the nodes displayed on the _Cluster_ page and the _Search_ results.

[#ldap-user-names-are-case-sensitive]
=== LDAP user names are case-sensitive
// 1.0.0:before 1.0.0.1

LDAP user names are case-sensitive.
You must use the name exactly the way it is configured in your LDAP directory.

[#console-features-might-not-display-in-firefox-earlier-versions]
=== Console features might not display in Firefox earlier version
// 1.0.0:before 1.0.0.1

The product supports Mozilla Firefox 74.0 or the latest version that is available for Linux, macOS, and Windows.
Upgrade to the latest version for the best console compatibility.

[#unable-to-search-using-values-with-empty-spaces]
=== Unable to search using values with empty spaces
// 1.0.0:1726

From the console and Visual Web Terminal, users are unable to search for values that contain an empty space.

[#at-logout-user-kubeadmin-gets-extra-browser-tab-with-blank-page]
=== At logout user kubeadmin gets extra browser tab with blank page

// 1.0.0:2191

When you are logged in as `kubeadmin` and you click the *Log out* option in the drop-down menu, the console returns to the login screen, but a browser tab opens with a `/logout` URL.
The page is blank and you can close the tab without impact to your console.

[#secret-content-is-no-longer-displayed]
=== Secret content is no longer displayed

// 2.1.0:6108

For security reasons, search does not display the contents of secrets found on managed clusters. When you search for a secret from the console, you might receive the following error message:

----
Unable to load resource data - Check to make sure the cluster hosting this resource is online
----

[#restrictions-for-storage-size-in-searchcustomization]
=== Restrictions for storage size in searchcustomization
//2.2:8501

When you update the storage size in the `searchcustomization` CR, the PVC configuration does not change. If you need to update the storage size, update the PVC (`_<storageclassname>-search-redisgraph-0_`) with the following command:
----
oc edit pvc <storageclassname>-search-redisgraph-0
----

[#observability-known-issues]
== Observability known issues

[#observability-endpoint-operator-fails-to-pull-image]
=== Observability endpoint operator fails to pull image
//2.2:9259

The observability endpoint operator fails if you create a pull-secret to deploy to the MultiClusterObservability CustomResource (CR) and there is no pull-secret in the `open-cluster-management-observability` namespace. When you import a new cluster, or import a Hive cluster that is created with {product-title-short}, you need to manually create a pull-image secret on the managed cluster.

For more information, see link:../observability/observability_enable.adoc#enabling-observability[Enabling observability].

[#missing-data-roks]
=== There is no data from ROKS cluster
//2.2.3:12114

{product-title-short} observability does not display data from an ROKS cluster on some panels within built-in dashboards. This is because ROKS does not expose any API Server metrics from servers they manage. The following Grafana dashboards contain panels that do not support ROKS clusters: `Kubernetes/API server`, `Kubernetes/Compute Resources/Workload`, `Kubernetes/Compute Resources/Namespace(Workload)`

[#missing-etcd-data-roks]
=== There is no etcd data from ROKS clusters
//2.2.3:12114

For ROKS clusters, {product-title-short} observability does not display data in the _etcd_ panel of the dashboard.

[#search-high-cpu]
=== High CPU usage by the search-collector pod
//2.3.13897

When search is disabled on a hub cluster that manages 1000 clusters, the `search-collector` pod CPU usage levels are higher than normal. For a four-day period, usage was approximately 2.148Mi of CPU. You can reduce memory usage by reducing the `search-collector` to `0` replicas.

[#search-pods-fail-invalid-certs]
=== Search pods fail to complete the TLS handshake due to invalid certificates
//2.3:14859

In some rare cases, the search pods are not automatically redeployed after certificates change. This causes a mismatch of certificates across the service pods, which causes the Transfer Layer Security (TLS) handshake to fail. To fix this problem, restart the search pods to reset the certificates.

[#observability-annotation-query-failed]
=== Metrics are unavailable in the Grafana console

* Annotation query failed in the Grafana console: 
// 2.1.0:5625
+
When you search for a specific annotation in the Grafana console, you might receive the following error message due to an expired token: 
+
`"Annotation Query Failed"`
+
Refresh your browser and verify you are logged into your hub cluster.

* Error in _rbac-query-proxy_ pod:
+
Due to unauthorized access to the `managedcluster` resource, you might receive the following error when you query a cluster or project:
+
`no project or cluster found`
+
Check the role permissions and update appropriately. See link:../access_control/rbac.adoc#role-based-access-control[Role-based access control] for more information. 

[#mco-error-during-upgrade]
=== Clusters are degraded after upgrade to 2.3.2
//2.3.2:16159

When you upgrade to {product-version}.2 where observability is enabled, some clusters are degraded because the observability add-on is not ready. Restart the `multicluster-observability-operator` pod by running the following command:

----
oc delete po multicluster-observability-operator -n open-cluster-management
----

The observability pod is recreated.

[#wrong-image-disconnected-env]
=== Observability stateful set uses wrong images in disconnected environment
//2.3.1:14817

In rare cases with disconnected environments, some pods of the observability `StatefulSet` are stuck in the following status, `ErrPullImage` because the pods cannot pull the images. The images defined in those pods are different from the ones defined in the related `StatefulSets`. To fix this problem, you need to delete the pods that use the wrong images. The pods restart automatically and should use the correct images.

[#cluster-management-issues]
== Cluster management known issues

[#cluster-namespace-terminiating-when-cluster-removed]
=== Namespace of the managed cluster stuck in terminating when deleting a managed cluster
//2.3:19236

In some cases, when you delete a managed cluster, the namespace of the managed cluster might be stuck in a terminating state because its `manifestworks` resources are not deleted. Run the following command to remove the remaining `manifestworks` resources and remove the namespace:

----
kubectl -n <managed-cluster-namespace> get manifestworks | grep -v NAME | awk '{print $1}' | xargs kubectl -n <managed-cluster-namespace> patch manifestworks -p '{"metadata":{"finalizers": []}}' --type=merge
----

[#region-typo-cluster-fail-not-displayed]
=== Invalid cluster deployment appears to be creating
//2.3:17363

When invalid information is provided when deploying a cluster, the uncreatable cluster status displays a `creating` status, even though it was not able to start. The invalid information, such as a region specified in the deployment that does not match the region specified within the `install-config` file, causes the following error to be added to the Hive provision pods: 

----
provision failed, requirements not met
----

{product-title-short} version 2.3 continues to report a status of `creating` for the cluster, though the invalid information is preventing the creation of the cluster.

[#iso-image-name-too-long]
=== Cluster provisioning with Infrastructure Operator fails
//2.4:17411

When creating {ocp-short} clusters using the Infrastructure Operator, the file name of the ISO image might be too long. The long image name causes the image provisioning and the cluster provisioning to fail. To determine if this is the problem, complete the following steps: 

. View the bare metal host information for the cluster that you are provisioning by running the following command: 
+
----
oc get bmh -n <cluster_provisioning_namespace>
----

. Run the `describe` command to view the error information:
+
----
oc describe bmh -n <cluster_provisioning_namespace> <bmh_name>
----

. An error similar to the following example indicates that the length of the filename is the problem: 
+
----
Status:
  Error Count:    1
  Error Message:  Image provisioning failed: ... [Errno 36] File name too long ...
----

If this problem occurs, it is typically on the following versions of {ocp-short}, because the infrastructure operator was not using image service:

* 4.8.17 and earlier

To avoid this error, upgrade your {ocp-short} to version 4.8.18 or later.

[#create-fail-gcp]
=== Cluster provisioning on Google Cloud Platform fails
//2.4:17930

When you try to provision a cluster on Google Cloud Platform (GCP), it might fail with the following error:

----
Cluster initialization failed because one or more operators are not functioning properly.
The cluster should be accessible for troubleshooting as detailed in the documentation linked below,
https://docs.openshift.com/container-platform/latest/support/troubleshooting/troubleshooting-installations.html
The 'wait-for install-complete' subcommand can then be used to continue the installation
----

You can work around this error by enabling the https://console.cloud.google.com/apis/library/networksecurity.googleapis.com[Network Security API] on the GCP project, which allows your cluster installation to continue. 

[#cluster-local-offline-reimport]
=== Local-cluster status offline after reimporting with a different name
//2.4:16977

When you accidentally try to reimport the cluster named `local-cluster` as a cluster with a different name, the status for `local-cluster` and for the reimported cluster display `offline.

To recover from this case, complete the following steps:

. Run the followiung command on the hub cluster to edit the setting for self-management of the hub cluster temporarily:
+
----
oc edit mch -n open-cluster-management multiclusterhub
----

. Add the setting `spec.disableSelfManagement=true`.

. Run the following command on the hub cluster to delete and redeploy the local-cluster:
+
----
oc delete managedcluster local-cluster
----

. Enter the following command to remove the `local-cluster` management setting: 
+
----
oc edit mch -n open-cluster-management multiclusterhub
----

. Remove `spec.disableSelfManagement=true` that you previously added.

[#failed-ansible-cluster-different-status]
=== Cluster status is different in different views of the console after a failed Ansible cluster creation
//2.3:15794

When you specify an invalid Ansible job template name while attempting to create a cluster and it fails, the cluster shows a different status on different screens of the console. When you view the status by selecting: *Infrastructure* > *Clusters* > *Managed clusters*, it displays a `Failed` status. When you select: *Infrastructure* > *Clusters* > *Cluster sets* > *`<your_cluster_set_name>`* > *Managed clusters*, the status remains stuck in `Creating`. The correct status is `Failed` for this case. You can try creating the cluster again, and enter the correct Ansible template name. 

[#local-cluster-not-reimported]
=== The local-cluster might not be automatically reimported
//2.3:15617

Sometimes, after you detach a local-cluster, the local-cluster might not be automatically reimported. When this happens, the local-cluster shows a constant status of `Pending Import` in the {product-title-short} console.

To reimport the local-cluster, complete the following steps:

. Delete the klusterlet deployment by running the following command:
+
----
oc -n open-cluster-management-agent delete deployment klusterlet
----

. Restart the `managedcluster-import-controller` by running the following command:
+
----
oc -n open-cluster-management get pods -l app=managedcluster-import-controller-v2 | awk 'NR>1{print $1}' | xargs oc -n open-cluster-management delete pods
----

[#clusterdeployment-stuck-terminating]
=== The clusterdeployment of the managed cluster is stuck in terminating state
//2.3:15518

When you delete a managed cluster that was created with the {product-title-short} console, the `clusterdeployment` of the managed cluster might get stuck in a terminating state. To bypass this issue and delete this `clusterdeployment`, manually delete the `agentclusterinstall.agent-install.openshift.io/ai-deprovision` finalizer by editing the `agentclusterinstall` resource for the cluster.

[#no-delete-cluster-namespace-before-remove-cluster]
=== Cannot delete managed cluster namespace manually
//2.3:13474

You cannot delete the namespace of a managed cluster manually. The managed cluster namespace is automatically deleted after the managed cluster is detached. If you delete the managed cluster namespace manually before the managed cluster is detached, the managed cluster shows a continuous terminating  status after you delete the managed cluster. To delete this terminating managed cluster, manually remove the finalizers from the managed cluster that you detached.

[#no-create-clusters-across-architectures]
=== Cannot create clusters across architectures
//2.2.3:14631

You cannot create a managed cluster on a different architecture than the architecture of the hub cluster without creating a release image (`ClusterImageSet`) that contains files for both architectures. For example, you cannot create an `x86_64` cluster from a `ppc64le` hub cluster. The cluster creation fails because the {ocp-short} release registry does not provide a multi-architecture image manifest. 

To work around this issue, complete the following steps:

. From the https://quay.io/repository/openshift-release-dev/ocp-release[{ocp-short} release registry], create a https://docs.docker.com/registry/spec/manifest-v2-2/[manifest list] that includes both `x86_64` and `ppc64le` release images.

.. Pull the manifest lists for both architectures from the Quay repository:
+
----
$ podman pull quay.io/openshift-release-dev/ocp-release:4.8.1-x86_64
$ podman pull quay.io/openshift-release-dev/ocp-release:4.8.1-ppc64le
----

.. Log in to your private repository where you maintain your images:
+
----
$ podman login <private-repo>
----
+
Replace `private-repo` with the path to your repository.

.. Add the release image manifest to your private repository by running the following commands:
+
----
$ podman push quay.io/openshift-release-dev/ocp-release:4.8.1-x86_64 <private-repo>/ocp-release:4.8.1-x86_64
$ podman push quay.io/openshift-release-dev/ocp-release:4.8.1-ppc64le <private-repo>/ocp-release:4.8.1-ppc64le
----
+
Replace `private-repo` with the path to your repository.

.. Create a manifest for the new information:
+
---- 
$ podman manifest create mymanifest
----

.. Add references to both release images to the manifest list:
+
----
$ podman manifest add mymanifest <private-repo>/ocp-release:4.8.1-x86_64
$ podman manifest add mymanifest <private-repo>/ocp-release:4.8.1-ppc64le
----
+
Replace `private-repo` with the path to your repository.

.. Merge the list in your manifest list with the existing manifest:
+
----
$ podman manifest push mymanifest docker://<private-repo>/ocp-release:4.8.1
----
+
Replace `private-repo` with the path to your repository.

. On the hub cluster, create a release image that references the manifest in your repository.

.. Create a `YAML` file that contains information that is similar to the following example:
+
----
apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  labels:
    channel: fast
    visible: "true"
  name: img4.8.1-appsub
spec:
  releaseImage: <private-repo>/ocp-release:4.8.1
----
+
Replace `private-repo` with the path to your repository.

.. Run the following command on your hub cluster to apply the changes:
+
----
oc apply -f <file-name>.yaml
----
+
Replace `file-name` with the name of the `YAML` file that you just created. 

. Select the new release image when you create your {ocp-short} cluster. 

The creation process uses the merged release images to create the cluster.  

[#no-reassign-cluster-label]
=== Cannot reassign a cluster to cluster set by changing label
//2.2.3:14408

You cannot reassign a cluster or cluster set from one cluster set to another by updating the label for the cluster to the new cluster set. To move a cluster or cluster set to another one, remove it from the cluster set by using the {product-title-short} console. After you remove it from the cluster set, add it to the new cluster set by using the console. 

[#no-ansible-power-hub]
=== Cannot use Ansible Tower integration with an IBM Power hub cluster
// 2.3:13523

You cannot use the Ansible Tower integration when the {product-title} hub cluster is running on IBM Power because the link:https://catalog.redhat.com/software/containers/ansible-automation-platform/platform-resource-rhel7-operator/5f6a0f22592d9a52663ccab6[Ansible Automation Platform Resource Operator] does not provide `ppc64le` images.

[#no-change-upgrade-cred]
=== Cannot change credentials on clusters after upgrading to version 2.3
//2.3:14098

After you upgrade {product-title-short} to version 2.3, you cannot change the credential secret for any of the managed clusters that were created and managed by {product-title-short} before the upgrade.  

[#no-create-bm-47]
=== Cannot create bare metal managed clusters on {ocp-short} version 4.8
// 2.2:10581

You cannot create bare metal managed clusters by using the {product-title-short} hub cluster when the hub cluster is hosted on {ocp-short} version 4.8.

[#create-resource-dropdown-error]
=== Create resource drop-down error
// 2.1:6299 Remove after 2.1.1????

When you detach a managed cluster, the _Create resources_ page might temporarily break and display the following error:

----
Error occurred while retrieving clusters info. Not found.
----

Wait until the namespace automatically gets removed, which takes 5-10 minutes after you detach the cluster. Or, if the namespace is stuck in a terminating state, you need to manually delete the namespace. Return to the page to see if the error resolved.

[#hub-managed-clusters-clock]
=== Hub cluster and managed clusters clock not synced
// 2.1:5636

Hub cluster and manage cluster time might become out-of-sync, displaying in the console `unknown` and eventually `available` within a few minutes. Ensure that the {ocp} hub cluster time is configured correctly. See https://docs.openshift.com/container-platform/4.6/installing/install_config/installing-customizing.html[Customizing nodes].

[#importing-certain-versions-of-ibm-red-hat-openshift-kubernetes-service-clusters-is-not-supported]
=== Importing certain versions of IBM {ocp-short} Kubernetes Service clusters is not supported
// 1.0.0:2179

You cannot import IBM {ocp-short} Kubernetes Service version 3.11 clusters.
Later versions of IBM OpenShift Kubernetes Service are supported.

[#detaching-openshift-container-platform-3.11-does-not-remove-the-open-cluster-management-agent]
=== Detaching {ocp-short} 3.11 does not remove the _open-cluster-management-agent_
// 2.0.0:3847

When you detach managed clusters on {ocp-short} 3.11, the `open-cluster-management-agent` namespace is not automatically deleted. Manually remove the namespace by running the following command:

----
oc delete ns open-cluster-management-agent
----

[#automatic-secret-updates-for-provisioned-clusters-is-not-supported]
=== Automatic secret updates for provisioned clusters is not supported
// 2.0.0:3702

When you change your cloud provider access key, the provisioned cluster access key is not updated in the namespace. This is required when your credentials expire on the cloud provider where the managed cluster is hosted and you try delete the managed cluster. If something like this occurs, run the following command for your cloud provider to update the access key: 

* Amazon Web Services (AWS)

+
----
oc patch secret {CLUSTER-NAME}-aws-creds -n {CLUSTER-NAME} --type json -p='[{"op": "add", "path": "/stringData", "value":{"aws_access_key_id": "{YOUR-NEW-ACCESS-KEY-ID}","aws_secret_access_key":"{YOUR-NEW-aws_secret_access_key}"} }]'
----

* Google Cloud Platform (GCP)

+
You can identify this issue by a repeating log error message that reads, `Invalid JWT Signature` when you attempt to destroy the cluster. If your log contains this message, obtain a new Google Cloud Provider service account JSON key and enter the following command:

+
----
oc set data secret/<CLUSTER-NAME>-gcp-creds -n <CLUSTER-NAME> --from-file=osServiceAccount.json=$HOME/.gcp/osServiceAccount.json
----
+
Replace `_CLUSTER-NAME_` with the name of your cluster.
+
Replace the path to the file `$HOME/.gcp/osServiceAccount.json` with the path to the file that contains your new Google Cloud Provider service account JSON key. 


* Microsoft Azure 

+
----
oc set data secret/{CLUSTER-NAME}-azure-creds -n {CLUSTER-NAME} --from-file=osServiceAccount.json=$HOME/.azure/osServiceAccount.json
----

* VMware vSphere

+
----
oc patch secret {CLUSTER-NAME}-vsphere-creds -n {CLUSTER-NAME} --type json -p='[{"op": "add", "path": "/stringData", "value":{"username": "{YOUR-NEW-VMware-username}","password":"{YOUR-NEW-VMware-password}"} }]'
----

[#no-run-mgt-ingress-non-root]
=== Cannot run management ingress as non-root user
//2.0:35532

You must be logged in as `root` to run the `management-ingress` service. 

[#node-information-from-the-managed-cluster-cannot-be-viewed-in-search]
=== Node information from the managed cluster cannot be viewed in search
// 2.0.2:4598

Search maps RBAC for resources in the hub cluster. Depending on user RBAC settings for {product-title-short}, users might not see node data from the managed cluster. Results from search might be different from what is displayed on the _Nodes_ page for a cluster.

[#cluster-might-not-be-destroyed]
=== Process to destroy a cluster does not complete
// 2.1.0:4748

When you destroy a managed cluster, the status continues to display `Destroying` after one hour, and the cluster is not destroyed. To resolve this issue complete the following steps:

. Manually ensure that there are no orphaned resources on your cloud, and that all of the provider resources that are associated with the managed cluster are cleaned up.

. Open the `ClusterDeployment` information for the managed cluster that is being removed by entering the following command:
+
----
oc edit clusterdeployment/<mycluster> -n <namespace>
----
+
Replace `_mycluster_` with the name of the managed cluster that you are destroying.
+
Replace `_namespace_` with the namespace of the managed cluster.

. Remove the `hive.openshift.io/deprovision` finalizer to forcefully stop the process that is trying to clean up the cluster resources in the cloud.

. Save your changes and verify that `ClusterDeployment` is gone.

. Manually remove the namespace of the managed cluster by running the following command:
+
----
oc delete ns <namespace>
----
+
Replace `_namespace_` with the namespace of the managed cluster.

[#no-upgrade-os-on-osd]
=== Cannot upgrade {ocp-short} managed clusters on {ocp-short} Dedicated with the console
// 2.2.0:8922

You cannot use the {product-title-short} console to upgrade {ocp-short} managed clusters that are in the {ocp-short} Dedicated environment.

[#work-manager-addon-search]
=== Work manager add-on search details
//2.3.0: 13715

The search details page for a certain resource on a certain managed cluster might fail. You must ensure that the work-manager add-on in the managed cluster is in `Available` status before you can search.

[#argo-not-supported-power]
=== Argo CD is not supported with IBM Power hub cluster
// 2.3:13524
The link:https://argo-cd.readthedocs.io/en/stable/[Argo CD] integration with {product-title-short} does not work on a {product-title-short} hub cluster that is running on IBM Power because there are no available `ppc64le` images.

[#non-ocp-logs]
=== Non-{ocp} managed clusters must have LoadBalancer enabled
//2.3:15705

Both {ocp} and non-{ocp-short} clusters support the pod log feature, however non-{ocp-short} clusters require `LoadBalancer` to be enabled to use the feature. Complete the following steps to enable `LoadBalancer`:

. Cloud providers have different `LoadBalancer` configurations. Visit your cloud provider documentation for more information. 
. Verify if `LoadBalancer` is enabled on your {product-title-short} by checking the `loggingEndpoint` in the status of `managedClusterInfo`. 
. Run the following command to check if the `loggingEndpoint.IP` or `loggingEndpoint.Host` has a valid IP address or host name:
+
----
oc get managedclusterinfo <clusterName> -n <clusterNamespace> -o json | jq -r '.status.loggingEndpoint'
----

For more information about the `LoadBalancer` types, see the _Service_ page in the link:https://kubernetes.io/docs/concepts/services-networking/service[Kubernetes documentation.]


[#application-management-known-issues]
== Application management known issues

[#application-search-undefined-error]
=== Application search undefined error Application table
//2.3.0: 16180

When you click on the `Search application by row` action button on the _Applications table_, you are directed to the _Search_ page with results that might differ from the same preset filters you can receive from the _Search_ resource from the _Application details_ page.

[#application-search-undefined-error-argo]
=== Application search undefined error Argo CD
//2.3.0: 16181

When clicking on the link `Search all related applications` from the application details page for an Argo CD application, the search page returns invalid preset filters. 

You can remove the directory filter in the search console to resolve this issue.

[#no-branch-application]
=== No branch information during application creation for proxy
//2.3.0: 14117

When you create a {product-title-short} application by using the *Create application* editor, you might receive the following error for Git repositories when your hub cluster is behind a proxy: 

`The connection to the Git repository failed. Cannot get branches.`

You can go to your Git repository URL instead to get the branch information. The application deploys properly when the branch is entered.

[#application-argo-search-error]
=== Application Argo search undefined error
//2.3.0: 14463

The cluster node search link for an Argo application might return `name:undefined`.

When you click the *Launch resource in search* link from the cluster node details of an Argo application, the search filter might contain `name:undefined`. 

Replace the `undefined` value with the cluster name in the cluster node details to resolve this error.

[#app-topology-not-grouped-properly]
=== Application topology clusters with multiple subscriptions not grouped properly 
//2.3.0: 14107

A cluster might not group properly in the _Application topology_ if the cluster is using multiple subscriptions.

When you deploy an application with multiple subscriptions, you might see that the _All subscriptions_ view does not group the cluster nodes properly. 

For instance, when you deploy an application with multiple subscriptions containing a mixed combination of _Helm_ and _Git_ repositories, the _All subscriptions_ view does not display statuses correctly for the resources within the Helm subscription.

View the topology from the individual subscription views instead to display the correct cluster node grouping information.

[#app-topology-switch]
=== Application topology subscription switch
//2.3.0: 14065

The Application topology can fail when you switch between application subscriptions by using the subscriptions drop-down menu.

To resolve, attempt to switch to another subscription, or refresh the browser to see a refresh of the topology display.

[#application-ansible-standalone]
=== Application Ansible hook stand-alone mode
// 2.2:8036

Ansible hook stand-alone mode is not supported. To deploy Ansible hook on the hub cluster with a subscription, you might use the following subscription YAML:

[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: Subscription
metadata:
  name: sub-rhacm-gitops-demo
  namespace: hello-openshift
annotations:
  apps.open-cluster-management.io/github-path: myapp
  apps.open-cluster-management.io/github-branch: master
spec:
  hooksecretref:
      name: toweraccess
  channel: rhacm-gitops-demo/ch-rhacm-gitops-demo
  placement:
     local: true
----

However, this configuration might never create the Ansible instance, since the `spec.placement.local:true` has the subscription running on `standalone` mode. You need to create the subscription in hub mode. 

. Create a placement rule that deploys to `local-cluster`. See the following sample:

+
[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata: 
  name: <towhichcluster>
  namespace: hello-openshift
spec:
  clusterSelector:
    matchLabels:
      local-cluster: "true" #this points to your hub cluster
----

. Reference that placement rule in your subscription. See the following:

+
[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: Subscription
metadata:
  name: sub-rhacm-gitops-demo
  namespace: hello-openshift
annotations:
  apps.open-cluster-management.io/github-path: myapp
  apps.open-cluster-management.io/github-branch: master
spec:
  hooksecretref:
      name: toweraccess
  channel: rhacm-gitops-demo/ch-rhacm-gitops-demo
  placement:
     placementRef:
        name: <towhichcluster>
        kind: PlacementRule
----

After applying both, you should see the Ansible instance created in your hub cluster.

[#application-local-cluster-limitation]
=== Application Deploy on local cluster limitation
// 2.1.0:6418

If you select *Deploy on local cluster* when you create or edit an application, the application Topology does not display correctly. *Deploy on local cluster* is the option to deploy resources on your hub cluster so that you can manage it as the `local cluster`, but this is not best practice for this release.

To resolve the issue, see the following procedure:

. Deselect the *Deploy on local cluster* option in the console.
. Select the *Deploy application resources only on clusters matching specified labels* option.
. Create the following label: `local-cluster : 'true'`.

[#namespace-channel-subscription-remains-in-failed-state]
=== Namespace channel subscription remains in failed state
// 2.0.0:3581

When you subscribe to a namespace channel and the subscription remains in `FAILED` state after you fixed other associated resources such as channel, secret, ConfigMap, or placement rule, the namespace subscription is not continuously reconciled. 

To force the subscription reconcile again to get out of `FAILED` state, complete the following steps:

. Log in to your hub cluster.
. Manually add a label to the subscription using the following command:

----
oc label subscriptions.apps.open-cluster-management.io the_subscription_name reconcile=true
----

[#edit-role-for-application-error]
=== Edit role for application error
// 2.0.0:1681

A user performing in an `Editor` role should only have `read` or `update` authority on an application, but erroneously editor can also `create` and `delete` an application. {ocp-short} Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole applications.app.k8s.io-v1beta2-edit -o yaml` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#edit-role-for-placement-rule-error]
=== Edit role for placement rule error
// 2.0.0:3693

A user performing in an `Editor` role should only have `read` or `update` authority on an placement rule, but erroneously editor can also `create` and `delete`, as well. {ocp-short} Operator Lifecycle Manager default settings change the setting for the product. To workaround the issue, see the following procedure:

1. Run `oc edit clusterrole placementrules.apps.open-cluster-management.io-v1-edit` to open the application edit cluster role.
2. Remove `create` and `delete` from the verbs list.
3. Save the change.

[#application-not-deployed-after-an-updated-placement-rule]
=== Application not deployed after an updated placement rule
// 1.0.0:1449

If applications are not deploying after an update to a placement rule, verify that the `klusterlet-addon-appmgr` pod is running.
The `klusterlet-addon-appmgr` is the subscription container that needs to run on endpoint clusters.

You can run `oc get pods -n open-cluster-management-agent-addon` to verify.

You can also search for `kind:pod cluster:yourcluster` in the console and see if the `klusterlet-addon-appmgr` is running.

If you cannot verify, attempt to import the cluster again and verify again.

[#subscription-operator-does-not-create-an-scc]
=== Subscription operator does not create an SCC
// 1.0.0:1764

Learn about {ocp} SCC at https://docs.openshift.com/container-platform/4.8/authentication/managing-security-context-constraints.html#security-context-constraints-about_configuring-internal-oauth[Managing Security Context Constraints (SCC)], which is an additional configuration required on the managed cluster.

Different deployments have different security context and different service accounts.
The subscription operator cannot create an SCC automatically.
Administrators control permissions for pods.
A Security Context Constraints (SCC) CR is required to enable appropriate permissions for the relative service accounts to create pods in the non-default namespace:

To manually create an SCC CR in your namespace, complete the following:

. Find the service account that is defined in the deployments.
For example, see the following `nginx` deployments:
+
----
 nginx-ingress-52edb
 nginx-ingress-52edb-backend
----

. Create an SCC CR in your namespace to assign the required permissions to the service account or accounts.
See the following example where `kind: SecurityContextConstraints` is added:
+
----
 apiVersion: security.openshift.io/v1
 defaultAddCapabilities:
 kind: SecurityContextConstraints
 metadata:
   name: ingress-nginx
   namespace: ns-sub-1
 priority: null
 readOnlyRootFilesystem: false
 requiredDropCapabilities:
 fsGroup:
   type: RunAsAny
 runAsUser:
   type: RunAsAny
 seLinuxContext:
   type: RunAsAny
 users:
 - system:serviceaccount:my-operator:nginx-ingress-52edb
 - system:serviceaccount:my-operator:nginx-ingress-52edb-backend
----

[#application-channels-require-unique-namespaces]
=== Application channels require unique namespaces
// 1.0.0:2311

Creating more than one channel in the same namespace can cause errors with the hub cluster.

For instance, namespace `charts-v1` is used by the installer as a Helm type channel, so do not create any additional channels in `charts-v1`. Ensure that you create your channel in a unique namespace. All channels need an individual namespace, except GitHub channels, which can share a namespace with another GitHub channel.

[#ansible-automation-platform-fail]
=== Ansible Automation Platform (early access) 2.0.0 job fail

When the Ansible Automation Platform (early access) 2.0.0 is installed, `AnsibleJobs` fails to run. If you want to submit prehook and posthook `AnsibleJobs` through {product-title-short}, use the Ansible Automation Platform Resource Operator 0.1.1.

[#application-name]
=== Application name requirements
// 2.3:#14310

An application name cannot exceed 37 characters. The application deployment displays the following error if the characters exceed this amount.

[source,yaml]
----
status:
  phase: PropagationFailed
  reason: 'Deployable.apps.open-cluster-management.io "_long_lengthy_name_" is invalid: metadata.labels: Invalid value: "_long_lengthy_name_": must be no more than 63 characters/n'
----

[#application-tables]
=== Application console tables
// 2.3:12410

See the following limitations to various _Application_ tables in the console:

- From the _Applications_ table on the _Overview_ page and the _Subscriptions_ table on the _Advanced configuration_ page, the _Clusters_ column displays a count of clusters where application resources are deployed. Since applications are defined by resources on the local cluster, the local cluster is included in the search results, whether actual application resources are deployed on the local cluster or not.

- From the _Advanced configuration_ table for _Subscriptions_, the _Applications_ column displays the total number of applications that use that subscription, but if the subscription deploys child applications, those are included in the search result, as well.

- From the _Advanced configuration_ table for _Channels_, the _Subscriptions_ column displays the total number of subscriptions on the local cluster that use that channel, but this does not include subscriptions that are deployed by other subscriptions, which are included in the search result.

[#governance-known-issues]
== Governance known issues

[#ans-job-cleanup-restarts-jobs]
=== Ansible Automation jobs continue to run hourly even though no new policy violations started the automation
//2.3:14928

In {ocp-short} 4.8 the TTL Controller for Finished Resources is enabled by default, which means jobs are removed hourly. This job cleanup causes the Ansible Automation Platform Resource Operator to rerun the associated automation. The automation runs again with the existing details in the `AnsibleJob` resource that was created by the policy framework. The details provided might include previously identified violations, which can mistakenly appear as a repeated violation. You can disable the controller that cleans up the jobs to prevent these duplicate violations. To disable the controller that cleans up the jobs, complete the following steps:

. Run the following command to edit the `kubeapiservers.operator.openshift.io` resource:
+
----
oc edit kubeapiservers.operator.openshift.io cluster
----

. Find the `unsupportedConfigOverrides` section.

. Update the `unsupportedConfigOverrides` section to contain content that resembles the following example, which disables the job cleanup feature:
+
[source,yaml]
----
  unsupportedConfigOverrides: 
    apiServerArguments:
      feature-gates:
      - TTLAfterFinished=false
----

. Run the following command to edit the `kubecontrollermanager` resource:
+
----
oc edit kubecontrollermanager cluster
----

. Complete steps 2 and 3 to update the same section in the `kubecontrollermanager` resource.

[#matchexpression-not-removed]
=== PlacementRule matchExpression not removed with new matchLabel
//2.3:16780
 
When you update a policy `PlacementRule` from `matchExpressions` to `matchLabels`, the old `matchExpression` is not removed.

See the following examples where `matchExpressions` in the first sample is changed to `matchLabels` in the second sample, but `matchExpressions` is not removed.

[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: placement-policy-etcdencryption
spec:
  clusterConditions:
  - status: "True"
    type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      - {key: environment, operator: In, values: ["test"]}
----

[source,yaml]
----
spec:
  clusterConditions:
  - status: "True"
    type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions: []
    matchLabel: {}
----

[#iam-controller-user-groups]
=== IAM policy controller does not consider group users
//2.3:14006

When the number of users with permissions to a given `ClusterRole` is determined, the IAM policy controller only checks for Kubernetes `User` resources and does not consider users in Kubernetes `Group` resources.

[#unable-to-log-out]
=== Unable to log out

When you use an external identity provider to log in to {product-title-short}, you might not be able to log out of {product-title-short}. This occurs when you use {product-title-short}, installed with IBM Cloud and Keycloak as the identity providers.

You must log out of the external identity provider before you attempt to log out of {product-title-short}.

[#rbac-automation-issue]
=== Administrator cluster manager unable to create automation policy
//2.3.2:15750

A user with a cluster-wide role binding to `open-cluster-management:cluster-manager-admin` is unable to create automation policies. To fix this issue, you must manually add the role to the automation policy.

Create or update a cluster role (`ClusterRole`) to add rules to the `cluster-manager-admin` role, for the `Ansible` resource. Your YAML might resemeble the following file: 

----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: add-ansible-rules
  labels:
    rbac.authorization.k8s.io/aggregate-to-ocm-cluster-manager-admin: "true"
rules:
- apiGroups: ["tower.ansible.com"]
  resources: ["ansiblejobs"]
  verbs: ["create","get", "list", "watch", "update", "delete", "deletecollection", "patch"]
----

[#gatekeeper-upgrade]
=== Gatekeeper operator installation fails
//2.3:16673

When you install the gatekeeper operator on {ocp} version 4.9, the installation fails. Before you upgrade {ocp-short} to version 4.9.0., you must upgrade the gatekeeper operator to version 0.2.0. See link:../governance/create_gatekeeper.adoc#upgrading-gatekeeper-gatekeeper-operator[Upgrading gatekeeper and the gatekeeper operator] for more information.
