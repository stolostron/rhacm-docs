[#known-issues-applications]
= Application known issues and limitations

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for application management. The following list contains known issues for this release, or known issues that continued from the previous release. 

For your {ocp} cluster, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/release_notes/ocp-4-12-release-notes#ocp-4-12-known-issues[{ocp-short} known issues]. 

For more about deprecations and removals, see xref:../release_notes/deprecate_remove.adoc#deprecations-removals[Deprecations and removals].

See the following known issues for the Application lifecycle component.

[#app-argo-ocp-clusters]
== Argo CD pull model does not work on {ocp-short} 4.17 clusters 
//2.9:ACM-14650

If you have {ocp-short} 4.17, the search `serviceaccount` is missing some permissions it needs to function as a user. Therefore, the Argo CD pull model resource sync controller fails to access the application resource list from the search API.

To work around this issue, complete the following steps: 

. Patch the search `clusterRole` by running the following command: 
+
[source,bash]
----
 oc patch clusterrole search --type='json' -p='[{"op":"add", "path":"/rules/-", "value": {"verbs":["impersonate"], "apiGroups":["authentication.k8s.io"], "resources":["userextras/authentication.kubernetes.io/credential-id", "userextras/authentication.kubernetes.io/node-name", userextras/authentication.kubernetes.io/node-uid]}}]'
----

.  Stop the `search-api` pod by running the following command:
+
[source,bash]
----
oc scale deployment search-api -n open-cluster-management --replicas=0
----

. Start the `search-api` pod by running the following command: 
+
[source,bash]
----
oc scale deployment search-api -n open-cluster-management  --replicas=1
----

[#app-addon-missing-311]
== Application Kubernetes Lease API missing for {ocp-short} 3.11 managed clusters
//2.10/2.9:ACM-10528

The application add-on component uses the _Kubernetes Lease API_ ,`leases.coordination.k8s.io`, which is missing for {ocp-short} 3.11 users. The Kubernetes Lease API was introduced in Kubernetes 1.14, but {ocp-short} 3.11 bundles Kubernetes version 1.11. 

To resolve this issue, manually apply the following Kubernetes Lease API `CustomResourceDefinition` to the {ocp-short} 3.11 managed cluster:

[source,yaml]
----
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: leases.coordination.k8s.io
spec:
  group: coordination.k8s.io
  names:
    kind: Lease
    listKind: LeaseList
    plural: leases
    singular: lease
    shortNames:
    - ls
  scope: Namespaced
  versions:
  - name: v1
    served: true    storage: true    schema:
      openAPIV3Schema:
        description: Lease defines a lease concept.
        type: object
        properties:
          apiVersion:
            type: string
          kind:
            type: string
          metadata:
            type: object
          spec:
            type: object
            properties:
              acquireTime:
                format: date-time
                type: string
              holderIdentity:
                type: string
              leaseDurationSeconds:
                format: int64
                type: integer
              leaseTransitions:
                format: int64
                type: integer
              renewTime:
                format: date-time
                type: string
            required:
            - holderIdentity
            - leaseDurationSeconds
            - renewTime
        required:
        - kind
        - metadata
        - spec
  additionalPrinterColumns:
  - JSONPath: .metadata.creationTimestamp
    name: Age
    type: date
  subresources:
    status: {}
----

*Note:* {acm-short} support of ({ocp-short} 3.11 is deprecated.

[#service-account-does-not-have-automatic-secrets]
== Service account does not have automatic secrets
//2.9:ACM-10439

When you create a service account in {ocp} 4.15 that is provisioned by some cloud providers, such as IBM VMware and Bare Metal, then the account does not automatically create a secret. Therefore, the {acm-short} `gitopsCluster` controller fails to generate the managed cluster secret for the Argo CD push model. 

This issue does not occur in {ocp} 4.15 that is provisioned by AWS. However, the issue can occur in {ocp} 4.15 provisioned by other cloud providers. This issue is delivered in {acm-short} 2.10.3 and in {acm-short} 2.9.4. 

To workaround this issue, you must manually create a secret and attach it to your service account, `open-cluster-management-agent-addon/application-manager`. To do this, complete the following steps:

. Log in to your managed cluster. 
. Create a secret by running the following secret template: 

+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: application-manager-dockercfg
  namespace: open-cluster-management-agent-addon
  annotations:
    kubernetes.io/service-account.name: application-manager
    openshift.io/token-secret.name: application-manager-dockercfg
    openshift.io/token-secret.value: application-manager-dockercfg
type: kubernetes.io/service-account-token 
----
. Retrieve the token from your created secret by running the following command:

+
[source,bash]
----
% oc get secrets -n open-cluster-management-agent-addon application-manager-dockercfg -o yaml 
data:
  token: <token1>
----
. Decode the `data.token` by running the following command:

+
[source,bash]
----
echo <token1 copied from data.token> |base64 -d 
----
. Update the token to the created secret annotation by running the following command: 

+
[source,bash]
----
% oc edit secrets -n open-cluster-management-agent-addon application-manager-dockercfg
metadata:
  annotations:
    openshift.io/token-secret.value: <paste the decoded token>
----
. Link the modified secret to your service account by running the following command: 

+
[source,bash]
----
% oc edit sa -n open-cluster-management-agent-addon application-manager
....
secrets:
- name: application-manager-dockercfg
----

To verify that you successfully created a secret and attached it to your service account, complete the following steps: 

. Go to your cluster namespace in your hub cluster. 
. Verify your cluster secret is generated there by running the following command: 

+
[source,bash]
----
% oc get secrets -n perf5 perf5-cluster-secret         
NAME                   TYPE     DATA   AGE
perf5-cluster-secret   Opaque   3      7m40s
----

[#editing-subscription-placement-rule]
== Editing subscription applications with _PlacementRule_ does not display the subscription YAML in editor
//2.9:ACM-8889

After you create a subscription application that references a `PlacementRule` resource, the subscription YAML does not display in the YAML editor in the console. Use your terminal to edit your subscription YAML file.

[#helm-chart-with-secret-dependencies-cannot-be-deployed]
== Helm Chart with secret dependencies cannot be deployed by the {acm-short} subscription 
//2.9:ACM-8727

Using Helm Chart, you can define privacy data in a Kubernetes secret and refer to this secret within the `value.yaml` file of the Helm Chart.  

The username and password are given by the referred Kubernetes secret resource `dbsecret`. For example, see the following sample `value.yaml` file: 

[source,yaml]
----
credentials:
  secretName: dbsecret
  usernameSecretKey: username
  passwordSecretKey: password
----

The Helm Chart with secret dependencies is only supported in the Helm binary CLI. It is not supported in the operator SDK Helm library. The {acm-short} subscription controller applies the operator SDK Helm library to install and upgrade the Helm Chart. Therefore, the {acm-short} subscription cannot deploy the Helm Chart with secret dependencies. 

[#creating-cluster-secrets-not-supported]
== Creating cluster secrets for Argo CD Push model is not supported
//2.9:ACM-8472

Customized cluster secrets cannot be created for the Argo CD Push model on your {ocp-short} 3.11 managed clusters. This occurs because the managed service account add-on is not supported on {ocp-short} 3.11 managed clusters.

[#argo-pull-model-topology]
== Topology does not correctly display for Argo CD pull model `ApplicationSet` application 
//2.9.0: 3910

When you use the Argo CD pull model to deploy `ApplicationSet` applications and the application resource names are customized, the resource names might appear different for each cluster. When this happens, the topology does not display your application correctly.

[#argo-pull-model-controller-local]
== Local cluster is excluded as a managed cluster for pull model
//2.8.0: 3910
//2.9:ACM-7843

The hub cluster application set deploys to target managed clusters, but the local cluster, which is a managed hub cluster, is excluded as a target managed cluster.

As a result, if the Argo CD application is propagated to the local cluster by the Argo CD pull model, the local cluster Argo CD application is not cleaned up, even though the local cluster is removed from the placement decision of the Argo CD `ApplicationSet` resource.

To work around the issue and clean up the local cluster Argo CD application, remove the `skip-reconcile` annotation from the local cluster Argo CD application. See the following annotation:

[source,yaml]
----
annotations:
    argocd.argoproj.io/skip-reconcile: "true"
----

Additionally, if you manually refresh the pull model Argo CD application in the *Applications* section of the Argo CD console, the refresh is not processed and the *REFRESH* button in the Argo CD console is disabled.

To work around the issue, remove the `refresh` annotation from the Argo CD application. See the following annotation:

[source,yaml]
----
annotations:
    argocd.argoproj.io/refresh: normal 
----

[#argo-pull-model-controller]
== Argo CD controller and the propagation controller might reconcile simultaneously
//2.8.0: 3910

Both the Argo CD controller and the propagation controller might reconcile on the same application resource and cause the duplicate instances of application deployment on the managed clusters, but from the different deployment models.

For deploying applications by using the pull model, the Argo CD controllers ignore these application resources when the Argo CD `argocd.argoproj.io/skip-reconcile` annotation is added to the template section of the `ApplicationSet`. 

The `argocd.argoproj.io/skip-reconcile` annotation is only available in the GitOps operator version 1.9.0, or later. To prevent conflicts, wait until the hub cluster and all the managed clusters are upgraded to GitOps operator version 1.9.0 before implementing the pull model. 

[#argo-pull-model-resource]
== Resource fails to deploy
//2.8.0: 3910

All the resources listed in the `MulticlusterApplicationSetReport` are actually deployed on the managed clusters. If a resource fails to deploy, the resource is not included in the resource list, but the cause is listed in the error message.

[#argo-pull-model-large]
== Resource allocation might take several minutes
//2.8.0: 3910

For large environments with over 1000 managed clusters and Argo CD application sets that are deployed to hundreds of managed clusters, Argo CD application creation on the hub cluster might take several minutes. You can set the `requeueAfterSeconds` to `zero` in the `clusterDecisionResource` generator of the application set, as it is displayed in the following example file: 

[source,yaml]
----
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: cm-allclusters-app-set
  namespace: openshift-gitops
spec:
  generators:
  - clusterDecisionResource:
      configMapRef: ocm-placement-generator
      labelSelector:
        matchLabels:
          cluster.open-cluster-management.io/placement: app-placement
      requeueAfterSeconds: 0
----

[#object-bucket-subscription-admin]
== Application ObjectBucket channel type cannot use allow and deny lists
//2.5.0: 22807

You cannot specify allow and deny lists with ObjectBucket channel type in the `subscription-admin` role. In other channel types, the allow and deny lists in the subscription indicates which Kubernetes resources can be deployed, and which Kubernetes resources should not be deployed.

[#argo-app-set-version]
=== Argo Application cannot be deployed on 3.x {ocp-short} managed clusters

Argo `ApplicationSet` from the console cannot be deployed on 3.x {ocp-short} managed clusters because the `Infrastructure.config.openshift.io` API is not available on  on 3.x.

[#changes-not-automatic]
== Changes to the multicluster_operators_subscription image do not take effect automatically
//2.5.0: 21446

The `application-manager` add-on that is running on the managed clusters is now handled by the subscription operator, when it was previously handled by the klusterlet operator. The subscription operator is not managed the `multicluster-hub`, so changes to the `multicluster_operators_subscription` image in the `multicluster-hub` image manifest ConfigMap do not take effect automatically.

If the image that is used by the subscription operator is overrided by changing the `multicluster_operators_subscription` image in the `multicluster-hub` image manifest ConfigMap, the `application-manager` add-on on the managed clusters does not use the new image until the subscription operator pod is restarted. You need to restart the pod.

[#policy-needs-subscription-admin]
== Policy resource not deployed unless by subscription administrator
//2.4.0: 17819

The `policy.open-cluster-management.io/v1` resources are no longer deployed by an application subscription by default for {acm-short} version 2.4.

A subscription administrator needs to deploy the application subscription to change this default behavior.

See link:../applications/allow_deny.adoc[Creating an allow and deny list as subscription administrator] for information. `policy.open-cluster-management.io/v1` resources that were deployed by existing application subscriptions in previous {acm-short} versions remain, but are no longer reconciled with the source repository unless the application subscriptions are deployed by a subscription administrator.

[#application-ansible-standalone]
== Application Ansible hook stand-alone mode
// 2.2:8036

Ansible hook stand-alone mode is not supported. To deploy Ansible hook on the hub cluster with a subscription, you might use the following subscription YAML:

[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: Subscription
metadata:
  name: sub-rhacm-gitops-demo
  namespace: hello-openshift
annotations:
  apps.open-cluster-management.io/github-path: myapp
  apps.open-cluster-management.io/github-branch: master
spec:
  hooksecretref:
      name: toweraccess
  channel: rhacm-gitops-demo/ch-rhacm-gitops-demo
  placement:
     local: true
----
However, this configuration might never create the Ansible instance, since the `spec.placement.local:true` has the subscription running on `standalone` mode. You need to create the subscription in hub mode. 

. Create a placement rule that deploys to `local-cluster`. See the following sample where `local-cluster: "true"` refers to your hub cluster:

+
[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata: 
  name: <towhichcluster>
  namespace: hello-openshift
spec:
  clusterSelector:
    matchLabels:
      local-cluster: "true" 
----
. Reference that placement rule in your subscription. See the following sample:

+
[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: Subscription
metadata:
  name: sub-rhacm-gitops-demo
  namespace: hello-openshift
annotations:
  apps.open-cluster-management.io/github-path: myapp
  apps.open-cluster-management.io/github-branch: master
spec:
  hooksecretref:
      name: toweraccess
  channel: rhacm-gitops-demo/ch-rhacm-gitops-demo
  placement:
     placementRef:
        name: <towhichcluster>
        kind: PlacementRule
----

After applying both, you should see the Ansible instance created in your hub cluster.

[#application-not-deployed-after-an-updated-placement-rule]
== Application not deployed after an updated placement rule
// 1.0.0:1449

If applications are not deploying after an update to a placement rule, verify that the `application-manager` pod is running.
The `application-manager` is the subscription container that needs to run on managed clusters.

You can run `oc get pods -n open-cluster-management-agent-addon |grep application-manager` to verify.

You can also search for `kind:pod cluster:yourcluster` in the console and see if the `application-manager` is running.

If you cannot verify, attempt to import the cluster again and verify again.

[#subscription-operator-does-not-create-an-scc]
== Subscription operator does not create an SCC
// 1.0.0:1764

Learn about {ocp} SCC at link:https://docs.openshift.com/container-platform/4.8/authentication/managing-security-context-constraints.html#security-context-constraints-about_configuring-internal-oauth[Managing Security Context Constraints (SCC)], which is an additional configuration required on the managed cluster.

Different deployments have different security context and different service accounts. The subscription operator cannot create an SCC CR automatically.. Administrators control permissions for pods. A Security Context Constraints (SCC) CR is required to enable appropriate permissions for the relative service accounts to create pods in the non-default namespace. To manually create an SCC CR in your namespace, complete the following steps:

. Find the service account that is defined in the deployments. For example, see the following `nginx` deployments:

+
----
nginx-ingress-52edb
nginx-ingress-52edb-backend
----

+
. Create an SCC CR in your namespace to assign the required permissions to the service account or accounts. See the following example, where `kind: SecurityContextConstraints` is added:

+
[source,yaml]
----
apiVersion: security.openshift.io/v1
 defaultAddCapabilities:
 kind: SecurityContextConstraints
 metadata:
   name: ingress-nginx
   namespace: ns-sub-1
 priority: null
 readOnlyRootFilesystem: false
 requiredDropCapabilities:
 fsGroup:
   type: RunAsAny
 runAsUser:
   type: RunAsAny
 seLinuxContext:
   type: RunAsAny
 users:
 - system:serviceaccount:my-operator:nginx-ingress-52edb
 - system:serviceaccount:my-operator:nginx-ingress-52edb-backend
----

[#application-channels-require-unique-namespaces]
== Application channels require unique namespaces
// 1.0.0:2311

Creating more than one channel in the same namespace can cause errors with the hub cluster.

For instance, namespace `charts-v1` is used by the installer as a Helm type channel, so do not create any additional channels in `charts-v1`. Ensure that you create your channel in a unique namespace. All channels need an individual namespace, except GitHub channels, which can share a namespace with another GitHub channel.

[#ansible-automation-platform-fail]
== {aap-short} job fail

Ansible jobs fail to run when you select an incompatible option. {aap-short} only works when the `-cluster-scoped` channel options are chosen. This affects all components that need to perform Ansible jobs.

[#ansible-automation-operator-access]
== {aap-short} operator access {aap-short} outside of a proxy

The {aap} operator cannot access {aap-short} outside of a proxy-enabled {ocp-short} cluster. To resolve, you can install the {aap-short} within the proxy. See install steps that are provided by {aap-short}.

[#application-name]
== Application name requirements
// 2.3:#14310

An application name cannot exceed 37 characters. The application deployment displays the following error if the characters exceed this amount.

[source,yaml]
----
status:
  phase: PropagationFailed
  reason: 'Deployable.apps.open-cluster-management.io "_long_lengthy_name_" is invalid: metadata.labels: Invalid value: "_long_lengthy_name_": must be no more than 63 characters/n'
----

[#application-tables]
== Application console table limitations
// 2.3:12410

See the following limitations to various _Application_ tables in the console:

- From the _Applications_ table on the _Overview_ page and the _Subscriptions_ table on the _Advanced configuration_ page, the _Clusters_ column displays a count of clusters where application resources are deployed. Since applications are defined by resources on the local cluster, the local cluster is included in the search results, whether actual application resources are deployed on the local cluster or not.

- From the _Advanced configuration_ table for _Subscriptions_, the _Applications_ column displays the total number of applications that use that subscription, but if the subscription deploys child applications, those are included in the search result, as well.

- From the _Advanced configuration_ table for _Channels_, the _Subscriptions_ column displays the total number of subscriptions on the local cluster that use that channel, but this does not include subscriptions that are deployed by other subscriptions, which are included in the search result.

[#app-topology]
== No Application console topology filtering

The _Console_ and _Topology_ for _Application_ changes for the {product-version}. There is no filtering capability from the console Topology page.

[#allow-deny-list-not-working-objectstorage-app]
== Allow and deny list does not work in Object storage applications
// 2.6:25445

The `allow` and `deny` list feature does not work in Object storage application subscriptions.
