[#known-issues-install]
= Installation known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for installation. The following list contains known issues for this release, or known issues that continued from the previous release. 

For your {ocp} cluster, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/release_notes/ocp-4-12-release-notes#ocp-4-12-known-issues[{ocp-short} known issues]. 

For more about deprecations and removals, see xref:../release_notes/deprecate_remove.adoc#deprecations-removals[Deprecations and removals].

[#ocm-controller-crash]
== ocm-controller crashes after {product-title-short} upgrade

After you upgrade from 2.7.x to 2.8.x and then to 2.9.0, the `ocm-controller` of the `multicluster-engine` namespace crashes. After you attempt to list and or both, `ManagedClusterSet` and `ManagedClusterSetBinding` custom resource definitions, the following error message appears:

[source,bash]
----
Error from server: request to convert CR from an invalid group/version: cluster.open-cluster-management.io/v1beta1
----

This indicates that the migration of `ManagedClusterSets` and `ManagedClusterSetBindings` custom resource definitions from `v1beta1` to `v1beta2` failed. 

To resolve this, you must initiate the API migration manually. Completet the following steps:

//I have concerns about the multiple commands. I recommend creating substeps so that the user understands the purpose for each command. 
. Revert the `cluster-manager` to a previous release. Run the following commands:

[source,bash]
----
oc annotate mce multiclusterengine pause=true
oc patch deployment cluster-manager -n multicluster-engine -p \  '{"spec":{"template":{"spec":{"containers":[{"name":"registration-operator","image":"registry.redhat.io/multicluster-engine/registration-operator-rhel8@sha256:35999c3a1022d908b6fe30aa9b85878e666392dbbd685e9f3edcb83e3336d19f"}]}}}}'
export ORIGIN_REGISTRATION_IMAGE=$(oc get clustermanager cluster-manager -o jsonpath='{.spec.registrationImagePullSpec}')
oc patch clustermanager cluster-manager --type='json' -p='[{"op": "replace", "path": "/spec/registrationImagePullSpec", "value": "registry.redhat.io/multicluster-engine/registration-rhel8@sha256:a3c22aa4326859d75986bf24322068f0aff2103cccc06e1001faaf79b9390515"}]' 
----

. Run the following commands to revert the `ManagedClusterSets` and `ManagedClusterSetBindings` custom resource definitions to a previous release:

[source,bash]
----
oc annotate crds managedclustersets.cluster.open-cluster-management.io operator.open-cluster-management.io/version-
oc annotate crds  managedclustersetbindings.cluster.open-cluster-management.io operator.open-cluster-management.io/version- 
----

. Restart the `cluster-manager` and wait for the custom resource definitions to be recreated. Run the following commands:

[source,bash]
----
oc -n multicluster-engine delete pods -l app=cluster-manager
oc wait crds managedclustersets.cluster.open-cluster-management.io --for=jsonpath="{.metadata.annotations['operator\.open-cluster-management\.io/version']}"="2.3.3" --timeout=120s
oc wait crds managedclustersetbindings.cluster.open-cluster-management.io --for=jsonpath="{.metadata.annotations['operator\.open-cluster-management\.io/version']}"="2.3.3" --timeout=120s 
----

. Start the storage version migration with the following commands:

[source,bash]
----
oc patch StorageVersionMigration managedclustersets.cluster.open-cluster-management.io --type='json' -p='[{"op":"replace", "path":"/spec/resource/version", "value":"v1beta1"}]'
oc patch StorageVersionMigration managedclustersets.cluster.open-cluster-management.io --type='json' --subresource status -p='[{"op":"remove", "path":"/status/conditions"}]'
oc patch StorageVersionMigration managedclustersetbindings.cluster.open-cluster-management.io --type='json' -p='[{"op":"replace", "path":"/spec/resource/version", "value":"v1beta1"}]'
oc patch StorageVersionMigration managedclustersetbindings.cluster.open-cluster-management.io --type='json' --subresource status -p='[{"op":"remove", "path":"/status/conditions"}]' 
----

. Run the following command to wait for the migration to complete:

[source,bash]
----
oc wait storageversionmigration managedclustersets.cluster.open-cluster-management.io --for=condition=Succeeded --timeout=120s 
oc wait storageversionmigration managedclustersetbindings.cluster.open-cluster-management.io --for=condition=Succeeded --timeout=120s
----

. Restore the `cluster-manager` back to {product-title-short} {product-version}. It might take several minutes. Run the following command:

[source,bash]
----
oc annotate mce multiclusterengine pause-
oc patch clustermanager cluster-manager --type='json' -p='[{"op": "replace", "path": "/spec/registrationImagePullSpec", "value": "'$ORIGIN_REGISTRATION_IMAGE'"}]' 
----

. Run the following commands to verify that {product-title-short} is recovered:

[source,bash]
----
oc get managedclusterset
oc get managedclustersetbinding -A 
----

//what will the user see?


[#upgrade-remaining-resource]
== Deprecated resources remain after upgrade to Errata releases
//2.6X: 26987

After you upgrade from 2.4.x to 2.5.x, and then to 2.6.x, deprecated resources in the managed cluster namespace might remain. You need to manually delete these deprecated resources if version 2.6.x was upgraded from 2.4.x:

*Note:* You need to wait 30 minutes or more before you upgrade from version 2.5.x to version 2.6.x.

You can delete from the console, or you can run a command similar to the following example for the resources you want to delete:

----
oc delete -n <managed cluster namespace> managedclusteraddons.addon.open-cluster-management.io <resource-name> 
----
 
See the list of deprecated resources that might remain:

----
managedclusteraddons.addon.open-cluster-management.io:
policy-controller
manifestworks.work.open-cluster-management.io:
-klusterlet-addon-appmgr
-klusterlet-addon-certpolicyctrl
-klusterlet-addon-crds
-klusterlet-addon-iampolicyctrl
-klusterlet-addon-operator
-klusterlet-addon-policyctrl
-klusterlet-addon-workmgr
----

[#upgrade-pod-not-up]
== Pods might not come back up after upgrading {product-title-short}
// 2.5, 2.4: 23730

After upgrading {product-title-short} to a new version, a few pods that belong to a `StatefulSet` might remain in a `failed` state. This infrequent event is caused by a known link:https://github.com/kubernetes/kubernetes/issues/60164[Kubernetes issue].

As a workaround for this problem, delete the failed pod. Kubernetes automatically relaunches it with the correct settings.

[#openshift-container-platform-cluster-upgrade-failed-status]
== OpenShift Container Platform cluster upgrade failed status
// 2.0.0:3442

When an {ocp-short} cluster is in the upgrade stage, the cluster pods are restarted and the cluster might remain in `upgrade failed` status for a variation of 1-5 minutes. This behavior is expected and resolves after a few minutes.

[#create-multiclusterengine-button-not-working]
== Create MultiClusterEngine button not working
//2.6:25641

After installing {product-title} in the {ocp} console, a pop-up window with the following message appears:

`MultiClusterEngine required`

`Create a MultiClusterEngine instance to use this Operator.`

The *Create MultiClusterEngine* button in the pop-up window message might not work. To work around the issue, select *Create instance* in the MultiClusterEngine tile in the Provided APIs section.
