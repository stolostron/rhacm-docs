[#backup-intro]
= Backup and restore
//Concept; As i edit, comment, and make suggestions, I think the procedure topic (backup_restore_enable.adoc) is becoming long and complicated. Are there ways that we can separate backup informaion and restore information (Backup validation and Restore?)  MJ, 10/11/22
The cluster backup and restore operator provides disaster recovery solutions for {product-title} hub cluster failure. The cluster backup and restore operato runs on the hub cluster and depends on the link:https://github.com/openshift/oadp-operator[OADP Operator] to install Velero, and to create a connection from the hub cluster to the backup storage location where the data is stored. 

Velero is the component that runs the backup and restore operations. The cluster backup and restore operator solution provides backup and restore support for all {product-title-short} hub cluster resources, including managed clusters, applications, policies, and bare metal assets.

The cluster backup and restore operator supports backups of any third-party resources that extend the hub cluster installation. With this backup solution, you can define cron-based backup schedules which run at specified time intervals. When the hub cluster goes down, a new hub cluster can be deployed and the backed up data is moved to the new hub cluster.

//Continue reading the following topics to learn more about the backup and restore operator:

//* <<backup-restore-architecture,Backup and restore operator architecture>>
//** <<resources-that-are-backed-up,Resources that are backed up>>
//** <<resources-restored-managed-cluster-activation,Resources restored at managed clusters activation time>>
//** <<resource-requests-and-limits,Resource requests and limits customization>>
//* <<active-passive-config,Active passive configuration>>
//* <<managed-cluster-activation-data,Managed cluster activation data>>
//* <<disaster-recovery,Disaster recovery>>
//* <<backup-validation-using-a-policy,Backup validation using a policy>>





// [#backup-restore-architecture]
// == Backup and restore operator architecture
// The operator defines the `backupSchedule.cluster.open-cluster-management.io` resource, which is used to set up {product-title-short} backup schedules, and `restore.cluster.open-cluster-management.io` resource, which is used to process and restore these backups. The operator creates corresponding Velero resources, and defines the options needed to backup remote clusters and any other hub cluster resources that need to be restored. View the following diagram:

// image:../images/cluster_backup_controller_dataflow25.png[Backup and restore architecture diagram] 

// [#resources-that-are-backed-up]
// === Resources that are backed up
// The cluster backup and restore operator solution provides backup and restore support for all hub cluster resources like managed clusters, applications, policies, and bare metal assets. You can use the solution to back up any third-party resources extending the basic hub cluster installation. With this backup solution, you can define a cron-based backup schedule, which runs at specified time intervals and continuously backs up the latest version of the hub cluster content. 

// When the hub cluster needs to be replaced or is in a disaster scenario when the hub cluster goes down, a new hub cluster can be deployed and backed up data is moved to the new hub cluster.

// View the following ordered list of the cluster backup and restore process for identifying backup data:

// * Exclude all resources in the `MultiClusterHub` namespace. This is to avoid backing up installation resources that are linked to the current hub cluster identity and should not be backed up.

// * Backup all CRDs with an API version suffixed by `.open-cluster-management.io`. This suffix indicates that all {product-title-short} resources are backed up.

// * Backup all CRDs from the following API groups: `argoproj.io`, `app.k8s.io`, `core.observatorium.io`, `hive.openshift.io`.

// * Exclude all CRDs from the following API groups: `admission.cluster.open-cluster-management.io`, `admission.work.open-cluster-management.io`,  `internal.open-cluster-management.io`, `operator.open-cluster-management.io`, `work.open-cluster-management.io`, `search.open-cluster-management.io`, `admission.hive.openshift.io`, `velero.io`.

// * Exclude the following CRDs that are a part of the included API groups, but are either not needed or are being recreated by owner-resources, which are also backed up: `clustermanagementaddon`, `observabilityaddon`, `applicationmanager`, `certpolicycontroller`, `iampolicycontroller`, `policycontroller`, `searchcollector`, `workmanager`, `backupschedule`, `restore`, `clusterclaim.cluster.open-cluster-management.io`.

// * Backup secrets and ConfigMaps with one of the following labels: `cluster.open-cluster-management.io/type`, `hive.openshift.io/secret-type`, `cluster.open-cluster-management.io/backup`. 

// * Use the following label for any other resources that you want to be backed up and are not included in the previously mentioned criteria, `cluster.open-cluster-management.io/backup`. See the following example:
// +
// [source,yaml]
// ----
// apiVersion: my.group/v1alpha1
// kind: MyResource
// metadata:
//   labels:
//    cluster.open-cluster-management.io/backup: ""
//----
//+
// *Note:* Secrets used by the `hive.openshift.io.ClusterDeployment` resource need to be backed up, and are automatically annotated with the `cluster.open-cluster-management.io/backup` label only when the cluster is created using the console. If the Hive cluster is deployed using GitOps instead, the `cluster.open-cluster-management.io/backup` label must be manually added to the secrets used by the `ClusterDeployment`.

// * Exclude specific resources that you do not want backed up. For example, see the following example to exclude Velero resources from the backup process:
// +
// [source,yaml]
// ----
// apiVersion: my.group/v1alpha1
// kind: MyResource
// metadata:
//  labels:
//    velero.io/exclude-from-backup: "true"
//----

// [#resources-restored-managed-cluster-activation]
// ==== Resources restored at managed clusters activation time

// When you add the `cluster.open-cluster-management.io/backup` label to a resource, the resource is automatically backed up in the `acm-resources-generic-schedule` backup. You must set the label value to `cluster-activation` if any of the resources need to be restored, only after the managed clusters are moved to the new hub cluster and when the `veleroManagedClustersBackupName:latest` is used on the restored resource. This ensures the resource is not restored unless the managed cluster activation is called. View the following example:

// [source,yaml]
// ----
// apiVersion: my.group/v1alpha1
// kind: MyResource
// metadata:
//  labels:
//    cluster.open-cluster-management.io/backup: cluster-activation
//----

//Aside from the activation data resources that are identified by using the `cluster.open-cluster-management.io/backup: cluster-activation` label and stored by the `acm-resources-generic-schedule` backup, the cluster backup and restore operator includes a few resources in the activation set, by default. The following resources are backed up by the `acm-managed-clusters-schedule` backup:

//* `managedcluster.cluster.open-cluster-management.io`
//* `managedcluster.clusterview.open-cluster-management.io`
//* `klusterletaddonconfig.agent.open-cluster-management.io`
//* `managedclusteraddon.addon.open-cluster-management.io`
//* `managedclusterset.cluster.open-cluster-management.io`
//* `managedclusterset.clusterview.open-cluster-management.io`
//* `managedclustersetbinding.cluster.open-cluster-management.io`
//* `clusterpool.hive.openshift.io`
//* `clusterclaim.hive.openshift.io`
//* `clustercurator.cluster.open-cluster-management.io`

//[#resource-requests-and-limits]
//=== Resource requests and limits customization

//When Velero is initially installed, Velero pod is set to the default CPU and memory limits as defined in the following sample:

//[source,yaml]
//----
//resources:
// limits:
//   cpu: "1"
//   memory: 256Mi
// requests:
//   cpu: 500m
//   memory: 128Mi
//----

//The limits from the previous sample work well with some scenarios, but might need to be updated when your cluster backs up a large number of resources. For instance, when back up is run on a hub cluster that manages 2000 clusters, then the Velero pod crashes due to the out-of-memory error (OOM). The following configuration allows for the backup to complete for this scenario:

//[source,yaml]
//----
//  limits:
//    cpu: "2"
//    memory: 1Gi
//  requests:
//    cpu: 500m
//    memory: 256Mi
//----

//To update the limits and requests for the Velero pod resource, you need to update the `DataProtectionApplication` resource and insert the `resourceAllocation` template for the Velero pod. View the following sample:

//[source,yaml]
//----
//apiVersion: oadp.openshift.io/v1alpha1
//kind: DataProtectionApplication
//metadata:
//  name: velero
//  namespace: open-cluster-management-backup
//spec:
//...
//  configuration:
//...
//    velero:
//      podConfig:
//        resourceAllocations:
//          limits:
//            cpu: "2"
//            memory: 1Gi
//          requests:
//            cpu: 500m
//            memory: 256Mi
//----

// Refer to the link:https://github.com/openshift/oadp-operator/blob/master/docs/config/resource_req_limits.md[Velero resource requests and limits customization] to find out more about the `DataProtectionApplication` parameters.

// [#managed-cluster-activation-data]
// === Managed cluster activation data

// Managed cluster activation data or other activation data, is a backup resource. When the activation data is restored on a new hub cluster, managed clusters are then being actively managed by the hub cluster where the restore is run. Activation data resources are stored by the managed clusters backup and by the resource-generic backup, when you use the `cluster.open-cluster-management.io/backup: cluster-activation` label. 

// [#resources-restored-managed-cluster]
// === Resources restored at managed activation time

// When you add the `cluster.open-cluster-management.io/backup: cluster-activation` label to a resource, the resource is automatically backed up in the `acm-resources-generic-schedule` backup resource. Resources usually need to be restored when you set the `veleroManagedClustersBackupName:latest` label value in the restore resource. If any of these resources need to be restored when the managed clusters are moved to the new hub cluster, set the `veleroManagedClustersBackupName:latest` label value to `cluster-activation`. This ensures that the resource is not restored unless the managed cluster activation starts.

// Your resource might resemble the following example:

//[source,yaml]
//----
//apiVersion: my.group/v1alpha1
//kind: MyResource
//metadata:
//  labels:
//    cluster.open-cluster-management.io/backup: cluster-activation
//----

//There are also default resources in the activation set that are backed up by the `acm-managed-clusters-schedule` resource. View the following default resources that are restored by the `acm-managed-clusters-schedule` resource:

//* `managedcluster.cluster.open-cluster-management.io`
//* `managedcluster.clusterview.open-cluster-management.io`
//* `klusterletaddonconfig.agent.open-cluster-management.io`
//* `managedclusteraddon.addon.open-cluster-management.io`
//* `clusterpool.hive.openshift.io`
//* `clusterclaim.hive.openshift.io`
//* `clustercurator.cluster.open-cluster-management.io`
//* `clustersync.hiveinternal.openshift.io`
//* `baremetalhost.metal3.io`
//* `bmceventsubscription.metal3.io`
//* `hostfirmwaresettings.metal3.io`

//[#disaster-recovery]
//== Disaster recovery

//When the primary hub cluster goes down, one of the passive hub clusters is chosen by the administrator to take over the managed clusters. In the following image, the administrator decides to use _Hub cluster N_ as the new primary hub cluster:

//image:../images/disaster_recovery.png[Disaster recovery diagram] 

//_Hub cluster N_ restores the managed cluster activation data. At this point, the managed clusters connect with _Hub cluster N_. The administrator activates a backup on the new primary hub cluster, _Hub cluster N_, by creating a `BackupSchedule.cluster.open-cluster-management.io` resource, and storing the backups at the same storage location as the initial primary hub cluster.

//All other passive hub clusters now restore passive data using the backup data created by the new primary hub cluster. _Hub N_ is now the primary hub cluster, managing clusters and backing up data.

//[#backup-validation-using-a-policy]
//== Backup validation using a policy

//The cluster backup and restore operator Helm chart (`cluster-backup-chart`) installs the `backup-restore-enabled` policy on your hub cluster, which is used to inform you about issues with the backup and restore component. The `backup-restore-enabled` policy includes a set of templates that check for the following constraints:

//- *Pod validation*
//+
//The following templates check the pod status for the backup component and dependencies:
//+
//** `acm-backup-pod-running` template checks if the backup and restore operator pod is running.
//** `oadp-pod-running` template checks if the OADP operator pod is running. 
//** `velero-pod-running` template checks if the Velero pod is running.

//- *Data Protection Application validation*
//+
//* `data-protection-application-available` template checks if a `DataProtectioApplicatio.oadp.openshift.io` resource is created. This OADP resource sets up Velero configurations.

//- *Backup storage validation*
//+
//* `backup-storage-location-available` template checks if a `BackupStorageLocation.velero.io` resource is created and if the status value is `Available`. This implies that the connection to the backup storage is valid. 

//- *BackupSchedule collision validation*
//+
//* `acm-backup-clusters-collision-report` template verifies that the status is not `BackupCollision`, if a `BackupSchedule.cluster.open-cluster-management.io` exists on the current hub cluster. This verifies that the current hub cluster is not in collision with any other hub cluster when you write backup data to the storage location.
//+
//For a definition of the `BackupCollision` state read the https://github.com/stolostron/cluster-backup-operator#backup-collisions[Backup Collisions section].

//- *BackupSchedule and restore status validation*
//+
//* `acm-backup-phase-validation` template checks that the status is not in `Failed`, or `Empty` state, if a `BackupSchedule.cluster.open-cluster-management.io` exists on the current cluster. This ensures that if this cluster is the primary hub cluster and is generating backups, the `BackupSchedule.cluster.open-cluster-management.io` status is healthy.
//* The same template checks that the status is not in a `Failed`, or `Empty` state, if a `Restore.cluster.open-cluster-management.io` exists on the current cluster. This ensures that if this cluster is the secondary hub cluster and is restoring backups, the `Restore.cluster.open-cluster-management.io` status is healthy.

//- *Backups exist validation*
//+
//* `acm-managed-clusters-schedule-backups-available` template checks if `Backup.velero.io` resources are available at the location specified by the `BackupStorageLocation.velero.io`, and if the backups are created by a `BackupSchedule.cluster.open-cluster-management.io` resource. This validates that the backups have been run at least once, using the backup and restore operator.

//- *Backups for completion*
//+
//* An `acm-backup-in-progress-report` template checks if `Backup.velero.io` resources are stuck in the `InProgress` state. This validation is added because with a large number of resources, the velero pod restarts as the backup runs, and the backup stays in progress without proceeding to completion. During a normal backup, the backup resources are in progress at some point when it is run, but are not stuck and run to completion. It is normal to see the `acm-backup-in-progress-report` template report a warning during the time the schedule is running and backups are in progress.

//- *Backups that actively run as a cron job*
//+
//* A `BackupSchedule.cluster.open-cluster-management.io` actively runs and saves new backups at the storage location. This validation is done by the `backup-schedule-cron-enabled` policy template. The template checks that there is a `Backup.velero.io` with `velero.io/schedule-name: acm-validation-policy-schedule` label at the storage location.
//+
//The `acm-validation-policy-schedule` backups are set to expire after the time is set for the backups cron schedule. If no cron job is running to create backups, the old `acm-validation-policy-schedule` backup is deleted because it expired and a new one is not created. As a result, if no `acm-validation-policy-schedule backups` exist at any moment, it means that there are no active cron jobs generating backups.
//+
//This policy is intended to help notify the hub cluster administrator of any backup issues when the hub cluster is active and produces or restore backups.

//* xref:../backup_restore/backup_restore_enable.adoc#enable-backup-and-restore[Enabling the backup and restore operator]
//* xref:../backup_restore/backup_restore_manage.adoc#manage-backup-restore[Managing backup and restore operator]
