[#creating-a-cluster-on-amazon-web-services-private]
= Creating a cluster on Amazon Web Services Private

You can use the {mce} console to create a {ocp} cluster on Amazon Web Services (AWS) private. 

When you create a cluster, the creation process uses the {ocp-short} installer with the Hive resource. If you have questions about cluster creation after completing this procedure, see https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html/installing/installing-on-aws[Installing on AWS] in the {ocp-short} documentation for more information about the process.  

* <<aws_prerequisites,Prerequisites>>
* <<aws-creating-your-cluster-with-the-console,Creating your cluster with the console>>
* <<aws_adding-your-cluster-to-existing-cluster-set,Adding your cluster to an existing cluster set>>

[#aws_prerequisites]
== Prerequisites

See the following prerequisites before creating a cluster on AWS:

* You must have a deployed {mce} hub cluster.
* You need Internet access for your {mce} hub cluster so it can create the Kubernetes cluster on Amazon Web Services.
* You need an AWS credential. See xref:../credentials/credential_aws.adoc#creating-a-credential-for-amazon-web-services[Creating a credential for Amazon Web Services] for more information.
* You need a configured domain in AWS. See https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html/installing/installing-on-aws#installing-aws-account[Configuring an AWS account] for instructions on how to configure a domain.
* You must have Amazon Web Services (AWS) login credentials, which include user name, password, access key ID, and secret access key. See https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html[Understanding and Getting Your Security Credentials].
* You must <<configuring_hive_to_enable_AWS_Private_Link,configuring hive to enable AWS Private Link>>
* You must <<set_permissions_for_AWS_private_link, set permissions for AWS private link>>
* You must have an {ocp-short} image pull secret. See https://access.redhat.com/documentation/en-us/openshift_container_platform/4.11/html/images/managing-images#using-image-pull-secrets[Using image pull secrets].

*Note:* If you change your cloud provider access key on the cloud provider, you also need to manually update the corresponding credential for the cloud provider on the console of {mce}. This is required when your credentials expire on the cloud provider where the managed cluster is hosted and you try to delete the managed cluster.

[#configuring_hive_to_enable_AWS_Private_Link]
== Configuring hive to enable AWS Private Link

To configure Hive to support Private Link in a specific region,

1. Create VPCs in that region that can be used to create VPC Endpoints.

NOTE: There is a hard limit of 255 VPC Endpoints in a region, therefore you will need multiple VPCs to support more cluster in that region.

2. For each VPC, create subnets in all the supported availability zones of the region.

NOTE: each subnet must have at least 255 usuable IPs because the controller.

For example let's create VPCs in us-east-1 region, that has 6 AZs.

    ```txt
    vpc-1 (us-east-1) : 10.0.0.0/20
      subnet-11 (us-east-1a): 10.0.0.0/23
      subnet-12 (us-east-1b): 10.0.2.0/23
      subnet-13 (us-east-1c): 10.0.4.0/23
      subnet-12 (us-east-1d): 10.0.8.0/23
      subnet-12 (us-east-1e): 10.0.10.0/23
      subnet-12 (us-east-1f): 10.0.12.0/23
    ```

    ```txt
    vpc-2 (us-east-1) : 10.0.16.0/20
      subnet-21 (us-east-1a): 10.0.16.0/23
      subnet-22 (us-east-1b): 10.0.18.0/23
      subnet-23 (us-east-1c): 10.0.20.0/23
      subnet-24 (us-east-1d): 10.0.22.0/23
      subnet-25 (us-east-1e): 10.0.24.0/23
      subnet-26 (us-east-1f): 10.0.28.0/23
    ```

3. Make sure all the MCE hub environments (MCE hub cluster VPCs) have network reachability to these VPCs created above for VPC Endpoints using peering, transit gateways, etc.

4. Gather a list of VPCs that will need to resolve the DNS setup for Private Link. This should at least include the VPC of the MCE being configured, and can include list of all VPCs where various Hive controllers exists.

5. Update the HiveConfig to enable Private Link for clusters in that region.

    ```yaml
    ## hiveconfig
    spec:
      awsPrivateLink:
        ## this this is list if inventory of VPCs that can be used to create VPC
        ## endpoints by the controller
        endpointVPCInventory:
        - region: us-east-1
          vpcID: vpc-1
          subnets:
          - availabilityZone: us-east-1a
            subnetID: subnet-11
          - availabilityZone: us-east-1b
            subnetID: subnet-12
          - availabilityZone: us-east-1c
            subnetID: subnet-13
          - availabilityZone: us-east-1d
            subnetID: subnet-14
          - availabilityZone: us-east-1e
            subnetID: subnet-15
          - availabilityZone: us-east-1f
            subnetID: subnet-16
        - region: us-east-1
          vpcID: vpc-2
          subnets:
          - availabilityZone: us-east-1a
            subnetID: subnet-21
          - availabilityZone: us-east-1b
            subnetID: subnet-22
          - availabilityZone: us-east-1c
            subnetID: subnet-23
          - availabilityZone: us-east-1d
            subnetID: subnet-24
          - availabilityZone: us-east-1e
            subnetID: subnet-25
          - availabilityZone: us-east-1f
            subnetID: subnet-26

        ## credentialsSecretRef points to a secret with permissions to create
        ## resources in account where the inventory of VPCs exist.
        credentialsSecretRef:
          name: < hub-account-credentials-secret-name >

        ## this is a list of VPC where various MCE clusters exists.
        associatedVPCs:
        - region: region-mce1
          vpcID: vpc-mce1
          credentialsSecretRef:
            name: < credentials that have access to account where MCE1 VPC exists >
        - region: region-mce2
          vpcID: vpc-mce2
          credentialsSecretRef:
            name: < credentials that have access to account where MCE2 VPC exists>
    ```

You can include VPC from all the regions where private link is supported in the endpointVPCInventory list. The controller will pick a VPC appropriate for the ClusterDeployment.

### Security Groups for VPC Endpoints

Each VPC Endpoint in AWS has a Security Group attached to control access to the endpoint.
See the [docs][control-access-vpc-endpoint] for details.

When Hive creates VPC Endpoint, it does not specify any Security Group and therefore the
default Security Group of the VPC is attached to the VPC Endpoint. Therefore, the default
security group of the VPC where VPC Endpoints are created must have rules to allow traffic
from the Hive installer pods.

For example, if Hive is running in hive-vpc(10.1.0.0/16), there must be a rule in default
Security Group of VPC where VPC Endpoint is created that allows ingess from 10.1.0.0/16.

[#set_permissions_for_AWS_private_link]
== Set Permissions for AWS Private Link

There multiple credentials involved in the configuring AWS Private Link and there are different
expectations of required permissions for these credentials.

1. The credentials on ClusterDeployment

The following permissions are required:

    ```txt
    ec2:CreateVpcEndpointServiceConfiguration
    ec2:DescribeVpcEndpointServiceConfigurations
    ec2:ModifyVpcEndpointServiceConfiguration
    ec2:DescribeVpcEndpointServicePermissions
    ec2:ModifyVpcEndpointServicePermissions
    ec2:DeleteVpcEndpointServiceConfigurations
    ```

2. The credentials specified in HiveConfig for endpoint VPCs account `.spec.awsPrivateLink.credentialsSecretRef`

The following permissions are required:

    ```txt
    ec2:DescribeVpcEndpointServices
    ec2:DescribeVpcEndpoints
    ec2:CreateVpcEndpoint
    ec2:CreateTags
    ec2:DescribeNetworkInterfaces
    ec2:DescribeVPCs

    ec2:DeleteVpcEndpoints

    route53:CreateHostedZone
    route53:GetHostedZone
    route53:ListHostedZonesByVPC
    route53:AssociateVPCWithHostedZone
    route53:DisassociateVPCFromHostedZone
    route53:CreateVPCAssociationAuthorization
    route53:DeleteVPCAssociationAuthorization
    route53:ListResourceRecordSets
    route53:ChangeResourceRecordSets

    route53:DeleteHostedZone
    ```

3. The credentials specified in HiveConfig for associating VPCs to the Private Hosted Zone.
  `.spec.awsPrivateLink.associatedVPCs[$idx].credentialsSecretRef`

The following permissions are required in the account where the VPC exists:

    ```txt
    route53:AssociateVPCWithHostedZone
    route53:DisassociateVPCFromHostedZone
    ec2:DescribeVPCs
    ```

[#aws-creating-your-cluster-with-the-console]
== Creating your cluster with the console

To create a cluster from the console, navigate to *Infrastructure* > *Clusters*. On the _Clusters_ page, click *Create cluster* and complete the steps in the console. 

The name of the cluster is used in the hostname of the cluster.

*Important:* When you create a cluster, the controller creates a namespace for the cluster and its resources. Ensure that you include only resources for that cluster instance in that namespace. Destroying the cluster deletes the namespace and all of the resources in it.

*Tip:* Select *YAML: On* to view content updates as you enter the information in the console.

The node pools include the control plane pool and the worker pools. The control plane nodes share the management of the cluster activity. The information includes the following fields:

* Region: Select the _AWS-Gov_ Region
* Architecture: If the architecture type of the managed cluster is not the same as the architecture of your hub cluster, enter a value for the instruction set architecture of the machines in the pool. Valid values are _amd64_, _ppc64le_, _s390x_, and _arm64_.

* Zones: Specify where you want to run your control plane pools. You can select multiple zones within the region for a more distributed group of control plane nodes. A closer zone might provide faster performance, but a more distant zone might be more distributed.

* Instance type: Specify the instance type for your control plane node. You can change the type and size of your instance after it is created. 

* Root storage: Specify the amount of root storage to allocate for the cluster. 

You can create zero or more worker nodes in a worker pool to run the container workloads for the cluster. They can be in a single worker pool, or distributed across multiple worker pools. If zero worker nodes are specified, the control plane nodes also function as worker nodes. The optional information includes the following fields:

* Zones: Specify where you want to run your worker pools. You can select multiple zones within the region for a more distributed group of nodes. A closer zone might provide faster performance, but a more distant zone might be more distributed.

* Instance type: Specify the instance type of your worker pools. You can change the type and size of your instance after it is created.

* Node count: Specify the node count of your worker pool. This setting is required when you define a worker pool.

* Root storage: Specify the amount of root storage allocated for your worker pool. This setting is required when you define a worker pool.

Networking details are required for your cluster, and multiple networks are required for using IPv6. You can add an additional network by clicking *Add network*. 

Proxy information that is provided in the credential is automatically added to the proxy fields. You can use the information as it is, overwrite it, or add the information if you want to enable a proxy. The following list contains the required information for creating a proxy:  

* HTTP proxy URL: Specify the URL that should be used as a proxy for `HTTP` traffic. 

* HTTPS proxy URL: Specify the secure proxy URL that should be used for `HTTPS` traffic. If no value is provided, the same value as the `HTTP Proxy URL` is used for both `HTTP` and `HTTPS`.

* No proxy domains: A comma-separated list of domains that should bypass the proxy. Begin a domain name with a period `.` to include all of the subdomains that are in that domain. Add an asterisk `*` to bypass the proxy for all destinations. 

* Additional trust bundle: One or more additional CA certificates that are required for proxying HTTPS connections.

Aws private configuration

* AMI ID: The ID of the AMI used to boot machines for the cluster. If set, the AMI must belong to the same region as the cluster.

* Hosted zone: The ID of your existing Route 53 private hosted zone.

* Subnets: Specify subnets for each availability zone that your cluster uses.

* Service Endpoints: The AWS service endpoint name. Custom API endpoints can be specified for EC2, S3, IAM, Elastic Load Balancing, Tagging, Route 53, and STS AWS services.

When you review your information and optionally customize it before creating the cluster, you can select *YAML: On* to view the `install-config.yaml` file content in the panel. You can edit the YAML file with your custom settings, if you have any updates.

*Note:* You do not have to run the `kubectl` command that is provided with the cluster details to import the cluster. When you create the cluster, it is automatically configured under the management of {mce}. 

Continue with xref:../cluster_lifecycle/access_cluster.adoc#accessing-your-cluster[Accessing your cluster] for instructions for accessing your cluster. 
