[#placement-managed]
= Using ManagedClusterSets with Placement

A `Placement` resource is a namespace-scoped resource that defines a rule to select a set of `ManagedClusters` from the `ManagedClusterSets`, which are bound to the placement namespace.

**Required access**: Cluster administrator, Cluster set administrator

[#placement-overview]
== Placement overview

See the following information about how placement with managed clusters works:

* Kubernetes clusters are registered with the hub cluster as cluster-scoped `ManagedClusters`.

* The `ManagedClusters` are organized into cluster-scoped `ManagedClusterSets`.

* The `ManagedClusterSets` are bound to workload namespaces.

* The namespace-scoped `Placements` specify a portion of `ManagedClusterSets` that select a working set of the potential `ManagedClusters`.

* `Placements` select from that working set by using label and claim selectors.
+
*Important:* `Placement` does not select a `ManagedCluster` if there is no `ManagedClusterSet` bound to the placement namespace.

* The placement of `ManagedClusters` can be controlled by using taints and tolerations. See xref:../cluster_lifecycle/taints_tolerations.adoc#taints-tolerations-managed[Using taints and tolerations to place managed clusters] for more information.

The `Placement` specification includes the following fields:

* `ClusterSets` represents the `ManagedClusterSets` from which the `ManagedClusters` are selected. 

  ** If not specified, `ManagedClusters` is selected from the `ManagedClusterSets` that are bound to the placement namespace. 

  ** If specified, `ManagedClusters` is selected from the intersection of this set and the `ManagedClusterSets` that are bound to the placement namespace.

* `NumberOfClusters` represents the desired number of `ManagedClusters` to be selected, which meet the placement requirements. 
+
If not specified, all `ManagedClusters` that meet the placement requirements are selected.

* `Predicates` represents a slice of predicates to select `ManagedClusters` with label and claim selector. The predicates are ORed.

* `prioritizerPolicy` represents the policy of prioritizers. 
  
  ** `mode` is either `Exact`, `Additive`, or `""` where `""` is `Additive` by default.
    
    *** In `Additive` mode, any prioritizer that is not specifically provided with configuration values is enabled with its default configurations. In the current default configuration, the _Steady_ and _Balance_ prioritizers have the weight of 1, while the other prioritizers have the weight of 0. The default configurations might change in the future, which might change the prioritization. `Additive` mode does not require that you configure all of the prioritizers. 

    *** In `Exact` mode, any prioritizer that is not specifically provided with configuration values has the weight of zero. `Exact` mode requires that you input the full set of prioritizers that you want, but avoids behavior changes between releases.

  ** `configurations` represents the configuration of prioritizers.

    *** scoreCoordinate represents the configuration of the prioritizer and score source.

      **** `type` defines the type of the prioritizer score. Type is either `BuiltIn`, `AddOn`, or `" "`, where `" "` is `BuiltIn` by default. When the type is `BuiltIn`, the built in prioritizer name must be specified. When the type is `AddOn`, you need to configure the score source in `AddOn`.
      
      **** `builtIn` defines the name of a BuiltIn prioritizer. The following list includes the valid `BuiltIn` prioritizer names:
         
        ***** Balance: Balance the decisions among the clusters.

        ***** Steady: Ensure the existing decision is stabilized.

        ***** ResourceAllocatableCPU & ResourceAllocatableMemory: Sort clusters based on the allocatable resources.

      **** `addOn` defines the resource name and score name. `AddOnPlacementScore` is introduced to describe addon scores. See <<extensible-scheduling,Extensible scheduling>> to learn more about it.

      ***** `resourceName` defines the resource name of the `AddOnPlacementScore`. The placement prioritizer selects the `AddOnPlacementScore` custom resource by this name.

      ***** `scoreName` defines the score name inside of the `AddOnPlacementScore`. `AddOnPlacementScore` contains a list of the score name and the score value. The `scoreName`, specifies the score to be used by the prioritizer.

    *** `weight` defines the weight of prioritizer. The value must be in the range of [-10,10].
      Each prioritizer calculates an integer score of a cluster in the range of [-100, 100]. The final score of a cluster is determined by the following formula `sum(weight * prioritizer_score)`.
      A higher weight indicates that the prioritizer receives a higher weight in the cluster selection, while 0 weight indicates that the prioritizer is disabled. A negative weight indicates that it is one of the last ones selected.

**Note:** The `configurations.name` file will be removed in v1beta1 and replaced by the `scoreCoordinate.builtIn` file. If both `name` and `scoreCoordinate.builtIn` are defined, the value in `scoreCoordinate.builtIn` is used to determine the selection.

[#placement-binding]
== Placement examples

You need to bind at least one `ManagedClusterSet` to a namespace by creating a `ManagedClusterSetBinding` in that namespace. *Note:* You need role-based access to `CREATE` on the virtual sub-resource of `managedclustersets/bind`. See the following examples:

- You can select `ManagedClusters` with the `labelSelector`. See the following sample where the `labelSelector` only matches clusters with label `vendor: OpenShift`:
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement1
  namespace: ns1
spec:
  predicates:
    - requiredClusterSelector:
        labelSelector:
          matchLabels:
            vendor: OpenShift

----

- You can select `ManagedClusters` with `claimSelector`. See the following sample where `claimSelector` only matches clusters with `region.open-cluster-management.io` with `us-west-1`:
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement2
  namespace: ns1
spec:
  predicates:
    - requiredClusterSelector:
        claimSelector:
          matchExpressions:
            - key: region.open-cluster-management.io
              operator: In
              values:
                - us-west-1
----

- You can select `ManagedClusters` from particular `clusterSets`. See the following sample where `claimSelector` only matches `clusterSets:` `clusterset1` `clusterset2`:
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement3
  namespace: ns1
spec:
  clusterSets:
    - clusterset1
    - clusterset2
  predicates:
    - requiredClusterSelector:
        claimSelector:
          matchExpressions:
            - key: region.open-cluster-management.io
              operator: In
              values:
                - us-west-1
----

- Select desired number of `ManagedClusters`. See the following sample where `numberOfClusters` is `3`:
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement4
  namespace: ns1
spec:
  numberOfClusters: 3
  predicates:
    - requiredClusterSelector:
        labelSelector:
          matchLabels:
            vendor: OpenShift
        claimSelector:
          matchExpressions:
            - key: region.open-cluster-management.io
              operator: In
              values:
                - us-west-1
----

- Select a cluster with the largest allocatable memory.
+
*Note:* Similar to Kubernetes https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable[Node Allocatable], 'allocatable' is defined as the amount of compute resources that are available for pods on each cluster.
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement6
  namespace: ns1
spec:
  numberOfClusters: 1
  prioritizerPolicy:
    configurations:
      - scoreCoordinate:
          builtIn: ResourceAllocatableMemory
----

- Select a cluster with the largest allocatable CPU and memory, and make placement sensitive to resource changes. 
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement7
  namespace: ns1
spec:
  numberOfClusters: 1
  prioritizerPolicy:
    configurations:
      - scoreCoordinate:
          builtIn: ResourceAllocatableCPU
        weight: 2
      - scoreCoordinate:
          builtIn: ResourceAllocatableMemory
        weight: 2
----

- Select two clusters with the largest allocatable memory and the largest add-on score cpu ratio, and pin the placement decisions. 
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement8
  namespace: ns1
spec:
  numberOfClusters: 2
  prioritizerPolicy:
    mode: Exact
    configurations:
      - scoreCoordinate:
          builtIn: ResourceAllocatableMemory
      - scoreCoordinate:
          builtIn: Steady
        weight: 3
      - scoreCoordinate:
          type: AddOn
          addOn:
            resourceName: default
            scoreName: cpuratio
----

[#placement-decision]
== Placement decision

One or multiple `PlacementDecisions` with label `cluster.open-cluster-management.io/placement={placement name}` are created to represent the `ManagedClusters` selected by a `Placement`.

If a `ManagedCluster` is selected and added to a `PlacementDecision`, components that consume this `Placement` might apply the workload on this `ManagedCluster`. After the `ManagedCluster` is no longer selected and it is removed from the `PlacementDecisions`, the workload that is applied on this `ManagedCluster` should be removed accordingly.

See the following `PlacementDecision` sample:

[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: PlacementDecision
metadata:
  labels:
    cluster.open-cluster-management.io/placement: placement1
  name: placement1-kbc7q
  namespace: ns1
  ownerReferences:
    - apiVersion: cluster.open-cluster-management.io/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Placement
      name: placement1
      uid: 05441cf6-2543-4ecc-8389-1079b42fe63e
status:
  decisions:
    - clusterName: cluster1
      reason: ''
    - clusterName: cluster2
      reason: ''
    - clusterName: cluster3
      reason: ''
----

[#addon-status]
== Add-on status

You might want to select managed clusters for your placements according to the status of the add-ons that are deployed on them. For example, you want to select a managed cluster for your placement only if there is a specific add-on that is enabled on the cluster. 

You can do this by specifying the label for the add-on, as well as its status, if necessary, when you create the Placement. A label is automatically created on a `ManagedCluster` resource if an add-on is enabled on the cluster. The label is automatically removed if the add-on is disabled.

Each add-on is represented by a label in the format of `feature.open-cluster-management.io/addon-<addon_name>=<status_of_addon>`. 

Replace `addon_name` with the name of the add-on that should be enabled on the managed cluster that you want to select. 

Replace `status_of_addon` with the status that the add-on should have if the cluster is selected. The possible values of `status_of_addon` are in the following list:

* `available`: The add-on is enabled and available.
* `unhealthy`: The add-on is enabled, but the lease is not updated continuously.
* `unreachable`: The add-on is enabled, but there is no lease found for it. This can also be caused when the managed cluster is offline.

For example, an available `application-manager` add-on is represented by a label on the managed cluster that reads:

----
feature.open-cluster-management.io/addon-application-manager: available
----

See the following examples of creating placements based on add-ons and their status:

- You can create a placement that includes all managed clusters that have `application-manager` enabled on them by adding the following YAML content: 
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement1
  namespace: ns1
spec:
  predicates:
    - requiredClusterSelector:
        labelSelector:
          matchExpressions:
            - key: feature.open-cluster-management.io/addon-application-manager
              operator: Exists
----

- You can create a placement that includes all managed clusters that have `application-manager` enabled with an `available` status by adding the following YAML content: 
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement2
  namespace: ns1
spec:
  predicates:
    - requiredClusterSelector:
        labelSelector:
          matchLabels:
            "feature.open-cluster-management.io/addon-application-manager": "available"
----

- You can create a placement that includes all managed clusters that have `application-manager` disabled by adding the following YAML content: 
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: placement3
  namespace: ns1
spec:
  predicates:
    - requiredClusterSelector:
        labelSelector:
          matchExpressions:
            - key: feature.open-cluster-management.io/addon-application-manager
              operator: DoesNotExist
----

[#extensible-scheduling]
== Extensible scheduling

In placement resource-based scheduling, sometimes the prioritizer needs more data than the default value provided by the `MananagedCluster` resource to calculate the score of the managed cluster. For example, schedule the clusters based on CPU or memory usage data of the clusters that are fetched obtained through a monitoring system.

The API `AddOnPlacementScore` supports a more extensible way to schedule based on customized scores.

* You can specify the score in the `placement.yaml` file to select clusters.
* As a  score provider, a 3rd party controller can run on either the hub cluster or the managed cluster, to maintain the lifecycle of `AddOnPlacementScore` and update score into it.

Refer to https://github.com/open-cluster-management-io/enhancements/blob/main/enhancements/sig-architecture/32-extensiblescheduling/32-extensiblescheduling.md[placement extensible scheduling enhancement] in the `open-cluster-management` repository to learn more.
