[#cluster-proxy-addon]
= Enabling cluster proxy add-ons

In some environments, a managed cluster is behind a firewall and cannot be accessed directly by the hub cluster. To gain access, you can set up a proxy add-on to access the `kube-api` server of the managed cluster to provide a more secure connection. 

*Required access*: Editor

To configure a cluster proxy add-on for a hub cluster and a managed cluster, complete the following steps:

. Enable the cluster proxy add-on on {product-title} hub cluster. See link:../install_upgrade/adv_config_install.adoc#cluster-proxy-addon[Advanced configuration] to learn more.

. Configure the `kubeconfig` file to access the managed cluster `kube-apiserver` by completing the following steps:

.. Provide a valid access token for the managed cluster. You can use the corresponding token of the service account, assuming the default service account is in the default namespace.

... Make sure that you are using the context of the managed cluster. Assume the file named `managed-cluster.kubeconfig` is the `kubeconfig` file of the managed cluster. *Tip:* Commands with `--kubeconfig=managed-cluster.kubeconfig` run on the managed cluster, and all of the commands in this procedure should run in the same console. Do not run commands in different consoles.

... Add a role to your service account that allows it to access pods by running the following commands:
+
----
oc create role -n default test-role --verb=list,get --resource=pods --kubeconfig=managed-cluster.kubeconfig
oc create rolebinding -n default test-rolebinding --serviceaccount=default:default --role=test-role --kubeconfig=managed-cluster.kubeconfig
----

... Run the following command to locate the secret of the service account token:
+
----
oc get secret -n default --kubeconfig=managed-cluster.kubeconfig | grep default-token
----

... Run the following command to copy the token:
+
----
export MANAGED_CLUSTER_TOKEN=$(kubectl --kubeconfig=managed-cluster.kubeconfig -n default get secret <default-token> -o jsonpath={.data.token} | base64 -d) 
----
+
Replace `default-token` with the name of your secret.

.. Configure the `kubeconfig` file on the {product-title-short} hub cluster.

... Export the current `kubeconfig` file on the hub cluster by running the following command:
+
----
oc config view --minify --raw=true > cluster-proxy.kubeconfig
----

... Modify the `server` file with your editor. This example uses commands when using `sed`. Run `alias sed=gsed`, if you are using OSX.
+
----
export TARGET_MANAGE_CLUSTER=<cluster1> 

export NEW_SERVER=https://$(oc get route -n open-cluster-management cluster-proxy-addon-user -o=jsonpath='{.spec.host}')/$TARGET_MANAGE_CLUSTER

sed -i'' -e '/server:/c\    server: '"$NEW_SERVER"'' cluster-proxy.kubeconfig

export CADATA=$(oc get configmap -n openshift-service-ca kube-root-ca.crt -o=go-template='{{index .data "ca.crt"}}' | base64)

sed -i'' -e '/certificate-authority-data:/c\    certificate-authority-data: '"$CADATA"'' cluster-proxy.kubeconfig
----
+
Replace `cluster1` with a managed cluster name that you want to access. 

... Delete the original user credentials by entering the following commands: 
+
----
sed -i'' -e '/client-certificate-data/d' cluster-proxy.kubeconfig
sed -i'' -e '/client-key-data/d' cluster-proxy.kubeconfig
sed -i'' -e '/token/d' cluster-proxy.kubeconfig
----

... Add the token of the service account:
+
----
sed -i'' -e '$a\    token: '"$MANAGED_CLUSTER_TOKEN"'' cluster-proxy.kubeconfig
----

. List all of the pods on the target namespace of the target managed cluster by running the following command: 
+
----
oc get pods --kubeconfig=cluster-proxy.kubeconfig -n <default> 
----
+
Replace the `default` namespace with the namespace that you want to use.

Your hub cluster is now communicating with the `kube-api` of your managed cluster. 
