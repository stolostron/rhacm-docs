[#creating-a-cluster-on-bare-metal]
= Creating a cluster on bare metal

You can use the {product-title} console to create a {ocp} cluster in a bare metal environment.

[#bare_prerequisites]
== Prerequisites

You need the following prerequisites before creating a cluster in a bare metal environment:

* A deployed {product-title} hub cluster on {ocp-short} version 4.5, or later.
* Internet access for your {product-title} hub cluster so it can create the Kubernetes cluster in the bare metal environment
* Bare metal provider connection;
see xref:../manage_cluster/prov_conn_bare.adoc#creating-a-provider-connection-for-bare-metal[Creating a provider connection for bare metal] for more information
* Login credentials for your bare metal environment, which include user name, password, and Baseboard Management Controller Address
* A {ocp} image pull secret;
see https://docs.openshift.com/container-platform/4.4/openshift_images/managing_images/using-image-pull-secrets.html[Using image pull secrets]

**Note**: If you change your cloud provider access key, you must manually update the provisioned cluster access key. For more information, see the known issue, link:../release_notes/known_issues.adoc#automatic-secret-updates-for-provisioned-clusters-is-not-supported[Automatic secret updates for provisioned clusters is not supported].

[#bare_creating-your-cluster-with-the-red-hat-advanced-cluster-management-for-kubernetes-console]
== Creating your cluster with the {product-title-short} console

To create clusters from the {product-title} console, complete the following steps:

. From the navigation menu, navigate to *Automate infrastructure* > *Clusters*.
. On the Clusters page, Click *Add Cluster*.
. Select *Create a cluster*.
+
*Note:* This procedure is for creating a cluster.
If you have an existing cluster that you want to import, see xref:../manage_cluster/import.adoc#importing-a-target-managed-cluster-to-the-hub-cluster[Importing a target managed cluster to the hub cluster] for those steps.

. Enter a name for your cluster. For a bare metal cluster, this name cannot be an arbitrary name. It is associated with the cluster URL. Make sure that the cluster name that you use is consistent with your DNS and network setup.
+
*Tip:* You can view the `yaml` content updates as you enter the information in the console by setting the _YAML_ switch to *ON*.

. Select *Bare Metal* for the infrastructure platform.
. Specify a *Release image* that you want to use for the cluster.
This identifies the version of the {ocp} image that is used to create the cluster.
If the version that you want to use is available, you can select the image from the list of images.
If the image that you want to use is not a standard image, you can enter the url to the image that you want to use.
See xref:../manage_cluster/release_images.adoc#release-images[Release images] for more information about release images.
. Select your provider connection from the available connections on the list.
If you do not have one configured, or want to configure a new one, select *Add provider*. See xref:../manage_cluster/prov_conn_bare.adoc#creating-a-provider-connection-for-bare-metal[Creating a provider connection for bare metal] for more information about creating a provider connection.
. Enter the base domain information that you configured in your bare metal environment. If there is already a base domain associated with the selected provider connection, that value is populated in that field. You can change the value by overwriting it. For a bare metal cluster, this setting is associated with the cluster URL. Make sure that the base domain that you use is consistent with your DNS and network setup.
. Select your hosts from the list of hosts that are associated with your provider connection.
Select a minimum of three assets that are on the same bridge networks as the hypervisor.
. Configure the cluster networking options.
+
|===
| Parameter | Description | Required or Optional

| Base DNS domain | The base domain of your provider, which is used to create routes to your {ocp} cluster components. It is configured in your cluster provider's DNS as a Start of Authority (SOA) record. This setting cannot be changed after the cluster is created. | Required
| Network type | The pod network provider plug-in to deploy. Only the OpenShiftSDN plug-in is supported on {ocp-short} 4.3. The OVNKubernetes plug-in is available as a Technology Preview on {ocp-short} 4.3. | Required
| Cluster network CIDR | A block of IP addresses from which pod IP addresses are allocated. The OpenShiftSDN network plug-in supports multiple cluster networks. The address blocks for multiple cluster networks must not overlap. Select address pools large enough to fit your anticipated workload. The default values is 10.128.0.0/14. | Required
| Network host prefix | The subnet prefix length to assign to each individual node. For example, if hostPrefix is set to 23, then each node is assigned a /23 subnet out of the given CIDR, allowing for 510 (2^(32-23)-2) pod IP addresses. The default is 23. | Required
| Service network CIDR | A block of IP addresses for services. OpenShiftSDN allows only one serviceNetwork block. The address must not overlap any other network block. The default value is 172.30.0.0/16. | Required
| Machine CIDR | A block of IP addresses used by the {ocp-short} hosts. The address block must not overlap any other network block. The default value is 10.0.0.0/16. | Required
| Provisioning network CIDR | The CIDR for the network to use for provisioning. The example format is: 172.30.0.0/16. | Required
| Provisioning network interface | The name of the network interface on the control plane nodes that are connected to the provisioning network. | Optional
| Provisioning network bridge | The name of the bridge on the hypervisor that is attached to the provisioning network. | Optional
| External network bridge | The name of the bridge of the hypervisor that is attached to the external network. | Optional
| API VIP | The Virtual IP to use for internal API communication. The DNS must be pre-configured with an A/AAAA or CNAME record so the `api.<cluster_name>.<Base DNS domain>` path resolves correctly. | Optional
| Ingress VIP | The Virtual IP to use for ingress traffic. The DNS must be pre-configured with an A/AAAA or CNAME record so the `*.apps.<cluster_name>.<Base DNS domain>` path resolves correctly. | Optional


|===



. *Optional:* Configure a label for the cluster.
. *Optional:* Update the advanced settings, if you want to change the setting for including a configmap.
. Click *Create*.
You can view your cluster details after the create and import process is complete.

+
NOTE: You do not have to run the `kubectl` command that is provided with the cluster details to import the cluster. When you create the cluster, it is automatically configured under the management of {product-title}. 

[#bare_accessing-your-cluster]
== Accessing your cluster

To access a cluster that is managed by {product-title}, complete the following steps:

. From the {product-title} navigation menu, navigate to *Automate infrastructure* > *Clusters*.
. Select the name of the cluster that you created or want to access.
The cluster details are displayed.
. Select *Reveal credentials* to view the user name and password for the cluster.
Note these values to use when you log in to the cluster.
. Select *Console URL* to link to the cluster.
. Log in to the cluster by using the user ID and password that you found in step 3.
. Select the _Actions_ menu for the cluster that you want to access.
. Select *Launch to cluster*.
+
TIP: If you already know the log in credentials, you can access the cluster by selecting the _Actions_ menu for the cluster, and selecting *Launch to cluster*.

[#bare_removing-a-cluster-from-management]
== Removing a cluster from management

When you remove a {ocp} cluster from management that was created with {product-title}, you can either _detach_ it or _destroy_ it.

When you detach or destroy a bare metal cluster, either action removes it from management by {product-title-short}. You can import it again, if you decide that you want to bring it back under management.
This is only an option when the cluster is in a _Ready_ state.

*Note:* Destroying a bare metal cluster by using the {product-title-short} console is different from destroying clusters from other providers in that it does not completely delete the bare metal cluster.

. From the navigation menu, navigate to *Automate infrastructure* > *Clusters*.
. Select the option menu beside the cluster that you want to delete.
. Select *Destroy cluster* or *Detach cluster*.
+
*Tip:* You can detach or destroy multiple clusters by selecting the check boxes of the clusters that you want to detach or destroy.
Then select *Detach* or *Destroy*.
