[#upgrading-disconnected-clusters]
= Upgrading disconnected clusters

You can use {cincinnati} with {product-title} to upgrade your clusters in a disconnected environment.

In some cases, security concerns prevent clusters from being connected directly to the Internet. This makes it difficult to know when upgrades are available, and how to process those upgrades. Configuring {cincinnati-short} can help. 

{cincinnati-short} is a separate operator and operand that monitors the available versions of your managed clusters in a disconnected environment, and makes them available for upgrading your clusters in a disconnected environment. After {cincinnati-short} is configured, it can perform the following actions:

. Monitor when upgrades are available for your disconnected clusters.
. Identify which updates are mirrored to your local site for upgrading by using the graph data file.
. Notify you that an upgrade is available for your cluster by using the {product-title-short} console.

[#cincinnati-prerequisites]
== Prerequisites

You must have the following prerequisites before you can use {cincinnati-short} to upgrade your disconnected clusters:

* A deployed {product-title-short} hub cluster that is running on {ocp} version 4.5, or later with restricted OLM configured. See https://docs.openshift.com/container-platform/4.5/operators/olm-restricted-networks.html[Using Operator Lifecycle Manager on restricted networks] for details about how to configure restricted OLM. 
+
*Tip:* Make a note of the catalog source image when you configure restricted OLM.
* An {ocp-short} cluster that is managed by the {product-title-short} hub cluster
* Access credentials to a local repository where you can mirror the cluster images. See https://docs.openshift.com/container-platform/4.5/installing/install_config/installing-restricted-networks-preparations.html[Creating a mirror registry for installation in a restricted network] for more information about how to create this repository.
+
*Note:* The image for the current version of the cluster that you upgrade must always be available as one of the mirrored images. If an upgrade fails, the cluster reverts back to the version of the cluster at the time that the upgrade was attempted.

[#prepare-your-disconnected-mirror-registry]
== Prepare your disconnected mirror registry

You must mirror both the image that you want to upgrade to and the current image that you are upgrading from to your local mirror registry. Complete the following steps to mirror the images:

. Create a script file that contains content that resembles the following example:
+
----
UPSTREAM_REGISTRY=quay.io
PRODUCT_REPO=openshift-release-dev
RELEASE_NAME=ocp-release
OCP_RELEASE=4.5.2-x86_64
LOCAL_REGISTRY=$(hostname):5000
LOCAL_SECRET_JSON=/path/to/pull/secret

oc adm -a ${LOCAL_SECRET_JSON} release mirror \
--from=${UPSTREAM_REGISTRY}/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE} \
--to=${LOCAL_REGISTRY}/ocp4 \
--to-release-image=${LOCAL_REGISTRY}/ocp4/release:${OCP_RELEASE}
----
+
Replace `/path/to/pull/secret` with the path to your {ocp-short} pull secret.

. Run the script to mirror the images, configure settings, and separate the release images from the release content.

*Tip:* You can use the output of the last line of this script when you create your `ImageContentSourcePolicy`.

[#deploy-the-operator-for-cincinnati]
== Deploy the operator for {cincinnati-short}

To deploy the operator for {cincinnati-short} in your {ocp-short} environment, complete the following steps:

. On the hub cluster, access the {ocp-short} operator hub. 
. Deploy the operator by selecting `{cincinnati} Operator`. Update the default values, if necessary. The deployment of the operator creates a new project named `openshift-cincinnati`.
. Wait for the installation of the operator to finish. 
+
**Tip:** You can check the status of the installation by entering the `oc get pods` command on your {ocp-short} command line. Verify that the operator is in the `running` state.

[#build-the-graph-data-init-container]
== Build the graph data init container

{cincinnati-short} uses graph data information to determine the available upgrades. In a connected environment, {cincinnati-short} pulls the graph data information for available upgrades directly from the https://github.com/openshift/cincinnati-graph-data[Cincinnati graph data GitHub repository]. Because you are configuring a disconnected environment, you must make the graph data available in a local repository by using an `init container`. Complete the following steps to create a graph data `init container`:

. Clone the _graph data_ Git repository by entering the following command:
+
----
git clone https://github.com/openshift/cincinnati-graph-data
----

. Create a file that contains the information for your graph data `init`. You can find this sample https://github.com/openshift/cincinnati-operator/blob/master/dev/Dockerfile[Dockerfile] in the `cincinnati-operator` GitHub repository. The contents of the file is shown in the following sample:
+
----
FROM registry.access.redhat.com/ubi8/ubi:8.1

RUN curl -L -o cincinnati-graph-data.tar.gz https://github.com/openshift/cincinnati-graph-data/archive/master.tar.gz

RUN mkdir -p /var/lib/cincinnati/graph-data/

CMD exec /bin/bash -c "tar xvzf cincinnati-graph-data.tar.gz -C /var/lib/
cincinnati/graph-data/ --strip-components=1"  
----
+
In this example:
+
* The `FROM` value is the external registry where {cincinnati-short} finds the images.

* The `RUN` commands create the directory and package the upgrade files. 

* The `CMD` command copies the package file to the local repository and extracts the files for an upgrade.

. Run the following commands to build the `graph data init container`:
+
----
podman build -f <path_to_Dockerfile> -t ${DISCONNECTED_REGISTRY}/cincinnati/cincinnati-graph-data-container:latest
podman push ${DISCONNECTED_REGISTRY}/cincinnati/cincinnati-graph-data-container:latest --authfile=/path/to/pull_secret.json
----
+
Replace _path_to_Dockerfile_ with the path to the file that you created in the previous step.
+
Replace _${DISCONNECTED_REGISTRY}/cincinnati/cincinnati-graph-data-container_ with the path to your local graph data init container.
+
Replace _/path/to/pull_secret_ with the path to your pull secret file.
+
*Note:* You can also replace `podman` in the commands with `docker`, if you don't have `podman` installed.


[#configure-certificate-for-the-mirrored-registry]
== Configure certificate for the mirrored registry 

If you are using a secure external container registry to store your mirrored {ocp-short} release images, {cincinnati-short} requires access to this registry to build
an upgrade graph. Complete the following steps to configure your CA certificate to work with the {cincinnati-short} pod:

. Find the {ocp-short} external registry API, which is located in `image.config.openshift.io`. This is where the external registry CA certificate is stored.  
+
See https://docs.openshift.com/container-platform/4.5/registry/configuring-registry-operator.html#images-configuration-cas_configuring-registry-operator[Image Registry Operator in OpenShift Container Platform] in the {ocp-short} documentation for more information.

. Create a ConfigMap in the `openshift-config` namespace. 

. Add your CA certificate under the key `cincinnati-registry`. {cincinnati-short} uses this setting to locate your certificate:
+
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: trusted-ca
data:
  cincinnati-registry: |
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
----

. Edit the `cluster` resource in the `image.config.openshift.io` API to set the `additionalTrustedCA` field to the name of the ConfigMap that you created.
+
----
oc patch image.config.openshift.io cluster -p '{"spec":{"additionalTrustedCA":{"name":"trusted-ca"}}}' --type merge
----
+
Replace _trusted-ca_ with the path to your new ConfigMap.

The {cincinnati-short} Operator watches the `image.config.openshift.io` API and the
ConfigMap you created in the `openshift-config` namespace for changes, then
restart the deployment if the CA cert has changed.

[#deploy-the-cincinnati-instance]
== Deploy the {cincinnati-short} instance

When you finish deploying the {cincinnati-short} instance on your hub cluster, this instance is located where the images for the cluster upgrades are mirrored and made available to the disconnected managed cluster. Complete the following steps to deploy the instance:

. If you do not want to use the default namespace of the operator, which is `openshift-cincinnati`, create a namespace for your {cincinnati-short} instance:
.. In the {ocp-short} hub cluster console navigation menu, select *Administration* > *Namespaces*.
.. Select *Create Namespace*.
.. Add the name of your namespace, and any other information for your namespace.
.. Select *Create* to create the namespace.
. In the _Installed Operators_ section of the {ocp-short} console, select *{cincinnati} Operator*.
. Select *Create Instance* in the menu.
. Paste the contents from your {cincinnati-short} instance. Your YAML instance might resemble the following manifest:
+
----
apiVersion: cincinnati.openshift.io/v1beta1
kind: Cincinnati
metadata:
  name: openshift-update-service-instance
  namespace: openshift-cincinnati
spec:
  registry: <registry_host_name>:<port>
  replicas: 1
  repository: ${LOCAL_REGISTRY}/ocp4/release
  graphDataImage: '<host_name>:<port>/cincinnati-graph-data-container'
----
+
Replace the `spec.registry` value with the path to your local disconnected registry for your images.
+
Replace the `spec.graphDataImage` value with the path to your graph data init container. *Tip:* This is the same value that you used when you ran the `podman push` command to push your graph data init container.
. Select *Create* to create the instance. 
. From the hub cluster CLI, enter the `oc get pods` command to view the status of the instance creation. It might take a while, but the process is complete when the result of the command shows that the instance and the operator are running.

[#deploy-a-policy-to-override-the-default-registry]
== Deploy a policy to override the default registry (optional)

*Note:* The steps in this section only apply if you have mirrored your releases into your mirrored registry. 

{ocp-short} has a default image registry value that specifies where it finds the upgrade packages. In a disconnected environment, you can create a policy to replace that value with the path to your local image registry where you mirrored your release images. 

For these steps, the policy is named _ImageContentSourcePolicy_. Complete the following steps to create the policy:

. Log in to the {ocp-short} environment of your hub cluster.
. In the {ocp-short} navigation, select *Administration* > *Custom Resource Definitions*.
. Select the _Instances_ tab.
. Select the name of the _ImageContentSourcePolicy_ that you created when you set up your disconnected OLM to view the contents.
. Select the _YAML_ tab to view the content in `YAML` format. 
. Copy the entire contents of the ImageContentSourcePolicy.

. From the {product-title-short} console, select *Govern risk* > *Create policy*.
. Set the `YAML` switch to _On_ to view the YAML version of the policy.
. Delete all of the content in the `YAML` code. 
. Paste the following `YAML` content into the window to create a custom policy:
+
----
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-pod
  namespace: default
  annotations:
    policy.open-cluster-management.io/standards: 
    policy.open-cluster-management.io/categories: 
    policy.open-cluster-management.io/controls: 
spec:
  disabled: false
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: policy-pod-sample-nginx-pod
        spec:
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: v1
                kind: Pod
                metadata:
                  name: sample-nginx-pod
                  namespace: default
                status:
                  phase: Running
          remediationAction: inform
          severity: low
  remediationAction: enforce
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: binding-policy-pod
  namespace: default
placementRef:
  name: placement-policy-pod
  kind: PlacementRule
  apiGroup: apps.open-cluster-management.io
subjects:
- name: policy-pod
  kind: Policy
  apiGroup: policy.open-cluster-management.io
---
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: placement-policy-pod
  namespace: default
spec:
  clusterConditions:
  - status: "True"
    type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      []  # selects all clusters if not specified
----

. Replace the content inside the `objectDefinition` section of the template with content that is similar to the following content to add the settings for your ImageContentSourcePolicy: 
+
----
apiVersion: operator.openshift.io/v1alpha1
kind: ImageContentSourcePolicy
metadata:
  name: ImageContentSourcePolicy
spec:
  repositoryDigestMirrors:
  - mirrors:
    - <path-to-local-mirror>
    source: registry.redhat.io
----
+
* Replace _path-to-local-mirror_ with the path to your local mirror repository.
* Tip: You can find your path to your local mirror by entering the `oc adm release mirror` command.

. Select the box for *Enforce if supported*.
. Select *Create* to create the policy. 

[#deploy-a-policy-to-deploy-a-disconnected-catalog-source]
== Deploy a policy to deploy a disconnected catalog source

Push the _Catalogsource_ policy to the managed cluster to change the default location from a connected location to your disconnected local registry. 

. In the {product-title-short} console, select *Automate infrastructure* > *Clusters*.
. Find the managed cluster to receive the policy in the list of clusters.
. Note the value of the `name` label for the managed cluster. The label format is `name=managed-cluster-name`. This value is used when pushing the policy.
. In the {product-title-short} console menu, select *Govern risk* > *Create policy*.
. Set the `YAML` switch to _On_ to view the YAML version of the policy.
. Delete all of the content in the `YAML` code. 
. Paste the following `YAML` content into the window to create a custom policy:
. Paste the following `YAML` content into the window to create a custom policy:
+
----
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-pod
  namespace: default
  annotations:
    policy.open-cluster-management.io/standards: 
    policy.open-cluster-management.io/categories: 
    policy.open-cluster-management.io/controls: 
spec:
  disabled: false
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: policy-pod-sample-nginx-pod
        spec:
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: v1
                kind: Pod
                metadata:
                  name: sample-nginx-pod
                  namespace: default
                status:
                  phase: Running
          remediationAction: inform
          severity: low
  remediationAction: enforce
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: binding-policy-pod
  namespace: default
placementRef:
  name: placement-policy-pod
  kind: PlacementRule
  apiGroup: apps.open-cluster-management.io
subjects:
- name: policy-pod
  kind: Policy
  apiGroup: policy.open-cluster-management.io
---
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: placement-policy-pod
  namespace: default
spec:
  clusterConditions:
  - status: "True"
    type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      []  # selects all clusters if not specified
----

. Add the following content to the policy:
+
----
apiVersion: config.openshift.io/vi
kind: OperatorHub
metadata:
 name: cluster
spec:
 disableAllDefaultSources: true
----
+
. Add the following content:
+
----
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: my-operator-catalog
  namespace: openshift-marketplace
spec:
  sourceType: grpc
  image: <registry_host_name>:<port>/olm/redhat-operators:v1 
  displayName: My Operator Catalog
  publisher: grpc
----
+
Replace the value of _spec.image_ with the path to your local restricted catalog source image.

. In the {product-title-short} console navigation, select *Automate infrastructure* > *Clusters* to check the status of the managed cluster. When the policy is applied, the cluster status is `ready`.

[#deploy-a-policy-to-change-the-managed-cluster-parameter]
== Deploy a policy to change the managed cluster parameter

Push the _ClusterVersion_ policy to the managed cluster to change the default location where it retrieves its upgrades. 

. From the managed cluster, confirm that the _ClusterVersion_ upstream parameter is currently the default public {cincinnati-short} operand by entering the following command:
+
----
oc get clusterversion -o yaml
----
+
The returned content might resemble the following content:
+
----
apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: ClusterVersion
[..]
  spec:
    channel: stable-4.4
    upstream: https://api.openshift.com/api/upgrades_info/v1/graph
---- 
 
. From the hub cluster, identify the route URL to the {cincinnati-short} operand by entering the following command: `oc get routes`. 
+
*Tip:* Note this value for later steps.

. In the hub cluster {product-title-short} console menu, select *Govern risk* > *Create a policy*.
. Set the `YAML` switch to _On_ to view the YAML version of the policy.
. Delete all of the content in the `YAML` code. 
. Paste the following `YAML` content into the window to create a custom policy:
+
----
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: policy-pod
  namespace: default
  annotations:
    policy.open-cluster-management.io/standards: 
    policy.open-cluster-management.io/categories: 
    policy.open-cluster-management.io/controls: 
spec:
  disabled: false
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: policy-pod-sample-nginx-pod
        spec:
          object-templates:
            - complianceType: musthave
              objectDefinition:
                apiVersion: v1
                kind: Pod
                metadata:
                  name: sample-nginx-pod
                  namespace: default
                status:
                  phase: Running
          remediationAction: inform
          severity: low
  remediationAction: enforce
---
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: binding-policy-pod
  namespace: default
placementRef:
  name: placement-policy-pod
  kind: PlacementRule
  apiGroup: apps.open-cluster-management.io
subjects:
- name: policy-pod
  kind: Policy
  apiGroup: policy.open-cluster-management.io
---
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: placement-policy-pod
  namespace: default
spec:
  clusterConditions:
  - status: "True"
    type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      []  # selects all clusters if not specified
----
. Add the following content to `policy.spec` in the _policy_ section:
+
----
apiVersion: config.openshift.io/v1
  kind: ClusterVersion
  metadata:
    name: version
  spec:
    channel: stable-4.4
    clusterID: example-cluster-id
    upstream: https://example-cincinnati-policy-engine-uri/api/upgrades_info/v1/graph
----
+
Replace the value of _spec.upstream_ with the path to your hub cluster {cincinnati-short} operand.
+
*Tip:* You can complete the following steps to determine the path to the operand:
+
 .. Run the `oc get get routes -A` command on the hub cluster.
 .. Find the route to `cincinnati`.
 +
 The path to the operand is the value in the `HOST/PORT` field.

. In the managed cluster CLI, confirm that the upstream parameter in the `ClusterVersion` is updated with the local hub cluster {cincinnati-short} URL by entering: 
+
----
oc get clusterversion -o yaml
----
+
Verify that the results resemble the following content:
+
----
apiVersion: v1
items:
- apiVersion: config.openshift.io/v1
  kind: ClusterVersion
[..]
  spec:
    channel: stable-4.4
    clusterID: 46acf6fb-471a-45c8-9a59-0fac0685dec6
    upstream: https://<hub-cincinnati-url>/api/upgrades_info/v1/graph
----

[#viewing-available-upgrades]
== Viewing available upgrades

You can view a list of available upgrades for your managed cluster by completing the following steps:

. Log in to your {product-title-short} console.
. In the navigation menu, select *Automate Infrastructure* > *Clusters*.
. Select a cluster that is in the _Ready_ state.
. From the *Options* menu, select *Upgrade cluster*. 
. Verify that the optional upgrade paths are available. 
+
*Note:* No available upgrade versions are shown if the current version is not mirrored into the local image repository.  

[#upgrading-the-cluster]
== Upgrading the cluster

After configuring the disconnnected registry, {product-title-short} and {cincinnati-short} use the disconnected registry to determine if upgrades are available. If no available upgrades are displayed, make sure that you have the release image of the current level of the cluster and at least one later level mirrored in the local repository. If the release image for the current version of the cluster is not available, no upgrades are available.

Complete the following steps to upgrade:

. In the {product-title-short} console, select *Automate infrastructure* > *Clusters*.

. Find the cluster that you want to determine if there is an available upgrade. 

. If there is an upgrade available, the *Distribution version* column for the cluster indicates that there is an upgrade available. 

. Select the _Options_ menu for the cluster, and select *Upgrade cluster*.

. Select the target version for the upgrade, and select *Upgrade*. 

The managed cluster is updated to the selected version. 