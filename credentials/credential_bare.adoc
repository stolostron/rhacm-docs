[#creating-a-credential-for-bare-metal]
= Creating a credential for bare metal

You need a credential to use the {product-title} console to deploy and manage a {ocp} cluster in a bare metal environment. The credential specifies the connection to a provisioning node that is used as a bootstrap host virtual machine (VM) when creating the cluster. 

**Required access**: Edit

* <<bare_cred_prerequisites,Prerequisites>>
* <<bare-set-up-provisioning,Preparing a provisioning host>>
* <<bare_cred,Creating a credential by using the console>>
* <<bare_delete_cred,Deleting your credential>>

[#bare_cred_prerequisites]
== Prerequisites

You need the following prerequisites before creating a credential:

* A {product-title} hub cluster that is deployed.
When managing bare metal clusters, you must have the hub cluster installed on {ocp} version 4.6 or later.
* Internet access for your {product-title} hub cluster so it can create the Kubernetes cluster on your bare metal server.
* For a disconnected environment, you must have a configured mirror registry where you can copy the release images for your cluster creation. See https://access.redhat.com/documentation/en-us/openshift_container_platform/4.8/html/installing/installing-mirroring-installation-images[Mirroring images for a disconnected installation] in the {ocp-short} documentation for more information.
* Account permissions that support installing clusters on the bare metal infrastructure.


[#bare-set-up-provisioning]
== Preparing a provisioning host

When you create a bare metal credential and cluster, you must have an available provisioning host. The provisioning host provides a bootstrap host VM for the installation. This can be a VM or a service running Kernel-based virtual machine (KVM). You need the details of this host when you are creating the credential and the cluster. Complete the following steps to configure a provisioning host:

. Log in to the provisioner node using `SSH`.

. Create a non-root user (user-name) and provide that user with sudo privileges by running the following commands:
+
----
useradd <user-name>
passwd <password>
echo "<user-name> ALL=(root) NOPASSWD:ALL" | tee -a /etc/sudoers.d/<user-name>
chmod 0440 /etc/sudoers.d/<user-name>
----

. Create an SSH key for the new user by entering the following command:
+
----
su - <user-name> -c "ssh-keygen -t rsa -f /home/<user-name>/.ssh/id_rsa -N ''"
----

. Log in as the new user on the provisioner node.
+
----
su - <user-name>
[user-name@provisioner ~]$
----

. Use Red Hat Subscription Manager to register the provisioner node by entering the following commands:
+
----
sudo subscription-manager register --username=<user-name> --password=<password> --auto-attach
sudo subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms --enable=rhel-8-for-x86_64-baseos-rpms
----
+
For more information about Red Hat Subscription Manager, see https://access.redhat.com/documentation/en-us/red_hat_subscription_management/1/html-single/rhsm/index[Using and Configuring Red Hat Subscription Manager] in the {ocp} documentation.

. Install the required packages by running the following command:
+
----
sudo dnf install -y libvirt qemu-kvm mkisofs python3-devel jq ipmitool
----

. Modify the user to add the `libvirt` group to the newly created user.
+
----
sudo usermod --append --groups libvirt <user-name>
----

. Restart `firewalld` and enable the `http` service by entering the following commands:
+
----
sudo systemctl start firewalld
sudo firewall-cmd --zone=public --add-service=http --permanent
sudo firewall-cmd --add-port=5000/tcp --zone=libvirt  --permanent
sudo firewall-cmd --add-port=5000/tcp --zone=public   --permanent
sudo firewall-cmd --reload
----

. Start and enable the `libvirtd` service by entering the following commands:
+
----
sudo systemctl start libvirtd
sudo systemctl enable libvirtd --now
----

. Create the default storage pool and start it by entering the following commands:
+
----
sudo virsh pool-define-as --name default --type dir --target /var/lib/libvirt/images
sudo virsh pool-start default
sudo virsh pool-autostart default
----

. View the followig examples to configure networking:
+
* _Provisioning Network (IPv4 address)_
+
----
sudo nohup bash -c """
    nmcli con down "$PROV_CONN"
    nmcli con delete "$PROV_CONN"
    # RHEL 8.1 appends the word "System" in front of the connection, delete in case it exists
    nmcli con down "System $PROV_CONN"
    nmcli con delete "System $PROV_CONN"
    nmcli connection add ifname provisioning type bridge con-name provisioning
    nmcli con add type bridge-worker ifname "$PROV_CONN" master provisioning
    nmcli connection modify provisioning ipv4.addresses 172.22.0.1/24 ipv4.method manual
    nmcli con down provisioning
    nmcli con up provisioning"""
----
+
The SSH connection might disconnect after you complete this step.
+
The IPv4 address can be any address as long as it is not routable using the baremetal network.
+
* _Provisioning Network (IPv6 address)_
+
----
sudo nohup bash -c """
    nmcli con down "$PROV_CONN"
    nmcli con delete "$PROV_CONN"
    # RHEL 8.1 appends the word "System" in front of the connection, delete in case it exists
    nmcli con down "System $PROV_CONN"
    nmcli con delete "System $PROV_CONN"
    nmcli connection add ifname provisioning type bridge con-name provisioning
    nmcli con add type bridge-worker ifname "$PROV_CONN" master provisioning
    nmcli connection modify provisioning ipv6.addresses fd00:1101::1/64 ipv6.method manual
    nmcli con down provisioning
    nmcli con up provisioning"""
----
+
The SSH connection might disconnect after you complete this step.
+
The IPv6 address can be any address as long as it is not routable using the baremetal network.
+
Ensure that UEFI is enabled and UEFI PXE settings are set to the IPv6 protocol when using IPv6 addressing.

. Reconnect to the provisioner node by using SSH (if required).
+
----
# ssh <user-name>@provisioner.<cluster-name>.<domain>
----

. Verify the connection bridges have been correctly created by running the following command:
+
----
nmcli con show
----
+
Your returned results resemble the following content:
+
[frame=none,grid=none,cols="15%,60%,5%,10%"]
|====
| NAME | UUID | TYPE | DEVICE
| baremetal | 4d5133a5-8351-4bb9-bfd4-3af264801530 | bridge | baremetal
| provisioning | 43942805-017f-4d7d-a2c2-7cb3324482ed | bridge | provisioning
| virbr0 | d9bca40f-eee1-410b-8879-a2d4bb0465e7 | bridge | virbr0
| bridge-worker-eno1 | 76a8ed50-c7e5-4999-b4f6-6d9014dd0812 | ethernet | eno1
| bridge-worker-eno2 | f31c3353-54b7-48de-893a-02d2b34c4736 | ethernet | eno2
|====

. Create a `pull-secret.txt` file by completing the following steps:
+
----
vim pull-secret.txt
----
+
.. In a web browser, navigate to https://console.redhat.com/openshift/install/metal/user-provisioned[Install OpenShift on Bare Metal with user-provisioned infrastructure], and scroll down to the _Downloads_ section. 
.. Click *Copy pull secret*. 
.. Paste the contents into the `pull-secret.txt` file and save the contents in the home directory of the `user-name` user. 

You are ready to create your bare metal credential. 

[#bare_cred]
== Creating a credential by using the console

To create a credential from the {product-title} console, complete the following steps:

. From the navigation menu, navigate to *Credentials*. Existing credentials are displayed.

. Select *Add credential*.
. Select *Bare metal* as your provider.
. Add a name for your credential.
. Select a namespace for your credential from the list.
+
*Tip:* Create a namespace specifically to host your credentials, both for convenience and added security.

. You can optionally add a _Base DNS domain_ for your credential. If you add the base DNS domain to the credential, it is automatically populated in the correct field when you create a cluster with this credential. If you do not add the DNS domain, you can add it when you create your cluster.
. Add your _libvirt URI_. The libvirt URI is for your provisioning node that you created for your bootstrap node. Your libvirt URI should resemble the following example: 
+
----
<qemu+ssh>:://<user-name>@<provision-host.com>/system
----
+
* Replace `qemu+ssh` with your method of connecting to the libvirt daemon on the provisioning host.
* Replace `user-name` with the user name that has access to create the bootstrap node on the provisioning host. 
* Replace `provision-host.com` with a link to your provisioning host. This can be either an IP address or a fully-qualified domain name address.
+
See https://libvirt.org/uri.html[Connection URIs] for more information.
. Add a list of your SSH known hosts for the provisioning host. This value can be an IP address or a fully-qualified domain name address, but is best to use the same format that you used in the libvirt URI value.  
. Enter your _Red Hat OpenShift pull secret_.
You can download your pull secret from https://cloud.redhat.com/openshift/install/pull-secret[Pull secret].
. Add your _SSH private key_ and your _SSH public key_ so you can access the cluster.
You can use an existing key, or use a key generation program to create a new one.
See https://access.redhat.com/documentation/en-us/openshift_container_platform/4.6/html/installing_on_bare_metal/installing-on-bare-metal#ssh-agent-using_installing-bare-metal[Generating an SSH private key and adding it to the agent] for more information about how to generate a key.
. For disconnected installations only: Complete the fields in the *Configuration for disconnected installation* subsection with the required information:
+
* _Image registry mirror_: This value contains the disconnected registry path. The path contains the hostname, port, and repository path to all of the installation images for disconnected installations. Example: `repository.com:5000/openshift/ocp-release`.
+
The path creates an image content source policy mapping in the `install-config.yaml` to the {ocp} release images. As an example, `repository.com:5000` produces this `imageContentSource` content:
+
[source,yaml]
----
imageContentSources:
- mirrors:
  - registry.example.com:5000/ocp4
  source: quay.io/openshift-release-dev/ocp-release-nightly
- mirrors:
  - registry.example.com:5000/ocp4
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - registry.example.com:5000/ocp4
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
----
* _Bootstrap OS image_: This value contains the URL to the image to use for the bootstrap machine.
* _Cluster OS image_: This value contains the URL to the image to use for {ocp} cluster machines. 
* _Additional trust bundle_: This value provides the contents of the certificate file that is required to access the mirror registry.
+
*Note:* If you are deploying managed clusters from a hub that is in a disconnected environment, and want them to be automatically imported post install, add an Image Content Source Policy to the `install-config.yaml` file by using the `YAML` editor. A sample entry is shown in the following example: 
+
[source,yaml]
----
imageContentSources:
- mirrors:
  - registry.example.com:5000/rhacm2
  source: registry.redhat.io/rhacm2
----

. Click *Create*.
. Review the new credential information, then click *Add*. When you add the credential, it is added to the list of credentials.

You can create a cluster that uses this credential by completing the steps in link:../clusters/create_bare.adoc#creating-a-cluster-on-bare-metal[Creating a cluster on bare metal].

[#bare_delete_cred]
== Deleting your credential

When you are no longer managing a cluster that is using a credential, delete the credential to protect the information in the credential.

. From the navigation menu, navigate to *Credentials*.
. Select the options menu for the credential that you want to delete.
. Select *Delete credential*.
