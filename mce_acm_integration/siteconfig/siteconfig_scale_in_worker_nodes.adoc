[#scale-in-worker-nodes]
= Scaling in a {sno} cluster with the {sco}

Scale in your managed cluster that was installed by the {sco}.
You can scale in your cluster by removing a worker node.

[#scale-in-preq]
== Prerequisites

* You have logged in to the hub cluster as a user with `cluster-admin` privileges.
* If using GitOps ZTP, you have configured your GitOps ZTP environment. To configure your environment, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.17/html/edge_computing/ztp-preparing-the-hub-cluster[Preparing the hub cluster for GitOps ZTP].
* You have the default cluster templates. To get familiar with the default templates, see xref:../../acm_mce_integration/siteconfig/cluster_templates.adoc#default-templates[Default set of templates]
* You have installed your cluster with the {sco}. To install a cluster with the {sco}, see see xref:../../acm_mce_integration/siteconfig/install-clusters.adoc##install-clusters[Installing {sno} clusters with the {sco}]

[#scale-in-annotation]
== Adding an annotation to your worker node

Add an annotation to mark the worker node for removal.

Complete the following steps to remove a worker node from the managed cluster:

. Add an annotation in the `extraAnnotations` field of the worker node entry in the `ClusterInstance` CR:
+
[source,yaml]
----
spec:
   ...
   nodes:
   - hostName: "worker-node2.example.com"
      role: "worker"
      ironicInspect: ""
      extraAnnotations:
        BareMetalHost:
          bmac.agent-install.openshift.io/remove-agent-and-node-on-delete: "true"
...
----

. Apply the changes.

.. If you are using {acm} only, run the following command on the hub cluster:
+
[source,terminal]
----
oc apply -f <clusterinstance>.yaml
----

.. If you are using GitOps ZTP, push to your Git repository and wait for Argo CD to synchronize the changes.

. Wait for the `BaremetalHost` resource of that worker node to be deleted. Monitor the process by running the following command on the hub cluster:
+
[source,terminal]
----
oc get bmh -n <managed_cluster_namespace> --watch
----

+
See the following example output for successful worker node deletion:

+
[source,terminal]
----
NAME                                STATE            CONSUMER   ONLINE   ERROR   AGE
master-node1.example.com            provisioned                 true             81m
worker-node2.example.com            deprovisioning              true             44m
worker-node2.example.com            deleting                    true             50m
----

[#scale-in-prunemanifests]
== Pruning the `BareMetalHost` resource of the worker node

Prune the `BareMetalHost` resource of the worker node that you want to delete to prevent reprovisioning of the node.

. Specify the API version and kind of the `BareMetalHost` resource in the node-level `pruneManifests` field:
+
[source,yaml]
----
...
spec:
   ...
   nodes:
     -hostName: "worker-node2.example.com"
      ...
      pruneManifests:
        - apiVersion: metal3.io/v1alpha1
          kind: BareMetalHost
...
----

. Apply the changes.

.. If you are using {acm} only, run the following command on the hub cluster:
+
[source,terminal]
----
oc apply -f <clusterinstance>.yaml
----

.. If you are using GitOps ZTP, push to your Git repository and wait for Argo CD to synchronize the changes.

[#scale-in-delete-verification]
=== Verifying worker node deletion

. Verify that the `Node`, `Agent`, and `BareMetalHost` resources are removed by running the following commands on the hub cluster:
+
[source,terminal]
----
oc get managedclusterinfo/<managed_cluster_name> -n <managed_cluster_namespace>  \
-o jsonpath='{range .status.nodeList[*]}{.name}{"\t"}{.conditions}{"\t"}{.labels}{"\n"}{end}'
----

+
[source,terminal]
----
oc get agents -n <managed_cluster_namespace> --watch
----

+
[source,terminal]
----
oc get bmh -n <managed_cluster_namespace> --watch
----