[#hub-backup-and-restore]
= Cluster backup and restore operator

The cluster backup and restore operator provides disaster recovery solutions for when {product-title} hub cluster goes down and needs to be recreated. It runs on the hub cluster, and depends on the link:https://github.com/openshift/oadp-operator[OADP Operator] to create a connection to a backup storage location on the hub cluster. The backup storage location is then used to backup and restore the user-created hub cluster resources.

The cluster backup and restore operator is not instaleld automatically. Enable the backup component by setting the `cluster-backup` parameter to `true`, in the `MultiClusterHub` resource. The OADP Operator is automatically installed, in the same namesapce as the backup resource, when you install the cluster backup and restore operator.

*Notes*: 

* The OADP Operator 1.0 has disabled building multi-arch builds and only produces `x86_64` builds for the official release. This means that if you are using an architecture other than `x86_64`, the OADP Operator installed by the backup component must be replaced with the correct version. In this case, uninstall the OADP Operator and find the operator matching your architecture, then install it.

* If you have previously installed and used the OADP Operator on the hub cluster, uninstall this version since the backup component works now with OADP installed in the component namespace. Use the same storage location for the link:https://github.com/openshift/oadp-operator/blob/master/docs/install_olm.md#create-the-dataprotectionapplication-custom-resource[`DataProtectionApplication`] resource owned by the OADP Operator installed with the backup component; it accesses the same backup data as the previous operator. Velero backup resources are now loaded within the new OADP Operator namespace on this hub cluster.

link:https://velero.io/[Velero] is installed with the OADP Operator on the {product-title-short} hub cluster; Velero is used to backup and restore {product-title-short} hub cluster resources. 

For a list of supported storage providers for Velero, see https://velero.io/docs/v1.7/supported-providers/#s3-compatible-object-store-providers[S3-Compatible object store providers].

* <<prerequisites-backup-restore,Prerequisites>>
* <<backup-restore-architecture,Backup and restore operator architecture>>
* <<restore-backup,Restore a backup>>
* <<active-passive-config,Active passive configuration>>
** <<managed-cluster-activation-data,Managed cluster activation data>>
** <<resources-restored-managed-cluster,Resources restored at managed activation time>>
** <<restore-passive-resources,Restore passive resources>>
** <<restore-passive-resources-check-backups,Restore passive resources while checking for backups>>
** <<restore-activation-resources,Restore activation resources>>
* <<disaster-recovery,Disaster recovery>>
* <<backup-validation-using-a-policy,Backup validation using a policy>>

[#prerequisites-backup-restore]
== Prerequisites

- You must install the latest version of the OADP Operator. See the link:https://github.com/openshift/oadp-operator/blob/master/docs/install_olm.md[Install OADP Operator using OperatorHub] documentation. 

- Be sure to complete the steps to link:https://github.com/openshift/oadp-operator/blob/master/docs/install_olm.md#create-credentials-secret[Create credentials secret] for the cloud storage where the backups are saved. 

- Use the created secret when you link:https://github.com/openshift/oadp-operator/blob/master/docs/install_olm.md#create-the-velero-custom-resource[Create the Velero Custom Resource].

**Note**: The cluster backup and restore operator resources must be created in the same namespace where the OADP Operator is installed.

[#backup-restore-architecture]
== Backup and restore operator architecture

The operator defines the `backupSchedule.cluster.open-cluster-management.io` resource, which is used to set up {product-title-short} backup schedules, and `restore.cluster.open-cluster-management.io` resource, which is used to process and restore these backups. The operator creates corresponding Velero resources, and defines the options needed to backup remote clusters and any other hub cluster resources that need to be restored. View the following diagram:

image:../images/cluster_backup_controller_dataflow25.png[Backup and restore architecture diagram] 

[#schedule-backup]
== Schedule a cluster backup

After you create a `backupschedule.cluster.open-cluster-management.io` resource you can run the following command to get the status of the scheduled cluster backups: `oc get bsch -n <oadp-operator-ns>`. The `<oadp-operator-ns>` is the namespace where `BackupSchedule` is created and must be in the same namespace where the OADP Operator is installed.

The `backupSchedule.cluster.open-cluster-management.io` resource creates the following three `schedule.velero.io` resources:

* `acm-managed-clusters-schedule`: This resource is used to schedule backups for the managed cluster resources, including managed clusters, cluster pools, and cluster sets.
+
**Note**: The following resources are being retrieved by this backup; they are required for restoring all managed clusters information on the new hub cluster:
+
*** Secrets and ConfigMaps from the `hive` and `openshift-operator-lifecycle-manager` namespaces, and from all of the `ManagedClusters resources` namespaces created on the hub cluster.
*** Cluster-level `ManagedCluster` resource.
*** Other namespace-scoped resources used to restore the managed cluster details, such as `ServiceAccount`, `ManagedClusterInfo`, `ManagedClusterSet`, `ManagedClusterSetBindings`, `KlusterletAddonConfig`, `ManagedClusterView`, `ClusterPool`, `ClusterProvision`, `ClusterDeployment`, `ClusterSyncLease`, `ClusterSync`, and `ClusterCurator`.

* `acm-credentials-schedule`: This resource is used to schedule backups for the user created credentials and any copy of those credentials. These credentials are identified by the `cluster.open-cluster-management.io/type` label selector; all secrets that define the label selector are included in the backup.
+
**Note**: If you have any user-defined private channels, you can include the channel secrets in this credentials backup if you set the `cluster.open-cluster-management.io/type` label selector to this secret. Without the `cluster.open-cluster-management.io/type` label selector, channel secrets are not identified by the `acm-credentials-schedule` backup and need to be manually recreated on the restored cluster.

* `acm-resources-schedule`: This resource is used to schedule backups for the `Applications` and `Policy` resources, including any required resources, such as:
+
`Applications`: `Channels`, `Subscriptions`, `Deployables` and `PlacementRules`
+
`Policy`: `PlacementBindings`, `Placement`, and `PlacementDecisions` 
+
There are no resources being collected from the `local-cluster` or `open-cluster-management` namespaces.


[#restore-backup]
== Restore a backup

In a usual restore scenario, the hub cluster where the backups are run becomes unavailable, and the backed up data needs to be moved to a new hub cluster. This is done by running the cluster restore operation on the new hub cluster. In this case, the restore operation runs on a different hub cluster than the one where the backup is created.

There are also cases where you want to restore the data on the same hub cluster where the backup was collected, so the data from a previous snapshot can be recovered. In this case, both restore and backup operations are run on the same hub cluster.

After you create a `restore.cluster.open-cluster-management.io` resource on the hub cluster, you can run the following command to get the status of the restore operation: `oc get restore -n <oadp-operator-ns>`. You should also be able to verify that the backed up resources that are contained by the backup file are created.

**Note**: The `restore.cluster.open-cluster-management.io` resource is run once. If you want to run the same restore operation again after the restore operation is complete, you have to create a new `restore.cluster.open-cluster-management.io` resource with the same `spec` options.

The restore operation is used to restore all three backup types that are created by the backup operation. However, you can choose to install only a certain type of backup (only managed clusters, only user credentials, or only hub cluster resources).

The restore defines the following three required `spec` properties, where the restore logic is defined for the types of backed up files:

* `veleroManagedClustersBackupName` is used to define the restore option for the managed clusters.
* `veleroCredentialsBackupName` is used to define the restore option for the user credentials.
* `veleroResourcesBackupName` is used to define the restore option for the hub cluster resources (`Applications` and `Policy`).
+
The valid options for the previously mentioned properties are following values:
+
** `latest` - This property restores the last available backup file for this type of backup.
** `skip` - This property does not attempt to restore this type of backup with the current restore operation.
** `<backup_name>` - This property restores the specified backup pointing to it by name. 

The name of the `restore.velero.io` resources that are created by the `restore.cluster.open-cluster-management.io` is generated using the following template rule, `<restore.cluster.open-cluster-management.io name>-<velero-backup-resource-name>`. View the following descriptions:
//Should the passive resource names be added to this list???
* `restore.cluster.open-cluster-management.io name` is the name of the current `restore.cluster.open-cluster-management.io` resource, which initiates the restore.
* `velero-backup-resource-name` is the name of the Velero backup file that is used for restoring the data. For example, the `restore.cluster.open-cluster-management.io` resource named `restore-acm` creates `restore.velero.io` restore resources. View the following examples for the format:
+
** `restore-acm-acm-managed-clusters-schedule-20210902205438` is used for restoring managed cluster backups. In this sample, the `backup.velero.io` backup name used to restore the resource is `acm-managed-clusters-schedule-20210902205438`.
** `restore-acm-acm-credentials-schedule-20210902206789` is used for restoring credential backups. In this sample, the `backup.velero.io` backup name used to restore the resource is `acm-managed-clusters-schedule-20210902206789`.
** `restore-acm-acm-resources-schedule-20210902201234` is used for restoring application and policy backups. In this sample, the `backup.velero.io` backup name used to restore the resource is `acm-managed-clusters-schedule-20210902201234`.
+
*Note*: If `skip` is used for a backup type, `restore.velero.io` is not created.

View the following YAML sample of the cluster `Restore` resource. In this sample, all three types of backed up files are being restored, using the latest available backed up files:

[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Restore
metadata:
  name: restore-acm
spec:
  veleroManagedClustersBackupName: latest
  veleroCredentialsBackupName: latest
  veleroResourcesBackupName: latest
----

**Notes**:

** Only managed clusters created using the `hive` API are automatically imported when the backup is restored on another hub cluster. All other managed clusters show up as _Pending Import_ and must be manually imported back on the new hub cluster.
** When you restore a backup on a new hub cluster, make sure that the previous hub cluster, where the backup was created, is shut down. If it is running, the previous hub cluster tries to reimport the managed clusters as soon as the managed cluster reconciliation finds that the managed clusters are no longer available.

[#active-passive-config]
== Active passive configuration

In an active passive configuration you have

one hub, called active or primary hub, which manages the clusters and is backing up resources at defined time intervals, using the BackupSchedule.cluster.open-cluster-management.io resource
one or more passive hubs, which are continously retrieving the latest backups and restoring the passive data. The passive hubs use the Restore.cluster.open-cluster-management.io resource to keep restoring passive data posted by the primary hub, when new backup data is available. These hubs are on standby to become a primary hub when the primary hub goes down. They are connected to the same storage location where the primary hub backs up data so they can access the primary hub backups. For more details on how to setup this automatic restore configuration see the Restoring passive resources and check for new backups section.


image:../images/active_passive_config_design.png[Active passive configration diagram] 

[#managed-cluster-activation-data]
=== Managed cluster activation data

Managed cluster activation data is the backup data that is being actively managed by the hub cluster, when restored on the hub cluster. Activation data resources are stored by the managed clusters backup and by the resource-generic backup, using the `cluster.open-cluster-management.io/backup: cluster-activations` label. 

[#resources-restored-managed-cluster]
=== Resources restored at managed activation time

When the `cluster.open-cluster-management.io/backup: cluster-activations` label is added to a resource the resource is automatically backed up in the `acm-resources-generic-schedule` backup resource. If any of these resources need to be restored, only when the managed clusters are moved to the new hub cluster, you have to set the label value to `cluster-actvation` when the `veleroManagedClustersBackupName:latest` label is used in the restored resource. 

Your resource might resemble the following example:

[source,yaml]
----
apiVersion: my.group/v1alpha1
kind: MyResource
metadata:
  labels:
    cluster.open-cluster-management.io/backup: cluster-activation
----

There are also default resources in the activation set that are backed up by the `acm-managed-clusters-schedule` resource. View the following default resources that are restored by the `acm-managed-clusters-schedule` resource:

* `managedcluster.cluster.open-cluster-management.io`
* `managedcluster.clusterview.open-cluster-management.io`
* `klusterletaddonconfig.agent.open-cluster-management.io`
* `managedclusteraddon.addon.open-cluster-management.io`
* `managedclusterset.cluster.open-cluster-management.io`
* `managedclusterset.clusterview.open-cluster-management.io`
* `managedclustersetbinding.cluster.open-cluster-management.io`
* `clusterpool.hive.openshift.io`
* `clusterclaim.hive.openshift.io`
* `clustercurator.cluster.open-cluster-management.io`

[#restore-activation-resources]
=== Restore activation resources

Use the link:https://github.com/stolostron/cluster-backup-operator/blob/main/config/samples/cluster_v1beta1_restore_passive_activate.yaml[`restore-passive-activate`] sample when you want the hub cluster to manage the clusters. In this case it is assumed that the other data has been restored already on the hub cluster that using the passive resource.

[#restore-passive-resources]
=== Restore passive resources

Passive data is backup data such as secrets, ConfigMaps, applications, policies, and all the managed cluster custom resources, which do not activate a connection between managed clusters and hub clusters. The backup resources are restored on the hub cluster by the credentials backup and restore resources.


[#restore-passive-resources-check-backups]
=== Restore passive resources while checking for backups

Use the link:https://github.com/stolostron/cluster-backup-operator/blob/main/config/samples/cluster_v1beta1_restore_passive_sync.yaml[`restore-passive-sync`] sample to restore passive data, while continuing to check if new backups are available and resroe them automatically. To automatically restore new backups, you must set the `syncRestoreWithNewBackups` parameter to `true`. You must also only restore the latest passive data.

Set the `VeleroResourcesBackupName` and `VeleroCredentialsBackupName` parameters to `latest`, and the `VeleroManagedClustersBackupName` parameter to `skip`. Immediately after the `VeleroManagedClustersBackupName` is set to `latest`, the managed clusters are activated on the new hub cluster and is now the primary hub cluster. 

When the activted managed cluster is becomes the primary hub cluster, the restore resource is set to `Finished` and the `syncRestoreWithNewBackups` is ignored, even if set to `true`. 

By default, the controler checks for new backups every 30 minutes when the `syncRestoreWithNewBackups` is set to `true`. If new backups are found, it restores the backed up resources. You can change the duration of the check by updating the `restoreSyncInterval` parameter.

For example, the following resource checks for backups every 10 minutes:

[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Restore
metadata:
  name: restore-acm-passive-sync
spec:
  syncRestoreWithNewBackups: true # restore again when new backups are available
  restoreSyncInterval: 10m # check for new backups every 10 minutes
  cleanupBeforeRestore: CleanupRestored 
  veleroManagedClustersBackupName: skip
  veleroCredentialsBackupName: latest
  veleroResourcesBackupName: latest
----

[#disaster-recovery]
== Disaster recovery

When the primary hub cluster goes down, one of the passive hubclusters is chosen by the administrator to take over the managed clusters. In the following image, the administrator decides to use _Hub cluster N_ as the new primary hub cluster:

image:../images/disaster_recovery.png[Disaster recovery diagram] 

_Hub cluster N_ restores the managed cluster activation data. At this point, the managed clusters connect with _Hub cluster N_. The administrator activates a backup on the new primary hub cluster, _Hub cluster 1_, by creating a `BackupSchedule.cluster.open-cluster-management.io` resource, and storing the backups at the same storage location as the initial primary hub cluster.

All other passive hub clusters now restore passive data using the backup data created by the new primary hub cluster. Hub D behaves now as the primary hub, managing clusters and backing up data.
Note:

Step 1 is not automated since the admin should decide if the primary hub is down and needs to be replaced, or there is some network communication error between the hub and managed clusters. The admin also decides which passive hub should become primary. If desired, this step could be automated using the Policy integration with Ansible jobs: the admin can setup an Ansible job to be executed when the Backup Policy reports backup execution errors.
Although Step 2 above is manual, the admin will be notified using the Backups are actively running as a cron job if he omits to start creating backups from the new primary hub.



[#backup-validation-using-a-policy]
== Backup validation using a policy

The cluster backup and restore operator Helm chart (`cluster-backup-chart`) installs the `backup-restore-enabled` policy on your hub cluster, which is used to inform you about issues with the backup and restore component. The `backup-restore-enabled` policy includes a set of templates that check for the following constraints:

- *Pod validation*
+
The following templates check the pod status for the backup component and dependencies:
+
** `acm-backup-pod-running` template checks if the backup and restore operator pod is running.
** `oadp-pod-running` template checks if the OADP operator pod is running. 
** `velero-pod-running` template checks if the Velero pod is running.

- *Backup storage validation*
+
* `backup-storage-location-available` template checks if a `BackupStorageLocation.velero.io` resource is created and if the status value is `Available`. This implies that the connection to the backup storage is valid. 

- *BackupSchedule collision validation*
+
* `acm-backup-clusters-collision-report` template verifies that the status is not `BackupCollision`, if a `BackupSchedule.cluster.open-cluster-management.io` exists on the current hub cluster. This verifies that the current hub cluster is not in collision with any other hub cluster when you write backup data to the storage location.
+
For a definition of the `BackupCollision` state read the https://github.com/stolostron/cluster-backup-operator#backup-collisions[Backup Collisions section].

- *BackupSchedule and restore status validation*
+
* `acm-backup-phase-validation` template checks that the status is not in `Failed`, or `Empty` state, if a `BackupSchedule.cluster.open-cluster-management.io` exists on the current cluster. This ensures that if this cluster is the primary hub cluster and is generating backups, the `BackupSchedule.cluster.open-cluster-management.io` status is healthy.
* The same template checks that the status is not in a `Failed`, or `Empty` state, if a `Restore.cluster.open-cluster-management.io` exists on the current cluster. This ensures that if this cluster is the secondary hub cluster and is restoring backups, the `Restore.cluster.open-cluster-management.io` status is healthy.

- *Backups exist validation*
+
* `acm-managed-clusters-schedule-backups-available` template checks if `Backup.velero.io` resources are available at the location specified by the `BackupStorageLocation.velero.io`, and if the backups are created by a `BackupSchedule.cluster.open-cluster-management.io` resource. This validates that the backups have been run at least once, using the backup and restore operator.

- *Backups are actively running as a cron job*
+
* A `BackupSchedule.cluster.open-cluster-management.io` actively runs and saves new backups at the storage location. This validation is done by the `backup-schedule-cron-enabled` policy template. The template checks that there is a `Backup.velero.io` with `velero.io/schedule-name: acm-validation-policy-schedule` label at the storage location.
+
The `acm-validation-policy-schedule` backups are set to expire after the time is set for the backups cron schedule. If no cron job is running to create backups, the old `acm-validation-policy-schedule` backup is deleted because it expired and a new one is not created. As a result, if no `acm-validation-policy-schedule backups` exist at any moment, it means that there are no active cron jobs generating backups.
+
This policy is intended to help notify the hub cluster administrator of any backup issues when the hub cluster is active and produces or restore backups.


Learn how to enable and manage the cluster backup and restore operator, see xref:../clusters/backup_restore_enable.adoc#backup-restore-enable[Enable the backup and restore operator].


