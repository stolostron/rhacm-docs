[#remove-managed-cluster]
= Removing a cluster from management

When you remove an {ocp-short} cluster from management that was created with {product-title}, you can either _detach_ it or _destroy_ it. Detaching a cluster removes it from management, but does not completely delete it. You can import it again if you want to manage it. This is only an option when the cluster is in a _Ready_ state.

The following procedures remove a cluster from management in either of the following situations:

* You already deleted the cluster and want to remove the deleted cluster from {product-title-short}.
* You want to remove the cluster from management, but have not deleted the cluster.

*Important:* 

- Destroying a cluster removes it from management and deletes the components of the cluster.
- When you detach a managed cluster, the related namespace is automatically deleted. Do not place custom resources in this namespace. 

* <<remove-a-cluster-by-using-the-console,Removing a cluster by using the console>>
* <<remove-a-cluster-by-using-the-cli,Removing a cluster by using the command line>>
* <<removing-a-cluster-from-management-in-special-cases,Removing remaining resources after removing a cluster>>
* <<defragmenting-the-hub-cluster-etcd-database,Defragmenting the etcd database after removing a cluster>>

[#remove-a-cluster-by-using-the-console]
== Removing a cluster by using the console


From the navigation menu, navigate to *Infrastructure* > *Clusters* and select *Destroy cluster* or *Detach cluster* from the options menu beside the cluster that you want to remove from management.
+
*Tip:* You can detach or destroy multiple clusters by selecting the check boxes of the clusters that you want to detach or destroy and selecting *Detach* or *Destroy*.

*Note:* If you attempt to detach the hub cluster while it is managed, which is called a `local-cluster`, check to see if the default setting of `disableHubSelfManagement` is `false`. This setting causes the hub cluster to reimport itself and manage itself when it is detached, and it reconciles the `MultiClusterHub` controller. It might take hours for the hub cluster to complete the detachment process and reimport. 

To reimport the hub cluster without waiting for the processes to finish, you can enter the following command to restart the `multiclusterhub-operator` pod and reimport faster:

----
oc delete po -n open-cluster-management `oc get pod -n open-cluster-management | grep multiclusterhub-operator| cut -d' ' -f1`
----

You can change the value of the hub cluster to not import automatically by changing the `disableHubSelfManagement` value to `true`, as described in link:../install/install_connected.adoc#installing-while-connected-online[Installing while connected online].

[#remove-a-cluster-by-using-the-cli]
== Removing a cluster by using the command line

To detach a managed cluster by using the command line of the hub cluster, run the following command: 

----
oc delete managedcluster $CLUSTER_NAME
----

To delete the managed cluster after detaching, run the following command:

----
oc delete clusterdeployment <CLUSTER_NAME> -n $CLUSTER_NAME
----

*Note:* If you attempt to detach the hub cluster, which is named `local-cluster`, be aware that the default setting of `disableHubSelfManagement` is `false`. This setting causes the hub cluster to reimport itself and manage itself when it is detached and it reconciles the `MultiClusterHub` controller. It might take hours for the hub cluster to complete the detachment process and reimport. If you want to reimport the hub cluster without waiting for the processes to finish, you can enter the following command to restart the `multiclusterhub-operator` pod and reimport faster:

----
oc delete po -n open-cluster-management `oc get pod -n open-cluster-management | grep multiclusterhub-operator| cut -d' ' -f1`
----

You can change the value of the hub cluster to not import automatically by changing the `disableHubSelfManagement` value to `true`, as described in link:../install/install_connected.adoc#installing-while-connected-online[Installing while connected online].

[#removing-a-cluster-from-management-in-special-cases]
== Removing remaining resources after removing a cluster

If there are remaining resources on the managed cluster that you removed, there are additional steps that are required to ensure that you remove all of the remaining components. Situations when these extra steps are required include the following examples:

* The managed cluster was detached before it was completely created, and components like the `klusterlet` remain on the managed cluster. 

* The hub that was managing the cluster was lost or destroyed before detaching the managed cluster, and there is no way to detach the managed cluster from the hub. 

* The managed cluster was not in an online state when it was detached.

If one of these situations apply to your attempted detachment of a managed cluster, there are some resources that cannot be removed from managed cluster. Complete the following steps to detach the managed cluster:

. Make sure you have the `oc` command line interface configured.
. Make sure you have `KUBECONFIG` configured on your managed cluster.
+ 
If you run `oc get ns | grep open-cluster-management-agent`, you should see two namespaces:
+
----
open-cluster-management-agent         Active   10m
open-cluster-management-agent-addon   Active   10m
----

. Run the following command to remove the remaining resources:

+
----
oc delete namespaces open-cluster-management-agent open-cluster-management-agent-addon --wait=false
oc get crds | grep open-cluster-management.io | awk '{print $1}' | xargs oc delete crds --wait=false
oc get crds | grep open-cluster-management.io | awk '{print $1}' | xargs oc patch crds --type=merge -p '{"metadata":{"finalizers": []}}'
----

. Run the following command to ensure that both namespaces and all open cluster management `crds` are removed: 
+
----
oc get crds | grep open-cluster-management.io | awk '{print $1}'
oc get ns | grep open-cluster-management-agent
----

[#defragmenting-the-hub-cluster-etcd-database]
== Defragmenting the etcd database after removing a cluster

Having many managed clusters can affect the size of the `etcd` database in the hub cluster. In {ocp-short} 4.8, when you delete a managed cluster, the `etcd` database in the hub cluster is not automatically reduced in size. In some scenarios, the `etcd` database can run out of space. An error `etcdserver: mvcc: database space exceeded` is displayed. To correct this error, reduce the size of the `etcd` database by compacting the database history and defragmenting the `etcd` database.

*Note:* For {ocp-short} version 4.9 and later, the etcd Operator automatically defragments disks and compacts the `etcd` history. No manual intervention is needed. The following procedure is for {ocp-short} version 4.8 and earlier.

Compact the `etcd` history and defragment the `etcd` database in the hub cluster by completing the following procedure.

[#prereq-defragmenting-the-hub-cluster-etcd-database]
=== Prerequisites

* Install the OpenShift CLI (`oc`).
* Log in as a user with `cluster-admin` privileges.

[#procedure-defragmenting-the-hub-cluster-etcd-database]
=== Procedure

. Compact the `etcd` history.

.. Open a remote shell session to the `etcd` member, for example:
+
[source,terminal]
----
$ oc rsh -n openshift-etcd etcd-control-plane-0.example.com etcdctl endpoint status --cluster -w table
----

.. Run the following command to compact the `etcd` history:
+
[source,terminal]
----
sh-4.4#etcdctl compact $(etcdctl endpoint status --write-out="json" |  egrep -o '"revision":[0-9]*' | egrep -o '[0-9]*' -m1)
----
+
.Example output
+
[source,terminal]
----
$ compacted revision 158774421
----

. Defragment the `etcd` database and clear any `NOSPACE` alarms as outlined in link:https://docs.openshift.com/container-platform/latest/scalability_and_performance/recommended-host-practices.html#etcd-defrag[Defragmenting `etcd` data].
