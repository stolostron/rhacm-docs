[#configuring-hosting-service-cluster-configure-bm]
= Configuring the hosting cluster on bare metal (Technology Preview)

To configure hosted control planes, you need a _hosting_ cluster and a _hosted_ cluster. You can enable a managed cluster to be a hosting cluster by using the `hypershift` add-on to deploy the HyperShift Operator on that cluster. Then, you can start to create the hosted cluster. 

The {mce-short} 2.2 supports only the default `local-cluster` and the hub cluster as the hosting cluster.

Hosted control planes is a Technology Preview feature, so the related components are disabled by default.

You can deploy hosted control planes by configuring a cluster to function as a hosting cluster. The hosting cluster is the {ocp-short} cluster where the control planes are hosted. 

On {product-title-short} 2.7, you can use the managed hub cluster, also known as the `local-cluster`, as the hosting cluster.

*Important:* 

- Run the hub cluster and workers on the same platform for hosted control planes.

- To provision hosted control planes on bare metal, you can use the Agent platform. The Agent platform uses the central infrastructure management service to add worker nodes to a hosted cluster. 

- Each bare metal host must be started with a Discovery Image that the central infrastructure management provides. You can start the hosts manually or through automation by using `Cluster-Baremetal-Operator`. After each host starts, it runs an Agent process to discover the host details and complete the installation. An `Agent` custom resource represents each host.

- When you create a hosted cluster with the Agent platform, HyperShift installs the Agent Cluster API provider in the hosted control plane namespace.

- When you scale up a node pool, a machine is created. The Cluster API provider finds an Agent that is approved, is passing validations, is not currently in use, and meets the requirements that are specified in the node pool specification. You can monitor the installation of an Agent by checking its status and conditions.

- When you scale down a node pool, Agents are unbound from the corresponding cluster. Before you can reuse the clusters, you must restart them by using the Discovery image to update the number of nodes.

* <<hosting-service-cluster-configure-prereq,Prerequisites>>
* <<configure-dns-bm,Configuring DNS>>
* <<additional-resources-config-bm,Additional resources>>

[#hosting-service-cluster-configure-prereq]
== Prerequisites

You must have the following prerequisites to configure a hosting cluster: 

* You need the {mce} 2.2 and later installed on an {ocp-short} cluster. The {mce-short} is automatically installed when you install {product-title-short}. You can also install {mce-short} without {product-title-short} as an Operator from the {ocp-short} OperatorHub.

* You need the {mce-short} must have at least one managed {ocp-short} cluster. The `local-cluster` is automatically imported in {mce-short} 2.2 and later. See xref:../install_upgrade/adv_config_install.adoc#advanced-config-engine[Advanced configuration] for more information about the `local-cluster`. You can check the status of your hub cluster by running the following command:

+
----
oc get managedclusters local-cluster
----

* You need a hosting cluster with at least 3 worker nodes to run the HyperShift Operator.

* You must enable the hosted control planes feature. For more information, see xref:../../clusters/hosted_control_planes/configure_hosted_aws.adoc#hosted-enable-feature-aws[Enabling the hosted control planes feature].

[#configure-dns-bm]
== Configuring DNS

The API Server for the hosted cluster is exposed as a `NodePort` service. A DNS entry must exist for `api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN}` that points to destination where the API Server can be reached.

The DNS entry can be as simple as a record that points to one of the nodes in the managed cluster that is running the hosted control plane. The entry can also point to a load balancer that is deployed to redirect incoming traffic to the Ingress pods. 

See the following example DNS configuration:
----
api.example.krnl.es.    IN A 192.168.122.20
api.example.krnl.es.    IN A 192.168.122.21
api.example.krnl.es.    IN A 192.168.122.22
api-int.example.krnl.es.    IN A 192.168.122.20
api-int.example.krnl.es.    IN A 192.168.122.21
api-int.example.krnl.es.    IN A 192.168.122.22
`*`.apps.example.krnl.es. IN A 192.168.122.23
----

[#additional-resources-config-bm]
== Additional resources

* For an introduction to the central infrastructure management service, see link:https://github.com/openshift/assisted-service/blob/master/docs/hive-integration/kube-api-getting-started.md[Kube API - Getting Started Guide]. 

* See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/installing/installing-on-any-platform#installation-load-balancing-user-infra_installing-platform-agnostic[Load balancing requirements for user-provisioned infrastructure] for more information about load balancers.

* You can now deploy the SR-IOV Operator. See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/networking/hardware-networks#sriov-operator-hosted-control-planes_configuring-sriov-operator[Deploying the SR-IOV Operator for hosted control planes] to learn more about deploying the SR-IOV Operator.
