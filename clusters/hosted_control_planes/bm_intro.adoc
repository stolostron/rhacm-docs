[#configuring-hosting-service-cluster-configure-bm]
= Configuring hosted control plane clusters on bare metal

You can deploy hosted control planes by configuring a cluster to function as a hosting cluster. The hosting cluster is the {ocp-short} cluster where the control planes are hosted. The hosting cluster is also known as the _management_ cluster.

*Note:* The management cluster is not the same thing as the _managed_ cluster. A managed cluster is a cluster that the hub cluster manages.

The hosted control planes feature is enabled by default.

The {mce-short} 2.5 supports only the default `local-cluster`, which is a hub cluster that is managed, and the hub cluster as the hosting cluster. On {product-title-short} 2.10, you can use the managed hub cluster, also known as the `local-cluster`, as the hosting cluster.

A _hosted cluster_ is an {ocp-short} cluster with its API endpoint and control plane that are hosted on the hosting cluster. The hosted cluster includes the control plane and its corresponding data plane. You can use the {mce-short} console or the hosted control plane command line interface, `hcp`, to create a hosted cluster. The hosted cluster is automatically imported as a managed cluster. If you want to disable this automatic import feature, see _Disabling the automatic import of hosted clusters into {mce-short}_.

*Important:*

- Run the hub cluster and workers on the same platform for hosted control planes.

- Each hosted cluster must have a cluster-wide unique name. A hosted cluster name cannot be the same as any existing managed cluster in order for {mce-short} to manage it.

- Do not use `clusters` as a hosted cluster name.

- A hosted cluster cannot be created in the namespace of a {mce-short} managed cluster.

- To provision hosted control planes on bare metal, you can use the Agent platform. The Agent platform uses the central infrastructure management service to add worker nodes to a hosted cluster. For an introduction to the central infrastructure management service, see xref:../cluster_lifecycle/cim_enable.adoc#enable-cim[Enabling the central infrastructure management service].

- All bare metal hosts require a manual boot with a Discovery Image ISO that the central infrastructure management provides. You can start the hosts manually or through automation by using `Cluster-Baremetal-Operator`. After each host starts, it runs an Agent process to discover the host details and complete the installation. An `Agent` custom resource represents each host.

- When you create a hosted cluster with the Agent platform, HyperShift installs the Agent Cluster API provider in the hosted control plane namespace.

- When you scale a replica by the node pool, a machine is created. For every machine, the Cluster API provider finds and installs an Agent that meets the requirements that are specified in the node pool specification. You can monitor the installation of an Agent by checking its status and conditions.

- When you scale down a node pool, Agents are unbound from the corresponding cluster. Before you can reuse the Agents, you must restart them by using the Discovery image.

- When you configure storage for hosted control planes, consider the recommended etcd practices. To ensure that you meet the latency requirements, dedicate a fast storage device to all hosted control plane etcd instances that run on each control-plane node. You can use LVM storage to configure a local storage class for hosted etcd pods. For more information, see _Recommended etcd practices_ and _Persistent storage using logical volume manager storage_ in the {ocp-short} documentation.

[#hosting-service-cluster-configure-prereq]
== Prerequisites

You must have the following prerequisites to configure a hosting cluster:

* You need the {mce} 2.2 and later installed on an {ocp-short} cluster. The {mce-short} is automatically installed when you install {product-title-short}. You can also install {mce-short} without {product-title-short} as an Operator from the {ocp-short} OperatorHub.

* The {mce-short} must have at least one managed {ocp-short} cluster. The `local-cluster` is automatically imported in {mce-short} 2.2 and later. See xref:../install_upgrade/adv_config_install.adoc#advanced-config-engine[Advanced configuration] for more information about the `local-cluster`. You can check the status of your hub cluster by running the following command:
+
----
oc get managedclusters local-cluster
----

* You must add the `topology.kubernetes.io/zone` label to your bare metal hosts on your management cluster. Otherwise, all of the hosted control plane pods are scheduled on a single node, causing single point of failure.

* You need to enable central infrastructure management. For more information, see xref:../cluster_lifecycle/cim_enable.adoc#enable-cim[Enabling the central infrastructure management service].

* You need to xref:../hosted_control_planes/install_hcp_cli.adoc#hosted-install-cli[install the hosted control plane command line interface].

[#firewall-port-reqs-bare-metal]
== Bare metal firewall, port, and service requirements

You must meet the firewall, port, and service requirements so that ports can communicate between the management cluster, the control plane, and hosted clusters.

*Note:* Services run on their default ports. However, if you use the `NodePort` publishing strategy, services run on the port that is assigned by the `NodePort` service.

Use firewall rules, security groups, or other access controls to restrict access to only required sources. Avoid exposing ports publicly unless necessary. For production deployments, use a load balancer to simplify access through a single IP address.

A hosted control plane exposes the following services on bare metal:

* `APIServer`

** The `APIServer` service runs on port 6443 by default and requires ingress access for communication between the control plane components.
** If you use MetalLB load balancing, allow ingress access to the IP range that is used for load balancer IP addresses.

* `OAuthServer`

** The `OAuthServer` service runs on port 443 by default when you use the route and ingress to expose the service.
** If you use the `NodePort` publishing strategy, use a firewall rule for the `OAuthServer` service.

* `Konnectivity`

** The `Konnectivity` service runs on port 443 by default when you use the route and ingress to expose the service.
** The `Konnectivity` agent establishes a reverse tunnel to allow the control plane to access the network for the hosted cluster. The agent uses egress to connect to the `Konnectivity` server. The server is exposed by using either a route on port 443 or a manually assigned `NodePort`.
** If the cluster API server address is an internal IP address, allow access from the workload subnets to the IP address on port 6443.
** If the address is an external IP address, allow egress on port 6443 to that external IP address from the nodes.

* `Ignition`

** The `Ignition` service runs on port 443 by default when you use the route and ingress to expose the service.
** If you use the `NodePort` publishing strategy, use a firewall rule for the `Ignition` service.

You do not need the following services on bare metal:

* `OVNSbDb`
* `OIDC`

[#infrastructure-reqs-bare-metal]
== Bare metal infrastructure requirements

The Agent platform does not create any infrastructure, but it does have the following requirements for infrastructure:

* Agents: An _Agent_ represents a host that is booted with a discovery image and is ready to be provisioned as an {ocp-short} node.

* DNS: The API and ingress endpoints must be routable.

For additional resources about hosted control planes on bare metal, see the following documentation:

* To learn about etcd and LVM storage recommendations, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/scalability_and_performance/recommended-performance-and-scalability-practices#recommended-etcd-practices[Recommended etcd practices] and link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/storage/configuring-persistent-storage#persistent-storage-using-lvms[Persistent storage using logical volume manager storage].

* To configure hosted control planes on bare metal in disconnected environment, see xref:../hosted_control_planes/disconnected_intro.adoc#configure-hosted-disconnected[Configuring hosted control planes in a disconnected environment].

* To disable the hosted control planes feature or, if you already disabled it and want to manually enable it, see link:../hosted_control_planes/enable_or_disable_hosted.adoc#enable-or-disable-hosted-control-planes[Enabling or disabling the hosted control planes feature].

* To manage hosted clusters by running {aap} jobs, see xref:../cluster_lifecycle/ansible_config_hosted_cluster.adoc#ansible-config-hosted-cluster[Configuring {aap-short} jobs to run on hosted clusters].

* To deploy the SR-IOV Operator, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/networking/hardware-networks#sriov-operator-hosted-control-planes_configuring-sriov-operator[Deploying the SR-IOV Operator for hosted control planes].

* If you want to disable the automatic import feature, see xref:../hosted_control_planes/disable_auto_import.adoc#hosted-disable-auto-import[Disabling the automatic import of hosted clusters into {mce-short}].
