[#scaling-the-nodepool]
= Scaling the NodePool object for a hosted cluster

You can scale up the `NodePool` object, by adding nodes to your hosted cluster.

. Scale the `NodePool` object to two nodes:

+
----
oc -n <hosted-cluster-namespace> scale nodepool <nodepool-name> --replicas 2
----

+
The Cluster API agent provider randomly picks two agents that are then assigned to the hosted cluster. Those agents go through different states and finally join the hosted cluster as {ocp-short} nodes. The agents pass through states in the following order:

+
* `binding`
* `discovering`
* `insufficient`
* `installing`
* `installing-in-progress`
* `added-to-existing-cluster`

. Enter the following command:

+
----
oc -n <hosted-control-plane-namespace> get agent
----

+
See the following example output:

+
----
NAME                                   CLUSTER         APPROVED   ROLE          STAGE
4dac1ab2-7dd5-4894-a220-6a3473b67ee6   hypercluster1   true       auto-assign
d9198891-39f4-4930-a679-65fb142b108b                   true       auto-assign
da503cf1-a347-44f2-875c-4960ddb04091   hypercluster1   true       auto-assign
----

. Enter the following command:

+
----
oc -n <hosted-control-plane-namespace> get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\.openshift\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{"\n"}{end}'
----

+
See the following example output:

+
----
BMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: binding
BMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: known-unbound
BMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: insufficient
----

. Obtain the kubeconfig for your new hosted cluster by entering the extract command:

+
----
oc extract -n <hosted-cluster-namespace> secret/<hosted-cluster-name>-admin-kubeconfig --to=- > kubeconfig-<hosted-cluster-name>
----

. After the agents reach the `added-to-existing-cluster` state, verify that you can see the {ocp-short} nodes in the hosted cluster by entering the following command:

+
----
oc --kubeconfig kubeconfig-<hosted-cluster-name> get nodes
----

+
See the following example output:

+
----
NAME           STATUS   ROLES    AGE     VERSION
ocp-worker-1   Ready    worker   5m41s   v1.24.0+3882f8f
ocp-worker-2   Ready    worker   6m3s    v1.24.0+3882f8f
----

+
Cluster Operators start to reconcile by adding workloads to the nodes.

. Enter the following command to verify that two machines were created when you scaled up the `NodePool` object:

+
----
oc -n <hosted-control-plane-namespace> get machines
----

+
See the following example output:

+
----
NAME                            CLUSTER               NODENAME       PROVIDERID                                     PHASE     AGE   VERSION
hypercluster1-c96b6f675-m5vch   hypercluster1-b2qhl   ocp-worker-1   agent://da503cf1-a347-44f2-875c-4960ddb04091   Running   15m   4.13z
hypercluster1-c96b6f675-tl42p   hypercluster1-b2qhl   ocp-worker-2   agent://4dac1ab2-7dd5-4894-a220-6a3473b67ee6   Running   15m   4.13z
----

+
The `clusterversion` reconcile process eventually reaches a point where only Ingress and Console cluster operators are missing.

. Enter the following command:

+
----
oc --kubeconfig kubeconfig-<hosted-cluster-name> get clusterversion,co
----

+
See the following example output:

+
----
NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version             False       True          40m     Unable to apply 4.13z: the cluster operator console has not yet successfully rolled out

NAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/console                                    4.12z    False       False         False      11m     RouteHealthAvailable: failed to GET route (https://console-openshift-console.apps.hypercluster1.domain.com): Get "https://console-openshift-console.apps.hypercluster1.domain.com": dial tcp 10.19.3.29:443: connect: connection refused
clusteroperator.config.openshift.io/csi-snapshot-controller                    4.12z    True        False         False      10m
clusteroperator.config.openshift.io/dns                                        4.12z    True        False         False      9m16s
----

[#adding-nodepool-bm]
== Adding node pools

You can create node pools for a hosted cluster by specifying a name, number of replicas, and any additional information, such as an agent label selector.

. To create a node pool, enter the following information:

+
----
export NODEPOOL_NAME=${CLUSTER_NAME}-extra-cpu
export WORKER_COUNT="2"

hcp create nodepool agent \
  --cluster-name $CLUSTER_NAME \
  --name $NODEPOOL_NAME \
  --node-count $WORKER_COUNT \
  --agentLabelSelector '{"matchLabels": {"size": "medium"}}' <1>
----

+
<1> The `--agentLabelSelector` is optional. The node pool uses agents with the `"size" : "medium"` label.

. Check the status of the node pool by listing `nodepool` resources in the `clusters` namespace:

+
----
oc get nodepools --namespace clusters
----

. Extract the `admin-kubeconfig` secret by entering the following command:

+
----
oc extract -n <hosted-control-plane-namespace> secret/admin-kubeconfig --to=./hostedcluster-secrets --confirm
----

+
See the following example output:

+
----
hostedcluster-secrets/kubeconfig
----

. After some time, you can check the status of the node pool by entering the following command:

+
----
oc --kubeconfig ./hostedcluster-secrets get nodes
----

. Verify that the number of available node pools match the number of expected node pools by entering this command:

+
----
oc get nodepools --namespace clusters
----

[#scale-nodepool-bm-additional-resources]
== Additional resources

* To scale down the data plane to zero, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.15/html/hosted_control_planes/troubleshooting-hosted-control-planes#scale-down-data-plane_hcp-troubleshooting[Scaling down the data plane to zero].
