[#scaling-the-nodepool]
= Scaling the NodePool object for a hosted cluster

You add nodes to your hosted cluster by scaling the `NodePool` object.  

. Scale the `NodePool` object to two nodes:

+
----
oc -n <cluster-namespace> scale nodepool <nodepool-name> --replicas 2
----

+
The Cluster API agent provider randomly picks two agents that are then assigned to the hosted cluster. Those agents go through different states and finally join the hosted cluster as {ocp-short} nodes. The agents pass through states in the following order:

+ 
* `binding`
* `discovering`
* `insufficient`
* `installing`
* `installing-in-progress`
* `added-to-existing-cluster`

. Enter the following command:

+
----
oc -n <hosted-control-plane-namespace> get agent
----

+
See the following example output:

+
----
NAME                                   CLUSTER         APPROVED   ROLE          STAGE
4dac1ab2-7dd5-4894-a220-6a3473b67ee6   hypercluster1   true       auto-assign   
d9198891-39f4-4930-a679-65fb142b108b                   true       auto-assign   
da503cf1-a347-44f2-875c-4960ddb04091   hypercluster1   true       auto-assign
----

. Enter the following command:

+
----
oc -n <hosted-control-plane-namespace> get agent -o jsonpath='{range .items[*]}BMH: {@.metadata.labels.agent-install\.openshift\.io/bmh} Agent: {@.metadata.name} State: {@.status.debugInfo.state}{"\n"}{end}'
----

+
See the following example output:

+
----
BMH: ocp-worker-2 Agent: 4dac1ab2-7dd5-4894-a220-6a3473b67ee6 State: binding
BMH: ocp-worker-0 Agent: d9198891-39f4-4930-a679-65fb142b108b State: known-unbound
BMH: ocp-worker-1 Agent: da503cf1-a347-44f2-875c-4960ddb04091 State: insufficient
----

. After the agents reach the `added-to-existing-cluster` state, verify that you can see the {ocp-short} nodes by entering the following command:

+
----
oc --kubeconfig <hosted-cluster-name>.kubeconfig get nodes
----

+
See the following example output:

+
----
NAME           STATUS   ROLES    AGE     VERSION
ocp-worker-1   Ready    worker   5m41s   v1.24.0+3882f8f
ocp-worker-2   Ready    worker   6m3s    v1.24.0+3882f8f
----

+
Cluster Operators start to reconcile by adding workloads to the nodes. 

. Enter the following command to verify that two machines were created when you scaled up the `NodePool` object:

+
----
oc -n <hosted-control-plane-namespace> get machines
----

+
See the following example output:

+
----
NAME                            CLUSTER               NODENAME       PROVIDERID                                     PHASE     AGE   VERSION
hypercluster1-c96b6f675-m5vch   hypercluster1-b2qhl   ocp-worker-1   agent://da503cf1-a347-44f2-875c-4960ddb04091   Running   15m   4.12z
hypercluster1-c96b6f675-tl42p   hypercluster1-b2qhl   ocp-worker-2   agent://4dac1ab2-7dd5-4894-a220-6a3473b67ee6   Running   15m   4.12z
----

+
The `clusterversion` reconcile process eventually reaches a point where only Ingress and Console cluster operators are missing.

. Enter the following command:

+
----
oc --kubeconfig <hosted-cluster-name>.kubeconfig get clusterversion,co
----

+
See the following example output:

+
----
NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version             False       True          40m     Unable to apply 4.12z: the cluster operator console has not yet successfully rolled out

NAME                                                                           VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/console                                    4.12z    False       False         False      11m     RouteHealthAvailable: failed to GET route (https://console-openshift-console.apps.hypercluster1.domain.com): Get "https://console-openshift-console.apps.hypercluster1.domain.com": dial tcp 10.19.3.29:443: connect: connection refused
clusteroperator.config.openshift.io/csi-snapshot-controller                    4.12z    True        False         False      10m     
clusteroperator.config.openshift.io/dns                                        4.12z    True        False         False      9m16s   
clusteroperator.config.openshift.io/image-registry                             4.12z    True        False         False      9m5s    
clusteroperator.config.openshift.io/ingress                                    4.12z    True        False         True       39m     The "default" ingress controller reports Degraded=True: DegradedConditions: One or more other status conditions indicate a degraded state: CanaryChecksSucceeding=False (CanaryChecksRepetitiveFailures: Canary route checks for the default ingress controller are failing)
clusteroperator.config.openshift.io/insights                                   4.12z    True        False         False      11m     
clusteroperator.config.openshift.io/kube-apiserver                             4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/kube-controller-manager                    4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/kube-scheduler                             4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/kube-storage-version-migrator              4.12z    True        False         False      10m     
clusteroperator.config.openshift.io/monitoring                                 4.12z    True        False         False      7m38s   
clusteroperator.config.openshift.io/network                                    4.12z    True        False         False      11m     
clusteroperator.config.openshift.io/openshift-apiserver                        4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/openshift-controller-manager               4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/openshift-samples                          4.12z    True        False         False      8m54s   
clusteroperator.config.openshift.io/operator-lifecycle-manager                 4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/operator-lifecycle-manager-catalog         4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/operator-lifecycle-manager-packageserver   4.12z    True        False         False      40m     
clusteroperator.config.openshift.io/service-ca                                 4.12z    True        False         False      11m     
clusteroperator.config.openshift.io/storage                                    4.12z    True        False         False      11m 
----

[#adding-nodepool-bm]
== Adding node pools

You can create node pools for a hosted cluster by specifying a name, number of replicas, and any additional information, such as an agent label selector.

. To create a node pool, enter the following information. In this example, the node pool uses agents with the `"size" : "medium"` label:

+
----
export NODEPOOL_NAME=${CLUSTER_NAME}-extra-cpu
export WORKER_COUNT="2"

hcp create nodepool agent \
  --cluster-name $CLUSTER_NAME \
  --name $NODEPOOL_NAME \
  --node-count $WORKER_COUNT \
  --agentLabelSelector '{"matchLabels": {"size": "medium"}}'
----

. Check the status of the node pool by listing `nodepool` resources in the `clusters` namespace:

+
----
oc get nodepools --namespace clusters
----

. After some time, you can check the status of the node pool by entering the following command:

+
----
oc --kubeconfig $CLUSTER_NAME-kubeconfig get nodes
----

. Verify that the node pool is in the status that you expect by entering this command:

+
----
oc get nodepools --namespace clusters
----
