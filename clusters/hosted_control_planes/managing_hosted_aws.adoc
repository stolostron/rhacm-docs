[#hosted-control-planes-manage-aws]
= Managing hosted control plane clusters on AWS (Technology Preview)

You can use the {mce} console to create a {ocp} hosted cluster. Hosted control planes are available as a Technology Preview on Amazon Web Services (AWS). If you use hosted control planes on AWS, you can create a hosted cluster by using the console, or you can import a hosted cluster by using either the console or the command line interface.

* <<hosted-prerequisites-aws,Prerequisites>>
* <<create-hosted-aws,Creating a hosted cluster on AWS with the console>>
* <<create-hosted-multi-zone-aws,Creating a hosted cluster in multiple zones on AWS>>
* <<hosted-deploy-cluster-aws,Deploying a hosted cluster on AWS>>
* <<hosting-service-cluster-access-aws,Accessing a hosting cluster>>
* <<importing-hosted-cluster-aws,Importing a hosted control plane cluster on AWS>>
* <<hosted-cluster-arm-aws,Enabling hosted control planes on an ARM64 {ocp-short} cluster>>
* <<hypershift-cluster-destroy-aws,Destroying a hosted cluster on AWS>>

[#hosted-prerequisites-aws]
== Prerequisites

You must configure hosted control planes before you can create hosted control plane clusters. See xref:../../clusters/hosted_control_planes/configure_hosted_aws.adoc#hosting-service-cluster-configure-aws[Configuring hosted control planes (Technology Preview)] for more information.

[#create-hosted-aws]
== Creating a hosted control plane cluster on AWS with the console

To create a hosted control plane cluster from the {mce-short} console, navigate to *Infrastructure* > *Clusters*. On the _Clusters_ page, click *Create cluster* > *Amazon Web Services* > *Hosted* and complete the steps in the console. 

*Important:* When you create a cluster, the {mce-short} controller creates a namespace for the cluster and its resources. Ensure that you include only resources for that cluster instance in that namespace. Destroying the cluster deletes the namespace and all of the resources in it.

If you want to add your cluster to an existing cluster set, you must have the correct permissions on the cluster set to add it. If you do not have `cluster-admin` privileges when you are creating the cluster, you must select a cluster set on which you have `clusterset-admin` permissions. If you do not have the correct permissions on the specified cluster set, the cluster creation fails. Contact your cluster administrator to provide you with `clusterset-admin` permissions if you do not have any cluster set options to select.

Every managed cluster must be associated with a managed cluster set. If you do not assign the managed cluster to a `ManagedClusterSet`, it is automatically added to the `default` managed cluster set.

The release image identifies the version of the {ocp-short} image that is used to create the cluster. Hosted control plane clusters must use one of the provided release images.

Proxy information that is provided in the infrastructure environment is automatically added to the proxy fields. You can use the existing information, overwrite it, or add the information if you want to enable a proxy. The following list contains the required information for creating a proxy: 

* HTTP proxy URL: The URL that should be used as a proxy for `HTTP` traffic. 

* HTTPS proxy URL: The secure proxy URL that should be used for `HTTPS` traffic. If no value is provided, the same value as the `HTTP Proxy URL` is used for both `HTTP` and `HTTPS`.

* No proxy domains: A comma-separated list of domains that should bypass the proxy. Begin a domain name with a period `.` to include all of the subdomains that are in that domain. Add an asterisk `*` to bypass the proxy for all destinations. 

* Additional trust bundle: The contents of the certificate file that is required to access the mirror registry.
  
*Note:* You have to run the `oc` command that is provided with the cluster details to import the cluster. When you create the cluster, it is not automatically configured with the management of {product-title-short}.

[#create-hosted-multi-zone-aws]
== Creating a hosted cluster in multiple zones on AWS

Create a cluster, specifying the `BASE_DOMAIN` of the public zone, by entering the following commands:


----
REGION=us-east-1
ZONES=us-east-1a,us-east-1b
CLUSTER_NAME=example
BASE_DOMAIN=example.com
AWS_CREDS="$HOME/.aws/credentials"
PULL_SECRET="$HOME/pull-secret"

hypershift create cluster aws \
--name $CLUSTER_NAME \
--node-pool-replicas=3 \
--base-domain $BASE_DOMAIN \
--pull-secret $PULL_SECRET \
--aws-creds $AWS_CREDS \
--region $REGION \
--zones $ZONES <1>
----

<1> The `--zones` flag must specify availability zones within the region that is specified by the `--region` flag. The `--zones` flag is also available on the `hypershift create infra aws` command that is used to create infrastructure separately.

The following per-zone infrastructure is created for all specified zones:

* Public subnet
* Private subnet
* NAT gateway
* Private route table (public route table is shared across public subnets)

One `NodePool` resource is created for each zone. The node pool name is suffixed by the zone name. The private subnet for zone is set in `spec.platform.aws.subnet.id`.

[#hosted-deploy-cluster-aws]
== Deploying a hosted cluster on AWS

After you set up the hosted control planes (`hypershift`) command line interface and enable the `local-cluster` as the hosting cluster, you can deploy a hosted cluster on AWS by completing the following steps. To deploy a private hosted cluster, see _Deploying a private hosted cluster on AWS_.

. Set environment variables as follows, replacing variables as needed with your credentials:

+
----
export REGION=us-east-1
export CLUSTER_NAME=clc-name-hs1
export INFRA_ID=clc-name-hs1
export BASE_DOMAIN=dev09.red-chesterfield.com
export AWS_CREDS=$HOME/name-aws
export PULL_SECRET=/Users/username/pull-secret.txt
export BUCKET_NAME=acmqe-hypershift
export BUCKET_REGION=us-east-1
----

+
. Verify that `CLUSTER_NAME` and `INFRA_ID` have the same values, otherwise the cluster might not appear correctly in the {mce} console. To see descriptions for each variable, run the following command

+
----
hypershift create cluster aws --help
----

. Verify that you are logged into your hub cluster.

. Run the following command to create the hosted cluster:

+
----
hypershift create cluster aws \
    --name $CLUSTER_NAME \
    --infra-id $INFRA_ID \
    --base-domain $BASE_DOMAIN \
    --aws-creds $AWS_CREDS \
    --pull-secret $PULL_SECRET \
    --region $REGION \
    --generate-ssh \
    --node-pool-replicas 3 \
    --namespace <hypershift-hosting-service-cluster>
----

+
*Note:* By default, all `HostedCluster` and `NodePool` custom resources are created in the `clusters` namespace. If you specify the `--namespace <namespace>` parameter, `HostedCluster` and `NodePool` custom resources are created in the namespace you chose.

. You can check the status of your hosted cluster by running the following command:

+
----
oc get hostedclusters -n <hypershift-hosting-service-cluster>
----

. You can check your node pools by running the following command:

+
----
oc get nodepools --namespace clusters
----

[#hosting-service-cluster-access-aws]
== Accessing the hosted cluster

You can access the hosted cluster by either getting the `kubeconfig` file and `kubeadmin` credential directly from resources or by using the `hypershift` CLI to generate a `kubeconfig` file.

* To access the hosted cluster by getting the `kubeconfig` file and credentials directly from resources, you need to be familiar with the access secrets for hosted control plane clusters. The secrets are stored in the hosted cluster (hosting) namespace. The _hosted cluster (hosting)_ namespace contains hosted cluster resources, and the _hosted control plane_ namespace is where the hosted control plane runs.
+
The secret name formats are as follows:

** `kubeconfig` secret: `<hostingNamespace>-<name>-admin-kubeconfig` (clusters-hypershift-demo-admin-kubeconfig)
** `kubeadmin` password secret: `<hostingNamespace>-<name>-kubeadmin-password` (clusters-hypershift-demo-kubeadmin-password)
+
The `kubeconfig` secret contains a Base64-encoded `kubeconfig` field, which you can decode and save into a file to use with the following command:

+
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes
----

+
The `kubeadmin` password secret is also Base64-encoded. You can decode it and use the password to log into the API server or console of the hosted cluster.

* To access the hosted cluster by using the `hypershift` CLI to generate the `kubeconfig` file, take the following steps:

. Generate the `kubeconfig` file by entering the following command:

+
----
hypershift create kubeconfig --namespace ${CLUSTERS_NAMESPACE} --name ${HOSTED_CLUSTER_NAME} > ${HOSTED_CLUSTER_NAME}.kubeconfig
----

+
. After you save the `kubeconfig` file, you can access the hosted cluster by entering the following example command:
+
----
oc --kubeconfig ${HOSTED_CLUSTER_NAME}.kubeconfig get nodes
----


[#importing-hosted-cluster-aws]
== Importing a hosted control plane cluster on AWS

You can import a hosted control plane cluster with the console.

. Click *Infrastructure* > *Clusters* and select the hosted cluster that you want to import.

. Click *Import hosted cluster*.

+
*Note:* For your _discovered_ hosted cluster, you can also import from the console, but the cluster must be in an upgradable state. Import on your cluster is disabled if the hosted cluster is not in an upgradable state because the hosted control plane is not available. Click *Import* to begin the process. The status is `Importing` while the cluster receives updates and then changes to `Ready`.

You can also import a hosted control plane cluster on AWS with the command line interface by completing the following steps:

. Add an annotation to the `HostedCluster` custom resource by running the following command:

+
----
oc edit hostedcluster <cluster_name> -n clusters
----

+
Replace `<cluster_name>` with the name of your hosted cluster.

. Run the following command to add the annotations to the `HostedCluster` custom resource:

+
----
cluster.open-cluster-management.io/hypershiftdeployment: local-cluster/<cluster_name>
cluster.open-cluster-management.io/managedcluster-name: <cluster_name>
----

+
Replace `<cluster_name>` with the name of your hosted cluster.

. Create your `ManagedCluster` resource by using the following sample YAML file:

+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:  
  annotations:    
    import.open-cluster-management.io/hosting-cluster-name: local-cluster    
    import.open-cluster-management.io/klusterlet-deploy-mode: Hosted
    open-cluster-management/created-via: other  
  labels:    
    cloud: auto-detect    
    cluster.open-cluster-management.io/clusterset: default    
    name: <cluster_name>  
    vendor: OpenShift  
  name: <cluster_name>
spec:  
  hubAcceptsClient: true  
  leaseDurationSeconds: 60
----

+
Replace `<cluster_name>` with the name of your hosted cluster.

. Run the following command to apply the resource:

+
----
oc apply -f <file_name>
----

+
Replace <file_name> with the YAML file name you created in the previous step.

. Create your `KlusterletAddonConfig` resource by using the following sample YAML file. This only applies to {product-title-short}. If you have installed {mce-short} only, skip this step:

+
[source,yaml]
----
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: <cluster_name>
  namespace: <cluster_name>
spec:
  clusterName: <cluster_name>
  clusterNamespace: <cluster_name>
  clusterLabels:
    cloud: auto-detect
    vendor: auto-detect
  applicationManager:
    enabled: true
  certPolicyController:
    enabled: true
  iamPolicyController:
    enabled: true
  policyController:
    enabled: true
  searchCollector:
    enabled: false
----

+
Replace `<cluster_name>` with the name of your hosted cluster.

. Run the following command to apply the resource:

+
----
oc apply -f <file_name>
----

+
Replace <file_name> with the YAML file name you created in the previous step.

. After the import process is complete, your hosted cluster becomes visible in the console. You can also check the status of your hosted cluster by running the following command:

+
----
oc get managedcluster <cluster_name>
----

[#hosted-cluster-arm-aws]
== Enabling hosted control planes on an ARM64 {ocp-short} cluster

You can enable an ARM64-hosted control plane to operate with an {ocp-short} ARM64 data plane in a management cluster environment. This feature is available for hosted control planes on AWS only.

[#prerequisites-hosted-arm]
=== Prerequisites

Before you begin, you must meet the following prerequisites:

* You must have an {ocp-short} cluster that was installed on a 64-bit ARM infrastructure. For more information, see link:https://console.redhat.com/openshift/install/aws/arm[Create an OpenShift Cluster: AWS (ARM)].
* You must have a HyperShift Operator that is built on a 64-bit ARM infrastructure. You can obtain a HyperShift Operator by going to the link:https://quay.io/repository/hypershift/hypershift-operator[hypershift/hypershift-operator repository] and selecting the build that has the `4.13-arm64` tag. 

To run a hosted cluster on an ARM64 {ocp-short} cluster, take the following steps:

. Install the HyperShift Operator for ARM64 on the management cluster to override the default HyperShift Operator image.
+
For example, through the hosted control planes (`hypershift`) command line interface, enter the following commands, being careful to replace the bucket name, AWS credentials, and region with your information:

+
----
hypershift install \
--oidc-storage-provider-s3-bucket-name $BUCKET_NAME \
--oidc-storage-provider-s3-credentials $AWS_CREDS \
--oidc-storage-provider-s3-region $REGION \
--hypershift-image quay.io/hypershift/hypershift-operator:4.13-arm64
----

. Create a hosted cluster that overrides the default release image with a multi-architecture release image.
+
For example, through the hosted control planes (`hypershift`) command line interface, enter the following commands, being careful to replace the cluster name, node pool replicas, base domain, pull secret, AWS credentials, and region with your information:

+
----
hypershift create cluster aws \ 
--name $CLUSTER_NAME \
--node-pool-replicas=$NODEPOOL_REPLICAS \
--base-domain $BASE_DOMAIN \
--pull-secret $PULL_SECRET \
--aws-creds $AWS_CREDS \
--region $REGION \
--release-image quay.io/openshift-release-dev/ocp-release:4.13.0-rc.0-multi
----

+
This example adds a default `NodePool` object through the `--node-pool-replicas` flag.

. Add a 64-bit x86 `NodePool` object to the hosted cluster.
+
For example, through the hosted control planes (`hypershift`) command line interface, enter the following commands, being careful to replace the cluster name, node pool name, and node pool replicas with your information:

+
----
hypershift create nodepool aws \
--cluster-name $CLUSTER_NAME \
--name $NODEPOOL_NAME \
--node-count=$NODEPOOL_REPLICAS
----

[#hypershift-cluster-destroy-aws]
== Destroying a hosted cluster on AWS

To destroy a hosted cluster and its managed cluster resource, complete the following steps:

. Delete the hosted cluster and its back-end resources by running the following command:

+
----
hypershift destroy cluster aws --name <cluster_name> --infra-id <infra_id> --aws-creds <aws-credentials> --base-domain <base_domain>
----

+
Replace names where necessary.

. Delete the managed cluster resource on {mce-short} by running the following command:

+
----
oc delete managedcluster <cluster_name>
----

+
Replace `cluster_name` with the name of your cluster.