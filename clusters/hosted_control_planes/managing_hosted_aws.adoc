[#hosted-control-planes-manage-aws]
= Managing hosted control plane clusters on AWS (Technology Preview)

You can use the {mce} console to create a {ocp} hosted cluster. Hosted control planes are available as a Technology Preview on Amazon Web Services (AWS). If you use hosted control planes on AWS, you can create a hosted cluster by using the console, or you can import a hosted cluster by using either the console or the command line interface.

* <<hosted-prerequisites-aws,Prerequisites>>
* <<create-hosted-aws,Creating a hosted cluster on AWS with the console>>
* <<importing-hosted-cluster-aws,Importing a hosted control plane cluster on AWS>>
* <<hosting-service-cluster-access-aws,Accessing a hosting cluster on AWS>>
* <<hosting-cluster-aws-permissions,AWS infrastructure requirements and IAM permissions>>
* <<hosted-cluster-arm-aws,Enabling hosted control planes on an ARM64 {ocp-short} cluster>>
* <<hypershift-cluster-destroy-aws,Destroying a hosted cluster on AWS>>

[#hosted-prerequisites-aws]
== Prerequisites

You must configure hosted control planes before you can create hosted control plane clusters. See xref:../../clusters/hosted_control_planes/configure_hosted_aws.adoc#hosting-service-cluster-configure-aws[Configuring hosted control planes (Technology Preview)] for more information.

[#create-hosted-aws]
== Creating a hosted control plane cluster on AWS with the console

To create a hosted control plane cluster from the {mce-short} console, navigate to *Infrastructure* > *Clusters*. On the _Clusters_ page, click *Create cluster* > *Host inventory* > *Hosted* and complete the steps in the console. 

*Important:* When you create a cluster, the {mce-short} controller creates a namespace for the cluster and its resources. Ensure that you include only resources for that cluster instance in that namespace. Destroying the cluster deletes the namespace and all of the resources in it.

Select *YAML: On* to view content updates as you enter the information in the console.

If you want to add your cluster to an existing cluster set, you must have the correct permissions on the cluster set to add it. If you do not have `cluster-admin` privileges when you are creating the cluster, you must select a cluster set on which you have `clusterset-admin` permissions. If you do not have the correct permissions on the specified cluster set, the cluster creation fails. Contact your cluster administrator to provide you with `clusterset-admin` permissions if you do not have any cluster set options to select.

Every managed cluster must be associated with a managed cluster set. If you do not assign the managed cluster to a `ManagedClusterSet`, it is automatically added to the `default` managed cluster set.

The release image identifies the version of the {ocp-short} image that is used to create the cluster. Hosted control plane clusters must use one of the provided release images.

Proxy information that is provided in the infrastructure environment is automatically added to the proxy fields. You can use the existing information, overwrite it, or add the information if you want to enable a proxy. The following list contains the required information for creating a proxy: 

* HTTP proxy URL: The URL that should be used as a proxy for `HTTP` traffic. 

* HTTPS proxy URL: The secure proxy URL that should be used for `HTTPS` traffic. If no value is provided, the same value as the `HTTP Proxy URL` is used for both `HTTP` and `HTTPS`.

* No proxy domains: A comma-separated list of domains that should bypass the proxy. Begin a domain name with a period `.` to include all of the subdomains that are in that domain. Add an asterisk `*` to bypass the proxy for all destinations. 

* Additional trust bundle: The contents of the certificate file that is required to access the mirror registry.
  
When you review your information and optionally customize it before creating the cluster, you can select *YAML: On* to view the YAML content in the panel. You can edit the YAML file with your custom settings, if you have any updates.  

*Note:* You have to run the `oc` command that is provided with the cluster details to import the cluster. When you create the cluster, it is not automatically configured with the management of {product-title-short}.

[#hosted-deploy-cluster-aws]
== Deploying a hosted cluster on AWS

After setting up the hosted control planes (`hypershift`) command line interface and enabling the `local-cluster` as the hosting cluster, you can deploy a hosted cluster on AWS by completing the following steps:

. Set environment variables as follows, replacing variables as needed with your credentials:
+
----
export REGION=us-east-1
export CLUSTER_NAME=clc-name-hs1
export INFRA_ID=clc-name-hs1
export BASE_DOMAIN=dev09.red-chesterfield.com
export AWS_CREDS=$HOME/name-aws
export PULL_SECRET=/Users/username/pull-secret.txt
export BUCKET_NAME=acmqe-hypershift
export BUCKET_REGION=us-east-1
----
+
. Verify that `CLUSTER_NAME` and `INFRA_ID` have the same values, otherwise the cluster might not appear correctly in the {mce} console. To see descriptions for each variable, run the following command
+
----
hypershift create cluster aws --help
----

. Verify that you are logged into your hub cluster.

. Run the following command to create the hosted cluster:
+
----
hypershift create cluster aws \
    --name $CLUSTER_NAME \
    --infra-id $INFRA_ID \
    --aws-creds $AWS_CREDS \
    --pull-secret $PULL_SECRET \
    --region $REGION \
    --generate-ssh \
    --node-pool-replicas 3 \
    --namespace <hypershift-hosting-service-cluster>
----
+
*Note:* By default, all `HostedCluster` and `NodePool` custom resources are created in the `clusters` namespace. If you specify the `--namespace <namespace>` parameter, `HostedCluster` and `NodePool` custom resources are created in the namespace you chose.

. You can check the status of your hosted cluster by running the following command:
+
----
oc get hostedclusters -n <hypershift-hosting-service-cluster>
----

[#importing-hosted-cluster-aws]
== Importing a hosted control plane cluster on AWS

You can import a hosted control plane cluster with the console.

. Navigate to *Infrastructure* > *Clusters* and select the hosted cluster that you want to import.

. Click *Import hosted cluster*.

+
*Note:* For your _discovered_ hosted cluster, you can also import from the console, but the cluster must be in an upgradable state. Import on your cluster is disabled if the hosted cluster is not in an upgradable state because the hosted control plane is not available. Click *Import* to begin the process. The status is `Importing` while the cluster receives updates and then changes to `Ready`.

You can also import a hosted control plane cluster on AWS with the command line interface by completing the following steps:

. Add an annotation to the `HostedCluster` custom resource by running the following command:
+
----
oc edit hostedcluster <cluster_name> -n clusters
----
+
Replace `<cluster_name>` with the name of your hosted cluster.

. Run the following command to add the annotations to the `HostedCluster` custom resource:
+
----
cluster.open-cluster-management.io/hypershiftdeployment: local-cluster/<cluster_name>
cluster.open-cluster-management.io/managedcluster-name: <cluster_name>
----
+
Replace `<cluster_name>` with the name of your hosted cluster.

. Create your `ManagedCluster` resource by using the following sample YAML file:
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:  
  annotations:    
    import.open-cluster-management.io/hosting-cluster-name: local-cluster    
    import.open-cluster-management.io/klusterlet-deploy-mode: Hosted
    open-cluster-management/created-via: other  
  labels:    
    cloud: auto-detect    
    cluster.open-cluster-management.io/clusterset: default    
    name: <cluster_name>  
    vendor: OpenShift  
  name: <cluster_name>
spec:  
  hubAcceptsClient: true  
  leaseDurationSeconds: 60
----
+
Replace `<cluster_name>` with the name of your hosted cluster.

. Run the following command to apply the resource:
+
----
oc apply -f <file_name>
----
+
Replace <file_name> with the YAML file name you created in the previous step.

. Create your `KlusterletAddonConfig` resource by using the following sample YAML file. This only applies to {product-title-short}. If you have installed {mce-short} only, skip this step:
+
[source,yaml]
----
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: <cluster_name>
  namespace: <cluster_name>
spec:
  clusterName: <cluster_name>
  clusterNamespace: <cluster_name>
  clusterLabels:
    cloud: auto-detect
    vendor: auto-detect
  applicationManager:
    enabled: true
  certPolicyController:
    enabled: true
  iamPolicyController:
    enabled: true
  policyController:
    enabled: true
  searchCollector:
    enabled: false
----
+
Replace `<cluster_name>` with the name of your hosted cluster.

. Run the following command to apply the resource:
+
----
oc apply -f <file_name>
----
+
Replace <file_name> with the YAML file name you created in the previous step.

. After the import process is complete, your hosted cluster becomes visible in the console. You can also check the status of your hosted cluster by running the following command:
+
----
oc get managedcluster <cluster_name>
----

[#hosting-service-cluster-access-aws]
== Accessing a hosting cluster on AWS

The access secrets for hosted control plane clusters are stored in the `hypershift-management-cluster` namespace. Learn about the following secret name formats:

- `kubeconfig` secret: `<hostingNamespace>-<name>-admin-kubeconfig` (clusters-hypershift-demo-admin-kubeconfig)
- `kubeadmin` password secret: `<hostingNamespace>-<name>-kubeadmin-password` (clusters-hypershift-demo-kubeadmin-password)

[#hosting-cluster-aws-permissions]
== AWS infrastructure requirements and IAM permissions

When you use hosted control planes on AWS, the infrastructure requirements fit in the following categories:

* Prerequired and unmanaged infrastructure for the HyperShift Operator in an arbitrary AWS account
* Prerequired and unmanaged infrastructure in a hosted cluster AWS account
* Hosted control planes-managed infrastructure in a management AWS account
* Hosted control planes-managed infrastructure in a hosted cluster AWS account
* Kubernetes-managed infrastructure in a hosted cluster AWS account

[#infra-prerequired-unmanaged-for-ho]
=== Prerequired and unmanaged infrastructure for the HyperShift Operator in an arbitrary AWS account

An arbitrary AWS account depends on the provider of the hosted control planes service. 

In self-managed hosted control planes, the cluster service provider controls the AWS account. The _cluster service provider_ is the administrator who hosts cluster control planes and is responsible for uptime.

In managed hosted control planes, the AWS account belongs to Red&nbsp;Hat.
// Do we need to compare self-managed and managed? I'm thinking that customers who will access this documentation will be using self-managed and won't need to worry about comparing managed vs. self-managed.

When your infrastructure is prerequired and unmanaged for the HyperShift Operator in an AWS account, the following infrastructure is required for a management cluster AWS account:
//This sentence is hard to understand. I would love to simplify it, if possible, but I'll need some help.

* One S3 Bucket
** OpenID Connect (OIDC)
* Route 53 hosted zones
** A domain to host private and public entries for hosted clusters

[#infra-prerequired-unmanaged-hosted-cluster-aws]
=== Prerequired and unmanaged infrastructure in a hosted cluster AWS account

When your infrastructure is prerequired and unmanaged in a hosted cluster AWS account, the infrastructure requirements for all access modes are as follows:

* One VPC
* One DHCP Option
* Two subnets
** A private subnet that is an internal data plane subnet
** A public subnet that enables access to the internet from the data plane
* One internet gateway
* One elastic IP
* One NAT gateway
* One security group (worker nodes)
* Two route tables (one private and one public)
* Two route 53 hosted zones
* Enough quota for the following items:
** One Ingress service load balancer for public hosted clusters
** One private link endpoint for private hosted clusters

*Note:* For private link networking to work, the endpoint zone in the hosted cluster AWS account must match the zone of the instance that is resolved by the service endpoint in the management cluster AWS account. In AWS, the zone names are aliases, such as `us-east-2b`, which do not necessarily map to the same zone in different accounts. As a result, for private link to work, the management cluster must have subnets or workers in all zones of its region.

[#infra-managed-by-hypershift-mgmt-aws-acct]
=== Hosted control planes-managed infrastructure in a management AWS account

When your infrastructure is managed by hosted control planes in a management AWS account, the infrastructure requirements differ depending on whether your clusters are public, private, or a combination.

For accounts with public clusters, the infrastructure requirements are as follows:

* Network load balancer: a load balancer Kube API server
** Kubernetes creates a security group
* Volumes
** For etcd (one or three depending on high availability)
** For OVN-Kube
//Should this be Kube-OVN?

For accounts with private clusters, the infrastructure requirements are as follows:

* Network Load Balancer: a load balancer private router
* Endpoint service (private link)

For accounts with public and private clusters, the infrastructure requirements are as follows:

* Network load balancer: a load balancer public router
* Network load balancer: a load balancer private router
* Endpoint service (private link)
* Volumes:
** For etcd (one or three depending on high availability)
** For OVN-Kube
//Should this be Kube-OVN?

[#infra-managed-by-hypershift-in-hosted-cluster-aws-acct]
=== Hosted control planes-managed infrastructure in a hosted cluster AWS account

When your infrastructure is managed by hosted control planes in a hosted cluster AWS account, the infrastructure requirements differ depending on whether your clusters are public, private, or a combination.

For accounts with public clusters, the infrastructure requirements are as follows:

* Node pools must have EC2 instances that have `Role` and `RolePolicy` defined.

For accounts with private clusters, the infrastructure requirements are as follows:

* One private link endpoint for each availability zone
* EC2 instances for node pools

For accounts with public and private clusters, the infrastructure requirements are as follows:

* One private link endpoint for each availability zone
* EC2 instances for node pools

[#infra-managed-by-kubernetes-in-hosted-cluster-aws-acct]
=== Kubernetes-managed infrastructure in a hosted cluster AWS account

When Kubernetes manages your infrastructure in a hosted cluster AWS account, the infrastructure requirements are as follows:

* A network load balancer for default Ingress
* An S3 bucket for registry

[#iam-aws]
=== Identity and Access Management (IAM) permissions

In the context of hosted control planes, the consumer, such as the command line interface or OpenShift Cluster Manager, is responsible to create the Amazon Resource Name (ARN) roles. Hosted control planes tries to enable granularity to honor the principle of least-privilege components, which means that every component uses its own role to operate or create AWS objects, and the roles are limited to what is required for the product to function normally.

//I'm keeping the next line commented-out until that documentation has been ported from upstream
//For an example of how the command line interface can create the ARN roles, see link:NEED LINK[Creating AWS infrastructure and IAM resources separately].

The hosted cluster receives the ARN roles as input and the consumer creates an AWS permission configuration for each component. As a result, the component can authenticate through STS and preconfigured OIDC IDP.

The following roles are consumed by some of the components from hosted control planes that run on the control plane and operate on the data plane:

* `controlPlaneOperatorARN`
* `imageRegistryARN`
* `ingressARN`
* `kubeCloudControllerARN`
* `nodePoolManagementARN`
* `storageARN`
* `networkARN`

The following example shows a reference to the IAM roles from the perspective of the hosted cluster:

// we need to replace cewong with an example name
----
...
endpointAccess: Public
  region: us-east-2
  resourceTags:
  - key: kubernetes.io/cluster/cewong-dev-bz4j5
    value: owned
rolesRef:
    controlPlaneOperatorARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-control-plane-operator
    imageRegistryARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-openshift-image-registry
    ingressARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-openshift-ingress
    kubeCloudControllerARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-cloud-controller
    networkARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-cloud-network-config-controller
    nodePoolManagementARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-node-pool
    storageARN: arn:aws:iam::820196288204:role/cewong-dev-bz4j5-aws-ebs-csi-driver-controller
type: AWS
...
----

The roles that hosted control planes uses are shown in the following examples:

* `ingressARN`
+
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:DescribeLoadBalancers",
                "tag:GetResources",
                "route53:ListHostedZones"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "route53:ChangeResourceRecordSets"
            ],
            "Resource": [
                "arn:aws:route53:::PUBLIC_ZONE_ID",
                "arn:aws:route53:::PRIVATE_ZONE_ID"
            ]
        }
    ]
}
----
* `imageRegistryARN`
+
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:CreateBucket",
                "s3:DeleteBucket",
                "s3:PutBucketTagging",
                "s3:GetBucketTagging",
                "s3:PutBucketPublicAccessBlock",
                "s3:GetBucketPublicAccessBlock",
                "s3:PutEncryptionConfiguration",
                "s3:GetEncryptionConfiguration",
                "s3:PutLifecycleConfiguration",
                "s3:GetLifecycleConfiguration",
                "s3:GetBucketLocation",
                "s3:ListBucket",
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListBucketMultipartUploads",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": "*"
        }
    ]
}
----

* `storageARN`
+
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:AttachVolume",
                "ec2:CreateSnapshot",
                "ec2:CreateTags",
                "ec2:CreateVolume",
                "ec2:DeleteSnapshot",
                "ec2:DeleteTags",
                "ec2:DeleteVolume",
                "ec2:DescribeInstances",
                "ec2:DescribeSnapshots",
                "ec2:DescribeTags",
                "ec2:DescribeVolumes",
                "ec2:DescribeVolumesModifications",
                "ec2:DetachVolume",
                "ec2:ModifyVolume"
            ],
            "Resource": "*"
        }
    ]
}
----

* `networkARN`
+
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeInstances",
                "ec2:DescribeInstanceStatus",
                "ec2:DescribeInstanceTypes",
                "ec2:UnassignPrivateIpAddresses",
                "ec2:AssignPrivateIpAddresses",
                "ec2:UnassignIpv6Addresses",
                "ec2:AssignIpv6Addresses",
                "ec2:DescribeSubnets",
                "ec2:DescribeNetworkInterfaces"
            ],
            "Resource": "*"
        }
    ]
}
----

* `kubeCloudControllerARN`
+
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "ec2:DescribeInstances",
                "ec2:DescribeImages",
                "ec2:DescribeRegions",
                "ec2:DescribeRouteTables",
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeSubnets",
                "ec2:DescribeVolumes",
                "ec2:CreateSecurityGroup",
                "ec2:CreateTags",
                "ec2:CreateVolume",
                "ec2:ModifyInstanceAttribute",
                "ec2:ModifyVolume",
                "ec2:AttachVolume",
                "ec2:AuthorizeSecurityGroupIngress",
                "ec2:CreateRoute",
                "ec2:DeleteRoute",
                "ec2:DeleteSecurityGroup",
                "ec2:DeleteVolume",
                "ec2:DetachVolume",
                "ec2:RevokeSecurityGroupIngress",
                "ec2:DescribeVpcs",
                "elasticloadbalancing:AddTags",
                "elasticloadbalancing:AttachLoadBalancerToSubnets",
                "elasticloadbalancing:ApplySecurityGroupsToLoadBalancer",
                "elasticloadbalancing:CreateLoadBalancer",
                "elasticloadbalancing:CreateLoadBalancerPolicy",
                "elasticloadbalancing:CreateLoadBalancerListeners",
                "elasticloadbalancing:ConfigureHealthCheck",
                "elasticloadbalancing:DeleteLoadBalancer",
                "elasticloadbalancing:DeleteLoadBalancerListeners",
                "elasticloadbalancing:DescribeLoadBalancers",
                "elasticloadbalancing:DescribeLoadBalancerAttributes",
                "elasticloadbalancing:DetachLoadBalancerFromSubnets",
                "elasticloadbalancing:DeregisterInstancesFromLoadBalancer",
                "elasticloadbalancing:ModifyLoadBalancerAttributes",
                "elasticloadbalancing:RegisterInstancesWithLoadBalancer",
                "elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer",
                "elasticloadbalancing:AddTags",
                "elasticloadbalancing:CreateListener",
                "elasticloadbalancing:CreateTargetGroup",
                "elasticloadbalancing:DeleteListener",
                "elasticloadbalancing:DeleteTargetGroup",
                "elasticloadbalancing:DescribeListeners",
                "elasticloadbalancing:DescribeLoadBalancerPolicies",
                "elasticloadbalancing:DescribeTargetGroups",
                "elasticloadbalancing:DescribeTargetHealth",
                "elasticloadbalancing:ModifyListener",
                "elasticloadbalancing:ModifyTargetGroup",
                "elasticloadbalancing:RegisterTargets",
                "elasticloadbalancing:SetLoadBalancerPoliciesOfListener",
                "iam:CreateServiceLinkedRole",
                "kms:DescribeKey"
            ],
            "Resource": [
                "*"
            ],
            "Effect": "Allow"
        }
    ]
}
----

* `nodePoolManagementARN`
+
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "ec2:AllocateAddress",
                "ec2:AssociateRouteTable",
                "ec2:AttachInternetGateway",
                "ec2:AuthorizeSecurityGroupIngress",
                "ec2:CreateInternetGateway",
                "ec2:CreateNatGateway",
                "ec2:CreateRoute",
                "ec2:CreateRouteTable",
                "ec2:CreateSecurityGroup",
                "ec2:CreateSubnet",
                "ec2:CreateTags",
                "ec2:DeleteInternetGateway",
                "ec2:DeleteNatGateway",
                "ec2:DeleteRouteTable",
                "ec2:DeleteSecurityGroup",
                "ec2:DeleteSubnet",
                "ec2:DeleteTags",
                "ec2:DescribeAccountAttributes",
                "ec2:DescribeAddresses",
                "ec2:DescribeAvailabilityZones",
                "ec2:DescribeImages",
                "ec2:DescribeInstances",
                "ec2:DescribeInternetGateways",
                "ec2:DescribeNatGateways",
                "ec2:DescribeNetworkInterfaces",
                "ec2:DescribeNetworkInterfaceAttribute",
                "ec2:DescribeRouteTables",
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeSubnets",
                "ec2:DescribeVpcs",
                "ec2:DescribeVpcAttribute",
                "ec2:DescribeVolumes",
                "ec2:DetachInternetGateway",
                "ec2:DisassociateRouteTable",
                "ec2:DisassociateAddress",
                "ec2:ModifyInstanceAttribute",
                "ec2:ModifyNetworkInterfaceAttribute",
                "ec2:ModifySubnetAttribute",
                "ec2:ReleaseAddress",
                "ec2:RevokeSecurityGroupIngress",
                "ec2:RunInstances",
                "ec2:TerminateInstances",
                "tag:GetResources",
                "ec2:CreateLaunchTemplate",
                "ec2:CreateLaunchTemplateVersion",
                "ec2:DescribeLaunchTemplates",
                "ec2:DescribeLaunchTemplateVersions",
                "ec2:DeleteLaunchTemplate",
                "ec2:DeleteLaunchTemplateVersions"
            ],
            "Resource": [
                "*"
            ],
            "Effect": "Allow"
        },
        {
            "Condition": {
                "StringLike": {
                    "iam:AWSServiceName": "elasticloadbalancing.amazonaws.com"
                }
            },
            "Action": [
                "iam:CreateServiceLinkedRole"
            ],
            "Resource": [
                "arn:*:iam::*:role/aws-service-role/elasticloadbalancing.amazonaws.com/AWSServiceRoleForElasticLoadBalancing"
            ],
            "Effect": "Allow"
        },
        {
            "Action": [
                "iam:PassRole"
            ],
            "Resource": [
                "arn:*:iam::*:role/*-worker-role"
            ],
            "Effect": "Allow"
        }
    ]
}
----

* `controlPlaneOperatorARN`
+
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreateVpcEndpoint",
                "ec2:DescribeVpcEndpoints",
                "ec2:ModifyVpcEndpoint",
                "ec2:DeleteVpcEndpoints",
                "ec2:CreateTags",
                "route53:ListHostedZones"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "route53:ChangeResourceRecordSets",
                "route53:ListResourceRecordSets"
            ],
            "Resource": "arn:aws:route53:::%s"
        }
    ]
}
----

[#hosted-cluster-arm-aws]
== Enabling hosted control planes on an ARM64 {ocp-short} cluster

You can enable an ARM64-hosted control plane to operate with an {ocp-short} ARM64 data plane in a management cluster environment. This feature is available for hosted control planes on AWS only.

[#prerequisites-hosted-arm]
=== Prerequisites

Before you begin, you must meet the following prerequisites:

* You must have an {ocp-short} cluster that was installed on a 64-bit ARM infrastructure. For more information, see link:https://console.redhat.com/openshift/install/aws/arm[Create an OpenShift Cluster: AWS (ARM)].
* You must have a HyperShift Operator that is built on a 64-bit ARM infrastructure. You can obtain a HyperShift Operator by going to the link:https://quay.io/repository/hypershift/hypershift-operator[hypershift/hypershift-operator repository] and selecting the build that has the `4.13-arm64` tag. 

To run a hosted cluster on an ARM64 {ocp-short} cluster, take the following steps:

. Install the HyperShift Operator for ARM64 on the management cluster to override the default HyperShift Operator image.
+
For example, through the hosted control planes (`hypershift`) command line interface, enter the following commands, being careful to replace the bucket name, AWS credentials, and region with your information:
+
----
hypershift install \
--oidc-storage-provider-s3-bucket-name $BUCKET_NAME \
--oidc-storage-provider-s3-credentials $AWS_CREDS \
--oidc-storage-provider-s3-region $REGION \
--hypershift-image quay.io/hypershift/hypershift-operator:4.13-arm64
----

. Create a hosted cluster that overrides the default release image with a multi-architecture release image.
+
For example, through the hosted control planes (`hypershift`) command line interface, enter the following commands, being careful to replace the cluster name, node pool replicas, base domain, pull secret, AWS credentials, and region with your information:
+
----
hypershift create cluster aws \ 
--name $CLUSTER_NAME \
--node-pool-replicas=$NODEPOOL_REPLICAS \
--base-domain $BASE_DOMAIN \
--pull-secret $PULL_SECRET \
--aws-creds $AWS_CREDS \
--region $REGION \
--release-image quay.io/openshift-release-dev/ocp-release:4.13.0-rc.0-multi
----
+
This example adds a default `NodePool` object through the `--node-pool-replicas` flag.

. Add a 64-bit x86 `NodePool` object to the hosted cluster.
+
For example, through the hosted control planes (`hypershift`) command line interface, enter the following commands, being careful to replace the cluster name, node pool name, and node pool replicas with your information:
+
----
hypershift create nodepool aws \
--cluster-name $CLUSTER_NAME \
--name $NODEPOOL_NAME \
--node-count=$NODEPOOL_REPLICAS
----

[#hypershift-cluster-destroy-aws]
== Destroying a hosted cluster on AWS

To destroy a hosted cluster and its managed cluster resource, complete the following steps:

. Delete the hosted cluster and its back-end resources by running the following command:
+
----
hypershift destroy cluster aws --name <cluster_name> --infra-id <infra_id> --aws-creds <aws-credentials> --base-domain <base_domain> --destroy-cloud-resources
----
+
Replace names where necessary.

. Delete the managed cluster resource on {mce-short} by running the following command:
+
----
oc delete managedcluster <cluster_name>
----
+
Replace `cluster_name` with the name of your cluster.
