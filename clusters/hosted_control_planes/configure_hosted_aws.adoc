[#hosted-control-planes-configure]
= Configuring hosted control planes (Technology Preview)

To configure hosted control planes, you need a hosting cluster and a hosted cluster. By deploying the HyperShift operator on an existing managed cluster by using the `hypershift-addon` managed cluster add-on, you can enable that cluster as a hosting cluster and start to create the hosted cluster. 

The {mce-short} 2.2 supports only the default `local-cluster` and the hub cluster as the hosting cluster.

Hosted control planes is a Technology Preview feature, so the related components are disabled by default. See the following topics to learn more about how to enable hosted control planes on Amazon Web Services (AWS) or bare metal:

* <<hosting-service-cluster-configure-aws,Configuring the hosting cluster on AWS>>
* <<hosting-service-cluster-configure-bm,Configuring the hosting cluster on bare metal>>

[#hosting-service-cluster-configure-aws]
== Configuring the hosting cluster on AWS

You can deploy hosted control planes by configuring an existing cluster to function as a hosting cluster. The hosting cluster is the {ocp} cluster where the control planes are hosted. {product-title-short} 2.7 can use the hub cluster, also known as the `local-cluster`, as the hosting cluster. See the following topics to learn how to configure the `local-cluster` as the hosting cluster.

*Best practice:* Be sure to run the hub cluster and workers on the same platform for hosted control planes.

[#hosting-service-cluster-configure-prereq-aws]
=== Prerequisites

You must have the following prerequisites to configure a hosting cluster: 

* The {mce} 2.2 and later installed on an {ocp-short} cluster. The {mce-short} is automatically installed when you install {product-title-short}. The {mce-short} can also be installed without {product-title-short} as an Operator from the {ocp-short} OperatorHub.

* The {mce-short} must have at least one managed {ocp-short} cluster. The `local-cluster` is automatically imported in {mce-short} 2.2 and later. See xref:../install_upgrade/adv_config_install.adoc#advanced-config-engine[Advanced configuration] for more information about the `local-cluster`. You can check the status of your hub cluster by running the following command:

+
----
oc get managedclusters local-cluster
----

* A hosting cluster with at least 3 worker nodes to run the HyperShift Operator.

[#hosted-create-aws-secret]
=== Creating the Amazon Web Services S3 OIDC secret

If you plan to create and manage hosted clusters on AWS, complete the following steps:

. Create an OIDC S3 secret named `hypershift-operator-oidc-provider-s3-credentials` for the HyperShift operator.

. Save the secret in the `local-cluster` namespace.

. See the following table to verify that the secret contains the following fields:
+
|===
| Field name | Description

| `bucket`
| Contains an S3 bucket with public access to host OIDC discovery documents for your HyperShift clusters.

| `credentials`
| A reference to a file that contains the credentials of the `default` profile that can access the bucket. By default, HyperShift only uses the `default` profile to operate the `bucket`. 

| `region`
| Specifies the region of the S3 bucket.
|===
+
See https://hypershift-docs.netlify.app/getting-started/[Getting started] in the HyperShift documentation for more information about the secret. The following example shows a sample AWS secret template:
+
----
oc create secret generic hypershift-operator-oidc-provider-s3-credentials --from-file=credentials=$HOME/.aws/credentials --from-literal=bucket=<s3-bucket-for-hypershift> --from-literal=region=<region> -n local-cluster
----
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. Run the following command to add the label that enables the `hypershift-operator-oidc-provider-s3-credentials` secret to be backed up for disaster recovery:
+
----
oc label secret hypershift-operator-oidc-provider-s3-credentials -n local-cluster cluster.open-cluster-management.io/backup=true
----

[#hosted-create-public-zone-aws]
=== Creating a routable public zone

To access applications in your guest clusters, the public zone must be routable. If the public zone exists, skip this step. Otherwise, the public zone will affect the existing functions.

Run the following command to create a public zone for cluster DNS records:

----
BASE_DOMAIN=www.example.com
aws route53 create-hosted-zone --name $BASE_DOMAIN --caller-reference $(whoami)-$(date --rfc-3339=date)
----

[#hosted-enable-ext-dns-aws]
=== Enabling external DNS

If you plan to provision hosted control plane clusters with service-level DNS (external DNS), complete the following steps:

. Create an AWS credential secret for the HyperShift Operator and name it `hypershift-operator-external-dns-credentials` in the `local-cluster` namespace.

. See the following table to verify that the secret contains the required fields:
+
|===
| Field name | Description | Optional or required

| `provider`
| The DNS provider that manages the service-level DNS zone.
| Required

| `domain-filter`
| The service-level domain.
| Required

| `credentials`
| The credential file that supports all external DNS types.
| Optional when using AWS keys

| `aws-access-key-id`
| The credential access key id.
| Optional when using the AWS DNS service

| `aws-secret-access-key`
| The credential access key secret.
| Optional when using the AWS DNS service
|===
+
See link:https://hypershift-docs.netlify.app/how-to/aws/external-dns/[External DNS] in the HyperShift documentation for more information. The following example shows the sample `hypershift-operator-external-dns-credentials` secret template:
+
----
oc create secret generic hypershift-operator-external-dns-credentials --from-literal=provider=aws --from-literal=domain-filter=service.my.domain.com --from-file=credentials=<credentials-file> -n local-cluster
----
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. Run the following command to add the label that enables the `hypershift-operator-external-dns-credentials` secret to be backed up for disaster recovery:
+
----
oc label secret hypershift-operator-external-dns-credentials -n local-cluster cluster.open-cluster-management.io/backup=""
----

[#hosted-enable-private-link]
=== Enabling AWS PrivateLink

If you plan to provision hosted control plane clusters on the AWS platform with PrivateLink, complete the following steps:

. Create an AWS credential secret for the HyperShift Operator and name it `hypershift-operator-private-link-credentials`. The secret must reside in the managed cluster namespace that is the namespace of the managed cluster being used as the hosting cluster. If you used `local-cluster`, create the secret in the `local-cluster` namespace.
+

. See the following table to confirm that the secret contains the required fields:
+
|===
| Field name | Description | Optional or required
| `region`
| Region for use with Private Link
| Required

| `aws-access-key-id`
| The credential access key id.
| Required

| `aws-secret-access-key`
| The credential access key secret.
| Required
|===
+
See https://hypershift-docs.netlify.app/how-to/aws/deploy-aws-private-clusters/[Deploying AWS private clusters] in the HyperShift documentation for more information. The following example shows the sample `hypershift-operator-private-link-credentials` secret template:
+
----
oc create secret generic hypershift-operator-private-link-credentials --from-literal=aws-access-key-id=<aws-access-key-id> --from-literal=aws-secret-access-key=<aws-secret-access-key> --from-literal=region=<region> -n local-cluster
----
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. Run the following command to add the label that enables the `hypershift-operator-private-link-credentials` secret to be backed up for disaster recovery:
+
----
oc label secret hypershift-operator-private-link-credentials -n local-cluster cluster.open-cluster-management.io/backup=""
----

[#hosted-enable-feature-aws]
=== Enabling the hosted control planes feature

The hosted control planes feature is disabled by default. Enabling the feature automatically also enables the `hypershift-addon` managed cluster add-on. You can run the following command to enable the feature:

----
oc patch mce multiclusterengine --type=merge -p '{"spec":{"overrides":{"components":[{"name":"hypershift-preview","enabled": true}]}}}'
----

Run the following command to verify that the `hypershift-preview` and `hypershift-local-hosting` features are enabled in the `MultiClusterEngine` custom resource.

----
oc get mce multiclusterengine -o yaml
----

[source,yaml]
----
apiVersion: multicluster.openshift.io/v1
kind: MultiClusterEngine
metadata:
  name: multiclusterengine
spec:
  overrides:
    components:
    - name: hypershift-preview
      enabled: true
    - name: hypershift-local-hosting
      enabled: true
----

[#hosted-enable-hypershift-add-on-aws]
==== Manually enabling the hypershift-addon managed cluster add-on for local-cluster

Enabling the hosted control planes feature automatically enables the `hypershift-addon` managed cluster add-on. If you need to enable the `hypershift-addon` managed cluster add-on manually, complete the following steps to use the `hypershift-addon` to install the HyperShift Operator on `local-cluster`:

. Create the `ManagedClusterAddon` HyperShift add-on by creating a file that resembles the following example:
+
[source,yaml]
----
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ManagedClusterAddOn
metadata:
  name: hypershift-addon
  namespace: local-cluster 
spec:
  installNamespace: open-cluster-management-agent-addon
----

. Apply the file by running the following command:
+
----
oc apply -f <filename>
----
+
Replace `filename` with the name of the file that you created. 

. Confirm that the `hypershift-addon` is installed by running the following command:
+
----
oc get managedclusteraddons -n local-cluster hypershift-addon
----

. If the add-on is installed, the output resembles the following example:
+
----
NAME               AVAILABLE   DEGRADED   PROGRESSING
hypershift-addon   True
----

Your HyperShift add-on is installed and the hosting cluster is available to create and manage HyperShift clusters.

[#hosted-install-cli]
=== Installing the hosted control planes CLI

The hosted control planes (HyperShift) CLI is used to create and manage {ocp-short} hosted control plane clusters. After enabling the hosted control planes feature, you can install the hosted control planes CLI by completing the following steps:

. From the {ocp-short} console, click the *Help icon* > *Command Line Tools*.

. Click *Download hypershift CLI* for your platform.
+
*Note:* The download is only visible if you have enabled the `hypershift-preview` feature.

. Unpack the downloaded archive by running the following command:
+
----
tar xvzf hypershift.tar.gz
----

. Run the following command to make the binary file executable:
+
----
chmod +x hypershift
----

. Run the following command to move the binary file to a directory in your path:
+
----
sudo mv hypershift /usr/local/bin/.
----

You can now use the `hypershift create cluster` command to create and manage hosted clusters. Use the following command to list the available parameters:

----
hypershift create cluster aws --help
----

[#hosting-service-cluster-configure-bm]
== Configuring the hosting cluster on bare metal

To provision hosted control planes on bare metal, you can use the Agent platform. The Agent platform uses the Central Infrastructure Management (CIM) service to add worker nodes to a hosted cluster. For an introduction to the CIM service, see link:https://github.com/openshift/assisted-service/blob/master/docs/hive-integration/kube-api-getting-started.md[Kube API - Getting Started Guide]. Each bare metal host must be started with a Discovery Image that the CIM provides. You can start the hosts manually or through automation by using the Cluster-Baremetal-Operator. After each host starts, it runs an Agent process to discover the host details and complete the installation. An `Agent` custom resource represents each host.

When you create a hosted cluster with the Agent platform, HyperShift installs the Agent CAPI provider in the Hosted Control Plane (HCP) namespace.

When you scale up a node pool, a machine is created. The CAPI provider finds an Agent that is approved, is passing validations, is not currently in use, and meets the requirements that are specified in the node pool specification. You can monitor the installation of an Agent by checking its status and conditions.

When you scale down a node pool, Agents are unbound from the corresponding cluster. Before you can reuse the clusters, you must restart them using the Discovery image to update the number of nodes.

[#hosting-service-cluster-configure-prereq-bm]
=== Prerequisites

You must have the following prerequisites to configure a hosting cluster: 

* {mce} 2.2 and later installed on an {ocp-short} cluster. When you install {product-title-short}, {mce-short} is automatically installed. You can also install {mce-short} without {product-title-short} as an Operator from the {ocp-short} OperatorHub.

* The hosted control planes command line interface (CLI) as a plugin for `oc` to create and manage the hosted cluster in {mce}. To install the CLI, follow the instructions in xref:../hosted_control_planes/configure_hosted.adoc#hosted-install-cli[Installing the hosted control plane CLI.

* {mce} must have at least one managed {ocp-short} cluster. The `local-cluster` is automatically imported in {mce} 2.2 and later. See xref:../install_upgrade/adv_config_install.adoc#advanced-config-engine[Advanced configuration] for more information about the `local-cluster`. You can check the status of your hub cluster by running the following command:
+
----
oc get managedclusters local-cluster
----

[#configure-dns-aws]
=== Configuring DNS

The API Server for the hosted cluster is exposed a `NodePort` service. A DNS entry must exist for `api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN}` that points to destination where the API Server can be reached.

The DNS entry can be as simple as a record that points to one of the nodes in the management cluster that is running the hosted control plane. The entry can also point to a load balancer that is deployed to redirect incoming traffic to the ingress pods. See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/installing/installing-on-any-platform#installation-load-balancing-user-infra_installing-platform-agnostic[Load balancing requirements for user-provisioned infrastructure] for more information about load balancers.

.Example DNS configuration
----
api.example.krnl.es.    IN A 192.168.122.20
api.example.krnl.es.    IN A 192.168.122.21
api.example.krnl.es.    IN A 192.168.122.22
api-int.example.krnl.es.    IN A 192.168.122.20
api-int.example.krnl.es.    IN A 192.168.122.21
api-int.example.krnl.es.    IN A 192.168.122.22
*.apps.example.krnl.es. IN A 192.168.122.23
----

You can now deploy the SR-IOV Operator. See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/networking/hardware-networks#sriov-operator-hosted-control-planes_configuring-sriov-operator[Deploying the SR-IOV Operator for hosted control planes] to learn more about deploying the SR-IOV Operator.
