[#hosting-service-cluster-configure-aws]
= Configuring the hosting cluster on AWS (Technology Preview)

To configure hosted control planes, you need a hosting cluster and a hosted cluster. By deploying the HyperShift Operator on an existing managed cluster by using the `hypershift-addon` managed cluster add-on, you can enable that cluster as a hosting cluster and start to create the hosted cluster. 

The {mce-short} 2.3 supports only the default `local-cluster` and the hub cluster as the hosting cluster.

Hosted control planes is a Technology Preview feature, so the related components are disabled by default.

You can deploy hosted control planes by configuring an existing cluster to function as a hosting cluster. The hosting cluster is the {ocp} cluster where the control planes are hosted. {product-title-short} 2.8 can use the hub cluster, also known as the `local-cluster`, as the hosting cluster. See the following topics to learn how to configure the `local-cluster` as the hosting cluster.

*Best practice:* Be sure to run the hub cluster and workers on the same platform for hosted control planes.

* <<hosting-service-cluster-configure-prereq-aws,Prerequisites>>
* <<hosted-create-aws-secret,Creating the Amazon Web Services S3 bucket and S3 OIDC secret>>
* <<hosted-create-public-zone-aws,Creating a routable public zone>>
* <<hosted-enable-ext-dns-aws,Enabling external DNS>>
* <<hosted-enable-private-link,Enabling AWS PrivateLink>>
* <<hosted-enable-feature-aws,Enabling the hosted control planes feature>>
* <<hosted-enable-hypershift-add-on-aws,Manually enabling the hypershift-addon managed cluster add-on for local-cluster>>
* <<hosted-install-cli,Installing the hosted control planes command line interface>>
* <<dr-hosted-cluster,Disaster recovery for a hosted cluster>>

[#hosting-service-cluster-configure-prereq-aws]
== Prerequisites

You must have the following prerequisites to configure a hosting cluster: 

* The {mce} 2.3 and later installed on an {ocp-short} cluster. The {mce-short} is automatically installed when you install {product-title-short}. The {mce-short} can also be installed without {product-title-short} as an Operator from the {ocp-short} OperatorHub.

* The {mce-short} must have at least one managed {ocp-short} cluster. The `local-cluster` is automatically imported in {mce-short} 2.3 and later. See xref:../install_upgrade/adv_config_install.adoc#advanced-config-engine[Advanced configuration] for more information about the `local-cluster`. You can check the status of your hub cluster by running the following command:

+
[source,bash]
----
oc get managedclusters local-cluster
----

* A hosting cluster with at least 3 worker nodes to run the HyperShift Operator.

* The AWS command line interface. 

[#hosted-create-aws-secret]
== Creating the Amazon Web Services S3 bucket and S3 OIDC secret

If you plan to create and manage hosted clusters on AWS, complete the following steps:

. Create an S3 bucket that has public access to host OIDC discovery documents for your clusters.
+
** To create the bucket in the us-east-1 region, enter the following code:
+ 
----
BUCKET_NAME=<your_bucket_name>
aws s3api create-bucket --bucket $BUCKET_NAME
aws s3api delete-public-access-block --bucket $BUCKET_NAME
echo '{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::${BUCKET_NAME}/*"
        }
    ]
}' | envsubst > policy.json
aws s3api put-bucket-policy --bucket $BUCKET_NAME --policy file://policy.json
----
+
** To create the bucket in a region other than the us-east-1 region, enter the following code:
+
----
BUCKET_NAME=your-bucket-name
REGION=us-east-2
aws s3api create-bucket --bucket $BUCKET_NAME \
  --create-bucket-configuration LocationConstraint=$REGION \
  --region $REGION
aws s3api delete-public-access-block --bucket $BUCKET_NAME
echo '{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::${BUCKET_NAME}/*"
        }
    ]
}' | envsubst > policy.json
aws s3api put-bucket-policy --bucket $BUCKET_NAME --policy file://policy.json
----

. Create an OIDC S3 secret named `hypershift-operator-oidc-provider-s3-credentials` for the HyperShift operator.

. Save the secret in the `local-cluster` namespace.

. See the following table to verify that the secret contains the following fields:
+
|===
| Field name | Description

| `bucket`
| Contains an S3 bucket with public access to host OIDC discovery documents for your HyperShift clusters.

| `credentials`
| A reference to a file that contains the credentials of the `default` profile that can access the bucket. By default, HyperShift only uses the `default` profile to operate the `bucket`. 

| `region`
| Specifies the region of the S3 bucket.
|===
+
The following example shows a sample AWS secret template:
+
[source,bash]
----
oc create secret generic hypershift-operator-oidc-provider-s3-credentials --from-file=credentials=$HOME/.aws/credentials --from-literal=bucket=<s3-bucket-for-hypershift> --from-literal=region=<region> -n local-cluster
----
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. Run the following command to add the label that enables the `hypershift-operator-oidc-provider-s3-credentials` secret to be backed up for disaster recovery:
+
[source,bash]
----
oc label secret hypershift-operator-oidc-provider-s3-credentials -n local-cluster cluster.open-cluster-management.io/backup=true
----

[#hosted-create-public-zone-aws]
== Creating a routable public zone

To access applications in your guest clusters, the public zone must be routable. If the public zone exists, skip this step. Otherwise, the public zone will affect the existing functions.

Run the following command to create a public zone for cluster DNS records:

----
BASE_DOMAIN=www.example.com
aws route53 create-hosted-zone --name $BASE_DOMAIN --caller-reference $(whoami)-$(date --rfc-3339=date)
----

[#hosted-enable-ext-dns-aws]
== Enabling external DNS

Because the control plane and the data plane are separate in hosted control planes, you can configure DNS in two independent areas:

- Ingress for workloads within the hosted cluster, such as the following domain: `*.apps.service-consumer-domain.com`

- Ingress for service endpoints within the management cluster, such as API or OAUTH endpoints through the service provider domain: `*.service-provider-domain.com`

The input for the `hostedCluster.spec.dns` dictates the Ingress for workloads within the hosted cluster. The input for `hostedCluster.spec.services.servicePublishingStrategy.route.hostname` dictates the Ingress for service endpoints within the management cluster.

External DNS creates name records for hosted cluster `Services` that specify a publishing type of `LoadBalancer` or `Route` and provide a hostname for that publishing type. For hosted clusters with `Private` or `PublicAndPrivate` endpoint access types, only the `APIServer` and `OAuth` services support hostnames. For `Private` hosted clusters, the DNS record resolves to a private IP of a Virtual Private Cloud (VPC) Endpoint in your VPC.

A hosted control plane exposes four services:

* `APIServer`
* `OAuthServer`
* `Konnectivity`
* `Ignition`

Each of those services is exposed by using `servicePublishingStrategy` in the `HostedCluster` specification. By default, for the `LoadBalancer` and `Route` types of `servicePublishingStrategy`, you publish the service in one of two ways: 

* By using the hostname of the load balancer that is in the status of the `Service` with the `LoadBalancer` type
* In the `status.host` field of the `Route`

However, when you deploy hosted control planes in a managed service context, those methods can expose the Ingress subdomain of the underlying management cluster and limit options for the management cluster lifecycle and disaster recovery.

When a DNS indirection is layered on the `LoadBalancer` and `Route` publishing types, a managed service operator can publish all public hosted cluster services by using a service-level domain. This architecture allows remapping on the DNS name to a new `LoadBalancer` or `Route` and does not expose the Ingress domain of the management cluster. Hosted control planes uses external DNS to achieve that indirection layer.

You can deploy `external-dns` alongside the `hypershift` Operator in the `hypershift` namespace of the management cluster. The external DNS watches for `Services` or `Routes` that have the `external-dns.alpha.kubernetes.io/hostname` annotation. That annotation is used to create a DNS record that points to the `Service`, such as a record, or the `Route`, such as a CNAME record.

[#external-dns-prereqs-aws]
=== Prerequisites

Before you can set up external DNS for hosted control planes, you must meet the following prerequisites:

* An external public domain that you can point to

* Access to the AWS Route53 Management console

[#set-up-external-dns-aws]
=== Setting up external DNS for hosted control planes

If you plan to provision hosted control plane clusters with service-level DNS (external DNS), complete the following steps:

. Create an AWS credential secret for the HyperShift Operator and name it `hypershift-operator-external-dns-credentials` in the `local-cluster` namespace.

. See the following table to verify that the secret has the required fields:
+
|===
| Field name | Description | Optional or required

| `provider`
| The DNS provider that manages the service-level DNS zone.
| Required

| `domain-filter`
| The service-level domain.
| Required

| `credentials`
| The credential file that supports all external DNS types.
| Optional when you use AWS keys

| `aws-access-key-id`
| The credential access key id.
| Optional when you use the AWS DNS service

| `aws-secret-access-key`
| The credential access key secret.
| Optional when you use the AWS DNS service
|===
+
The following example shows the sample `hypershift-operator-external-dns-credentials` secret template:
+
[source,bash]
----
oc create secret generic hypershift-operator-external-dns-credentials --from-literal=provider=aws --from-literal=domain-filter=service.my.domain.com --from-file=credentials=<credentials-file> -n local-cluster
----
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. To add the label that enables the `hypershift-operator-external-dns-credentials` secret to be backed up for disaster recovery, enter the following command:
+
[source,bash]
----
oc label secret hypershift-operator-external-dns-credentials -n local-cluster cluster.open-cluster-management.io/backup=""
----

[#create-public-dns-hosted-zone-aws]
=== Creating the public DNS hosted zone

You can create the public DNS hosted zone to use as the external DNS domain-filter in the AWS Route 53 management console:

. In the Route 53 management console, click *Create hosted zone*.

. On the *Hosted zone configuration* page, type a domain name, verify that *Publish hosted zone* is selected as the type, and click *Create hosted zone*.

. After the zone is created, on the *Records* tab, note the values in the *Value/Route traffic to* column.

. In the main domain, create an NS record to redirect the DNS requests to the delegated zone. In the *Value* field, enter the values that you noted in the previous step.

. Click *Create records*.

. Verify that the DNS hosted zone is working by creating a test entry in the new subzone and testing it with a `dig` command like the following example:
+
----
dig +short test.user-dest-public.aws.kerberos.com
192.168.1.1
----

. To create a hosted cluster that sets the hostname for `LoadBalancer` and `Route` services, enter the following command, where `external-dns-domain` matches the public hosted zone that you created:
+
[source,bash]
----
hypershift create cluster aws --name=example --endpoint-access=PublicAndPrivate --external-dns-domain=service-provider-domain.com ...
----

This example shows the resulting `services` block for the hosted cluster:

[source,yaml]
----
  platform:
    aws:
      endpointAccess: PublicAndPrivate
...
  services:
  - service: APIServer
    servicePublishingStrategy:
      route:
        hostname: api-example.service-provider-domain.com
      type: Route
  - service: OAuthServer
    servicePublishingStrategy:
      route:
        hostname: oauth-example.service-provider-domain.com
      type: Route
  - service: Konnectivity
    servicePublishingStrategy:
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
----

When the Control Plane Operator creates the `Services` and `Routes`, it annotates them with the `external-dns.alpha.kubernetes.io/hostname` annotation. The value is the `hostname` field in the `servicePublishingStrategy` for that type. The Control Plane Operator uses that name for the service endpoints, and it expects that if the hostname is set, a mechanism exists, such as external-dns or otherwise, which can create the DNS records.

Only public services can have service-level DNS indirection. Private services use the `hypershift.local` private zone, and it is not valid to set `hostname` for services that are private for a given endpoint access type. 

The following table notes when it is valid to set `hostname` for a service and endpoint combination:

|===
|Service |Public |PublicAndPrivate |Private

|`APIServer`
|Y
|Y
|N

|`OAuthServer`
|Y
|Y
|N

|`Konnectivity`
|Y
|N
|N

|`Ignition`
|Y
|N
|N
|===

[#deploy-cluster-cli-external-dns-aws]
=== Deploying a cluster by using the command line interface and external DNS

You need to deploy the `hypershift` and `external-dns` Operators when the external public hosted zone already exists. Ensure that the `external-dns` Operator is running and that the internal flags point to the public hosted zone by entering the following commands:

----
export KUBECONFIG=<path_to_management_cluster_kubeconfig>
export AWS_CREDS=~/.aws/credentials
export REGION=<region>

hypershift create cluster aws \
    --aws-creds ${AWS_CREDS} \
    --instance-type m6i.xlarge \
    --region ${REGION} \
    --auto-repair \
    --generate-ssh \
    --name <cluster_name> \
    --namespace clusters \
    --base-domain service-consumer-domain.com \ <1>
    --node-pool-replicas 2 \
    --pull-secret ${HOME}/pull_secret.json \
    --release-image quay.io/openshift-release-dev/ocp-release:4.12.0-ec.3-x86_64 \
    --external-dns-domain=service-provider-domain.com \ <2>
    --endpoint-access=PublicAndPrivate <3>
----

<1> Points to the public hosted zone, `service-consumer-domain.com`, which is typically in an AWS account that the service consumer owns.
<2> Points to the public external DNS hosted zone, `service-provider-domain.com`, which is typically in an AWS account that the service provider owns.
<3> Set as `PublicAndPrivate.` The external DNS can be used with only `Public` or `PublicAndPrivate` configurations.

[#hosted-enable-private-link]
== Enabling AWS PrivateLink

If you plan to provision hosted control plane clusters on the AWS platform with PrivateLink, complete the following steps:

. Create an AWS credential secret for the HyperShift Operator and name it `hypershift-operator-private-link-credentials`. The secret must reside in the managed cluster namespace that is the namespace of the managed cluster being used as the hosting cluster. If you used `local-cluster`, create the secret in the `local-cluster` namespace.

. See the following table to confirm that the secret contains the required fields:
+
|===
| Field name | Description | Optional or required
| `region`
| Region for use with Private Link
| Required

| `aws-access-key-id`
| The credential access key id.
| Required

| `aws-secret-access-key`
| The credential access key secret.
| Required
|===
+
The following example shows the sample `hypershift-operator-private-link-credentials` secret template:
+
[source,bash]
----
oc create secret generic hypershift-operator-private-link-credentials --from-literal=aws-access-key-id=<aws-access-key-id> --from-literal=aws-secret-access-key=<aws-secret-access-key> --from-literal=region=<region> -n local-cluster
----
+
*Note:* Disaster recovery backup for the secret is not automatically enabled. Run the following command to add the label that enables the `hypershift-operator-private-link-credentials` secret to be backed up for disaster recovery:
+
[source,bash]
----
oc label secret hypershift-operator-private-link-credentials -n local-cluster cluster.open-cluster-management.io/backup=""
----

[#hosted-enable-feature-aws]
== Enabling the hosted control planes feature

The hosted control planes feature is disabled by default. Enabling the feature automatically also enables the `hypershift-addon` managed cluster add-on. 

. You can run the following commands to enable the feature:
+
[source,bash]
----
oc patch mce multiclusterengine --type=merge -p '{"spec":{"overrides":{"components":[{"name":"hypershift-preview","enabled": true}]}}}' <1>
----
<1> The default `MultiClusterEngine` resource instance name is `multiclusterengine`, but you can get the `MultiClusterEngine` name from your cluster by running the following command: `$ oc get mce`.

+
[source,bash]
----
oc patch mce multiclusterengine --type=merge -p '{"spec":{"overrides":{"components":[{"name":"hypershift-local-hosting","enabled": true}]}}}' <1>
----
+
<1> The default `MultiClusterEngine` resource instance name is `multiclusterengine`, but you can get the `MultiClusterEngine` name from your cluster by running the following command: `$ oc get mce`. 

. Run the following command to verify that the `hypershift-preview` and `hypershift-local-hosting` features are enabled in the `MultiClusterEngine` custom resource:
+
[source,bash]
----
oc get mce multiclusterengine -o yaml <1>
----
<1> The default `MultiClusterEngine` resource instance name is `multiclusterengine`, but you can get the `MultiClusterEngine` name from your cluster by running the following command: `$ oc get mce`.
+
The output resembles the following example:
+
[source,yaml]
----
apiVersion: multicluster.openshift.io/v1
kind: MultiClusterEngine
metadata:
  name: multiclusterengine
spec:
  overrides:
    components:
    - name: hypershift-preview
      enabled: true
    - name: hypershift-local-hosting
      enabled: true
----

[#hosted-enable-hypershift-add-on-aws]
=== Manually enabling the hypershift-addon managed cluster add-on for local-cluster

Enabling the hosted control planes feature automatically enables the `hypershift-addon` managed cluster add-on. If you need to enable the `hypershift-addon` managed cluster add-on manually, complete the following steps to use the `hypershift-addon` to install the HyperShift Operator on `local-cluster`:

. Create the `ManagedClusterAddon` HyperShift add-on by creating a file that resembles the following example:
+
[source,yaml]
----
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ManagedClusterAddOn
metadata:
  name: hypershift-addon
  namespace: local-cluster 
spec:
  installNamespace: open-cluster-management-agent-addon
----

. Apply the file by running the following command:
+
[source,bash]
----
oc apply -f <filename>
----
+
Replace `filename` with the name of the file that you created. 

. Confirm that the `hypershift-addon` is installed by running the following command:
+
[source,bash]
----
oc get managedclusteraddons -n local-cluster hypershift-addon
----
+
If the add-on is installed, the output resembles the following example:
+
----
NAME               AVAILABLE   DEGRADED   PROGRESSING
hypershift-addon   True
----

Your HyperShift add-on is installed and the hosting cluster is available to create and manage hosted clusters.

[#hosted-install-cli]
== Installing the hosted control planes command line interface

The hosted control planes (`hypershift`) command line interface is used to create and manage {ocp-short} hosted control plane clusters. After you enable the hosted control planes feature, you can install the hosted control planes command line interface by completing the following steps:

. From the {ocp-short} console, click the *Help icon* > *Command Line Tools*.

. Click *Download hypershift CLI* for your platform.
+
*Note:* The download is only visible if you have enabled the `hypershift-preview` feature.

. Unpack the downloaded archive by running the following command:
+
----
tar xvzf hypershift.tar.gz
----

. Run the following command to make the binary file executable:
+
----
chmod +x hypershift
----

. Run the following command to move the binary file to a directory in your path:
+
----
sudo mv hypershift /usr/local/bin/.
----

You can now use the `hypershift create cluster` command to create and manage hosted clusters. Use the following command to list the available parameters:

----
hypershift create cluster aws --help
----

[#dr-hosted-cluster]
== Disaster recovery for a hosted cluster

The hosted control plane runs on the {mce-short} hub cluster. The data plane runs on a separate platform that you choose. When recovering the {mce-short} hub cluster from a disaster, you might also want to recover the hosted control planes.

See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html/hosted_control_planes/hcp-backup-restore-dr#hcp-dr-aws[Disaster recovery for a hosted cluster within an AWS region] to learn how to back up a hosted control plane cluster and restore it on a different cluster.

*Important:* Disaster recovery for hosted clusters is available on AWS only.

[#additional-resources-configure-hosted-cluster-aws]
== Additional resources

For more information about hosted control planes on AWS, see the following resource:

* You can now deploy the SR-IOV Operator. For more information, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html/networking/hardware-networks#sriov-operator-hosted-control-planes_configuring-sriov-operator[Deploying the SR-IOV Operator for hosted control planes].
