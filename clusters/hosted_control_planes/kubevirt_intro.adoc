[#hosted-control-planes-manage-kubevirt]
= Managing hosted control plane clusters on OpenShift Virtualization

With hosted control planes and {ocp-virt}, you can create {ocp-short} clusters with worker nodes that are hosted by KubeVirt virtual machines. Hosted control planes on {ocp-virt-short} provides several benefits:

* Enhances resource usage by packing hosted control planes and hosted clusters in the same underlying bare metal infrastructure
* Separates hosted control planes and hosted clusters to provide strong isolation
* Reduces cluster provision time by eliminating the bare metal node bootstrapping process
* Manages many releases under the same base {ocp-short} cluster

The hosted control planes feature is enabled by default.

You can use the hosted control plane command line interface, `hcp`, to create an {ocp-short} hosted cluster. The hosted cluster is automatically imported as a managed cluster. If you want to disable this automatic import feature, see xref:../hosted_control_planes/disable_auto_import.adoc#hosted-disable-auto-import[Disabling the automatic import of hosted clusters into {mce-short}].

*Important:*

- Run the hub cluster and workers on the same platform for hosted control planes.

- Each hosted cluster must have a cluster-wide unique name. A hosted cluster name cannot be the same as any existing managed cluster in order for {mce-short} to manage it.

- Do not use `clusters` as a hosted cluster name.

- A hosted cluster cannot be created in the namespace of a {mce-short} managed cluster.

- When you configure storage for hosted control planes, consider the recommended etcd practices. To ensure that you meet the latency requirements, dedicate a fast storage device to all hosted control plane etcd instances that run on each control-plane node. You can use LVM storage to configure a local storage class for hosted etcd pods. For more information, see _Recommended etcd practices_ and _Persistent storage using logical volume manager storage_ in the {ocp-short} documentation.

[#create-hosted-clusters-prereqs-kubevirt]
== Prerequisites

You must meet the following prerequisites to create an {ocp-short} cluster on {ocp-virt-short}:

- You need administrator access to an {ocp-short} cluster, version 4.14 or later, specified by the `KUBECONFIG` environment variable.
- The {ocp-short} hosting cluster must have wildcard DNS routes enabled, as shown in the following DNS:

+
----
oc patch ingresscontroller -n openshift-ingress-operator default --type=json -p '[{ "op": "add", "path": "/spec/routeAdmission", "value": {wildcardPolicy: "WildcardsAllowed"}}]'
----
- The {ocp-short} hosting cluster must have {ocp-virt-short}, version 4.14 or later, installed on it. For more information, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/virtualization/installing#installing-virt-web[Installing OpenShift Virtualization using the web console].
- The {ocp-short} hosting cluster must be configured with OVNKubernetes as the default pod network CNI.
- The {ocp-short} hosting cluster must have a default storage class. For more information, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.15/html/postinstallation_configuration/post-install-storage-configuration[Postinstallation storage configuration]. The following example shows how to set a default storage class:

+
----
oc patch storageclass ocs-storagecluster-ceph-rbd -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
----

- You need a valid pull secret file for the `quay.io/openshift-release-dev` repository. For more information, see link:https://console.redhat.com/openshift/install/platform-agnostic/user-provisioned[Install OpenShift on any x86_64 platform with user-provisioned infrastructure].
- You need to xref:../hosted_control_planes/install_hcp_cli.adoc#hosted-install-cli[install the hosted control plane command line interface].
- Before you can provision your cluster, you need to configure a load balancer. For more information, see xref:../hosted_control_planes/config_metallb_bm.adoc#hosting-service-cluster-configure-metallb-config[Optional: Configuring MetalLB].
- For optimal network performance, use a network maximum transmission unit (MTU) of 9000 or greater on the {ocp-short} cluster that hosts the KubeVirt virtual machines. If you use a lower MTU setting, network latency and the throughput of the hosted pods are affected. Enable multiqueue on node pools only when the MTU is 9000 or greater.

- The {mce-short} must have at least one managed {ocp-short} cluster. The `local-cluster` is automatically imported in {mce-short} 2.4 and later. See xref:../install_upgrade/adv_config_install.adoc#advanced-config-engine[Advanced configuration] for more information about the `local-cluster`. You can check the status of your hub cluster by running the following command:

+
----
oc get managedclusters local-cluster
----

[#firewall-port-reqs-kubevirt]
== Firewall and port requirements

Ensure that you meet the firewall and port requirements so that ports can communicate between the management cluster, the control plane, and hosted clusters:

* The `kube-apiserver` service runs on port 6443 by default and requires ingress access for communication between the control plane components.

** If you use the `NodePort` publishing strategy, ensure that the node port that is assigned to the `kube-apiserver` service is exposed.
** If you use MetalLB load balancing, allow ingress access to the IP range that is used for load balancer IP addresses.

* If you use the `NodePort` publishing strategy, use a firewall rule for the `ignition-server` and `Oauth-server` settings.

* The `konnectivity` agent, which establishes a reverse tunnel to allow bi-directional communication on the hosted cluster, requires egress access to the cluster API server address on port 6443. With that egress access, the agent can reach the `kube-apiserver` service.

** If the cluster API server address is an internal IP address, allow access from the workload subnets to the IP address on port 6443.
** If the address is an external IP address, allow egress on port 6443 to that external IP address from the nodes.

* If you change the default port of 6443, adjust the rules to reflect that change.
* Ensure that you open any ports that are required by the workloads that run in the clusters.
* Use firewall rules, security groups, or other access controls to restrict access to only required sources. Avoid exposing ports publicly unless necessary.
* For production deployments, use a load balancer to simplify access through a single IP address.

For additional resources about hosted control planes on {ocp-virt}, see the following documentation:

* To learn about etcd and LVM storage recommendations, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/scalability_and_performance/recommended-performance-and-scalability-practices#recommended-etcd-practices[Recommended etcd practices] and link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/storage/configuring-persistent-storage#persistent-storage-using-lvms[Persistent storage using logical volume manager storage].

* To configure hosted control planes on {ocp-virt} in a disconnected environment, see xref:../hosted_control_planes/disconnected_intro.adoc#configure-hosted-disconnected[Configuring hosted control planes in a disconnected environment].

* To disable the hosted control planes feature or, if you already disabled it and want to manually enable it, see xref:../hosted_control_planes/enable_or_disable_hosted.adoc#enable-or-disable-hosted-control-planes[Enabling or disabling the hosted control planes feature].

* To manage hosted clusters by running {aap} jobs, see xref:../cluster_lifecycle/ansible_config_hosted_cluster.adoc#ansible-config-hosted-cluster[Configuring {aap-short} jobs to run on hosted clusters].
