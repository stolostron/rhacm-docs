[#deploying-aws-private-clusters]
= Deploying a private hosted cluster on AWS

After you set up the hosted control planes command line interface, `hcp`, and enable the `local-cluster` as the hosting cluster, you can deploy a hosted cluster or a private hosted cluster on Amazon Web Services (AWS). To deploy a public hosted cluster on AWS, see _Deploying a hosted cluster on AWS_.

By default, hosted control plane guest clusters are publicly accessible through public DNS and the default router for the management cluster.

For private clusters on AWS, all communication with the guest cluster occurs over AWS PrivateLink. To configure hosted control planes for private cluster support on AWS, take the following steps.

*Important:* Although public clusters can be created in any region, you can create private clusters only in the region that is specified by the `--aws-private-region` flag.

* <<prerequisites-aws-private-clusters,Prerequisites>>
* <<create-aws-private-hosted-cluster,Creating a private hosted cluster on AWS>>
* <<access-aws-private-hosted-cluster,Accessing a private hosting cluster on AWS>>

[#prerequisites-aws-private-clusters]
== Prerequisites

* To enable private hosted clusters for AWS, you must first enable AWS PrivateLink. For more information, see xref:../hosted_control_planes/enable_aws_private_link.adoc#hosted-enable-private-link[Enabling AWS PrivateLink].

* You must create an AWS Identity and Access Management (IAM) role and AWS Security Token Service (STS) credentials. For more information, see xref:../hosted_control_planes/create_role_sts_aws.adoc#create-role-sts-aws[Creating an AWS IAM role and STS credentials] and link:../hosted_control_planes/manage_aws_infra_iam.adoc#iam_aws[Identity and Access Management (IAM) permissions]. 

* To access a private cluster, you need a bastion instance on AWS. For more information, see link:https://aws.amazon.com/solutions/implementations/linux-bastion/[Linux Bastion Hosts on AWS] in the AWS documentation.

[#create-aws-private-hosted-cluster]
== Creating a private hosted cluster on AWS

To create a private hosted cluster on AWS, complete the following steps:

. Create a private hosted cluster by entering the following command, replacing variables with your values as needed:

+
[source,bash]
----
hcp create cluster aws \
--name <hosted_cluster_name> \ <1>
--node-pool-replicas=<node_pool_replica_count> \ <2>
--base-domain <basedomain> \ <3>
--pull-secret <path_to_pull_secret> \ <4>
--sts-creds <path_to_sts_credential_file> \ <5>
--region <region> \ <6>
--endpoint-access Private <7>
--role-arn <role_name> <8>
----

+
<1> Specify the name of your hosted cluster, for instance, `example`.
<2> Specify the node pool replica count, for example, `3`.
<3> Specify your base domain, for example, `example.com`.
<4> Specify the path to your pull secret, for example, `/user/name/pullsecret`.
<5> Specify the path to your AWS STS credentials file, for example, `/home/user/sts-creds/sts-creds.json`.
<6> Specify the AWS region name, for example, `us-east-1`.
<7> Defines whether a cluster is public or private.
<8> Specify the Amazon Resource Name (ARN), for example, `arn:aws:iam::820196288204:role/myrole`. For more information about ARN roles, see _Identity and Access Management (IAM) permissions_.

+
The API endpoints for the cluster are accessible through a private DNS zone:

- `api.<hosted_cluster_name>.hypershift.local`
- `*.apps.<hosted_cluster_name>.hypershift.local`

[#addl-rs-private-hc-aws]
=== Additional resources

* For more information about deploying a public hosted cluster on AWS, see xref:../hosted_control_planes/managing_hosted_aws.adoc#hosted-deploy-cluster-aws[Deploying a hosted cluster on AWS].

* For more information about the ARN roles that you need to create a hosted cluster, see link:../hosted_control_planes/manage_aws_infra_iam.adoc#iam_aws[Identity and Access Management (IAM) permissions].

[#access-aws-private-hosted-cluster]
== Accessing a private hosting cluster on AWS

. Find the private IPs of nodes in the cluster node pool by entering the following command:
+
[source,bash]
----
aws ec2 describe-instances --filter="Name=tag:kubernetes.io/cluster/<infra_id>,Values=owned" | jq '.Reservations[] | .Instances[] | select(.PublicDnsName=="") | .PrivateIpAddress'
----

. Create a `kubeconfig` file for the hosted cluster that you can copy to a node by entering the following command:
+
[source,bash]
----
hcp create kubeconfig > <hosted_cluster_kubeconfig>
----

. Enter the following command to SSH into one of the nodes through the bastion:
+
[source,bash]
----
ssh -o ProxyCommand="ssh ec2-user@<bastion_ip> -W %h:%p" core@<node_ip>
----

. From the SSH shell, copy the `kubeconfig` file contents to a file on the node by entering the following command:
+
[source,bash]
----
mv <path_to_kubeconfig_file> <new_file_name>
----

. Export the kubeconfig file by entering the following command:
+
[source,bash]
----
export KUBECONFIG=<path_to_kubeconfig_file>
----

. Observe the guest cluster status by entering the following command:
+
[source,bash]
----
oc get clusteroperators clusterversion
----