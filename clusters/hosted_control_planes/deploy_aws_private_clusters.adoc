[#deploying-aws-private-clusters]
= Deploying a private hosted cluster on AWS (Technology Preview)

After you set up the hosted control planes command line interface, `hcp`, and enable the `local-cluster` as the hosting cluster, you can deploy a hosted cluster or a private hosted cluster on AWS. To deploy a public hosted cluster on AWS, see _Deploying a hosted cluster on AWS_.

By default, hosted control plane guest clusters are publicly accessible through public DNS and the default router for the management cluster.

For private clusters on AWS, all communication with the guest cluster occurs over AWS PrivateLink. To configure hosted control planes for private cluster support on AWS, take the following steps.

*Important:* Although public clusters can be created in any region, private clusters can be created only in the region that is specified by `--aws-private-region`.

* <<prerequisites-aws-private-clusters,Prerequisites>>
* <<create-aws-private-hosted-cluster,Creating a private hosted cluster on AWS>>
* <<access-aws-private-hosted-cluster,Accessing a private hosting cluster on AWS>>
* <<additional-resources-private-hosted-cluster-aws,Additional resources>>

[#prerequisites-aws-private-clusters]
== Prerequisites

To enable private hosted clusters for AWS, you must first enable AWS PrivateLink. For more information, see xref:../../clusters/hosted_control_planes/configure_hosted_aws.adoc#hosted-enable-private-link[Enabling AWS PrivateLink].

[#create-aws-private-hosted-cluster]
== Creating a private hosted cluster on AWS

. Create the private cluster IAM policy document by entering the following command:
+
----
cat << EOF >> policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateVpcEndpointServiceConfiguration",
        "ec2:DescribeVpcEndpointServiceConfigurations",
        "ec2:DeleteVpcEndpointServiceConfigurations",
        "ec2:DescribeVpcEndpointServicePermissions",
        "ec2:ModifyVpcEndpointServicePermissions",
        "ec2:CreateTags",
        "elasticloadbalancing:DescribeLoadBalancers"
      ],
      "Resource": "\*"
    }
  ]
}
----

. Create the IAM policy in AWS by entering the following command:
+
----
aws iam create-policy --policy-name=hypershift-operator-policy --policy-document=file://policy.json
----

. Create a `hypershift-operator` IAM user by entering the following command:
+
----
aws iam create-user --user-name=hypershift-operator
----

. Attach the policy to the `hypershift-operator` user by entering this command, replacing `$POLICY_ARN` with the ARN of the policy that you created:
+
----
aws iam attach-user-policy --user-name=hypershift-operator --policy-arn=$POLICY_ARN
----

. Create an IAM access key for the user by entering this command:
+
----
aws iam create-access-key --user-name=hypershift-operator
----

. Create a private hosted cluster by entering the following command, replacing variables with your values as needed:
+
----
CLUSTER_NAME=example
BASE_DOMAIN=example.com
AWS_CREDS="$HOME/.aws/credentials"
PULL_SECRET="$HOME/pull-secret"

hcp create cluster aws \
--name $CLUSTER_NAME \
--node-pool-replicas=3 \
--base-domain $BASE_DOMAIN \
--pull-secret $PULL_SECRET \
--aws-creds $AWS_CREDS \
--region $REGION \
--endpoint-access Private <1>
----
<1> The `--endpoint-access` flag designates whether a cluster is public or private.

The API endpoints for the cluster are accessible through a private DNS zone:

- `api.$CLUSTER_NAME.hypershift.local`
- `*.apps.$CLUSTER_NAME.hypershift.local`

[#access-aws-private-hosted-cluster]
== Accessing a private hosting cluster on AWS

To access a private cluster, you use a bastion.

//lahinson - july 2023 - update hypershift cli command here
. Start a bastion instance by entering the following command, replacing `$SSH_KEY` with the credentials to connect to the bastion:
+
----
hypershift create bastion aws --aws-creds=$AWS_CREDS --infra-id=$INFRA_ID --region=$REGION --ssh-key-file=$SSH_KEY
----

. Find the private IPs of nodes in the cluster node pool by entering the following command:
+
----
aws ec2 describe-instances --filter="Name=tag:kubernetes.io/cluster/$INFRA_ID,Values=owned" | jq '.Reservations[] | .Instances[] | select(.PublicDnsName=="") | .PrivateIpAddress'
----

. Create a `kubeconfig` file for the cluster that can be copied to a node by entering the following command:
+
----
hcp create kubeconfig > $CLUSTER_KUBECONFIG
----

. Enter the following command to SSH into one of the nodes through the bastion by using the IP that is printed from the `create bastion` command: 
+
----
ssh -o ProxyCommand="ssh ec2-user@$BASTION_IP -W %h:%p" core@$NODE_IP
----

. From the SSH shell, copy the `kubeconfig` file contents to a file on the node by entering the following commands:
+
----
cat << EOF >> kubeconfig
<paste kubeconfig contents>
export KUBECONFIG=$PWD/kubeconfig
----

. From the SSH shell, observe the guest cluster status or run other `oc` commands as shown in this example:
+
----
oc get clusteroperators
oc get clusterversion
----

[#additional-resources-private-hosted-cluster-aws]
== Additional resources

For more information about deploying a public hosted cluster on AWS, see xref:../hosted_control_planes/managing_hosted_aws.adoc#hosted-deploy-cluster-aws[Deploying a hosted cluster on AWS].
