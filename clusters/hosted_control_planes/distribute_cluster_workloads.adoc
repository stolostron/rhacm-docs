[#hosted-cluster-workload-distributing]
= Distributing hosted cluster workloads

Before you get started with hosted control planes for {ocp-short}, you must label nodes to schedule the hosted cluster pods into infrastructure nodes.

Node labeling ensures the following functions:

* High availability and proper workload deployment. For example, you can set the `node-role.kubernetes.io/infra` label to avoid having the control-plane workload count toward your {ocp-short} subscription.
* Keeping control plane workloads separate from other workloads in the management cluster.
//lahinson - sept. 2023 - commenting out the following lines until those levels are supported for self-managed hypershift
//* To ensure that control plane workloads are configured at one of the following multi-tenancy distribution levels:
//** Everything shared: Control planes for hosted clusters can run on any node that is designated for control planes.
//** Request serving isolation: Serving pods are requested in their own dedicated nodes.
//** Nothing shared: Every control plane has its own dedicated nodes.

*Important:* Do not use the management cluster for your workload. Workloads must not run on nodes where control planes run.

[#hosted-cluster-labels-taints-overview]
== Labels and taints for management cluster nodes

As a management cluster administrator, use the following labels and taints in management cluster nodes to schedule a control plane workload:

* `hypershift.openshift.io/control-plane: true`: Use this label and taint to dedicate a node to running hosted control plane workloads. By setting `true`, you avoid sharing the control plane nodes with other components.
* `hypershift.openshift.io/cluster: <hosted-control-plane-namespace>`: Use this label and taint when you want to dedicate a node to a single hosted cluster.

Apply the following labels on the nodes that host control-plane pods:

* `node-role.kubernetes.io/infra`: Use this label to avoid having the control-plane workload count toward your subscription.
* `topology.kubernetes.io/zone`: Use this label on the management cluster nodes to deploy highly available clusters across failure domains. The zone might be a location, rack name, or the hostname of the node where the zone is set.

To use each rack as an availability zone for management cluster nodes, enter the following command:

+
----
oc label node/<management_node1_name> node/<management_node2_name> topology.kubernetes.io/zone=<rack_name>
----

Pods for a hosted cluster have tolerations, and the scheduler uses affinity rules to schedule them. The scheduler prioritizes the scheduling of pods to nodes that are labeled with `hypershift.openshift.io/control-plane` and `hypershift.openshift.io/cluster: <hosted_control_plane_namespace>`.

For the `ControllerAvailabilityPolicy` option, use `HighlyAvailable`, which is the default value that the hosted control planes command line interface, `hcp`, deploys. When you use that option, you can schedule pods for each deployment within a hosted cluster across different failure domains by setting `topology.kubernetes.io/zone` as the topology key. Control planes that are not highly available are not supported.

[#hosted-cluster-schedule-pods-infra-nodes]
== Labeling nodes for hosted clusters

*Important:* You must add labels to nodes before deploying hosted control planes.

To schedule pods running in a hosted cluster to infrastructure nodes, add the `role.kubernetes.io/infra: ""` label in the `HostedCluster` custom resource (CR). See the following example:

[source,yaml]
----
  spec:
    nodeSelector:
      role.kubernetes.io/infra: ""
----

[#hosted-cluster-workload-distributing-priority]
== Priority classes

Four built-in priority classes influence the priority and preemption of the hosted cluster pods. You can create the pods in the management cluster in the following order from highest to lowest:

* `hypershift-operator`: HyperShift Operator pods.
* `hypershift-etcd`: Pods for etcd.
* `hypershift-api-critical`: Pods that are required for API calls and resource admission to succeed. This priority class include pods such as `kube-apiserver`, aggregated API servers, and web hooks.
* `hypershift-control-plane`: Pods in the control plane that are not API-critical but still need elevated priority, such as the cluster version Operator.

[#hosted-cluster-workload-distributing-additional-resources]
== Additional resources

For more information about hosted control planes, see the following topics:

* xref:../hosted_control_planes/bm_intro.adoc#configuring-hosting-service-cluster-configure-bm[Configuring hosted control plane clusters on bare metal]
* xref:../hosted_control_planes/kubevirt_intro.adoc#hosted-control-planes-manage-kubevirt[Managing hosted control plane clusters on OpenShift Virtualization]
* xref:../hosted_control_planes/aws_intro.adoc#hosting-service-cluster-configure-aws[Configuring hosted control plane clusters on AWS]
