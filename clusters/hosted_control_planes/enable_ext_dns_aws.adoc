[#hosted-enable-ext-dns-aws]
= Enabling external DNS

Because the control plane and the data plane are separate in hosted control planes, you can configure DNS in two independent areas:

- Ingress for workloads within the hosted cluster, such as the following domain: `*.apps.service-consumer-domain.com`

- Ingress for service endpoints within the management cluster, such as API or OAUTH endpoints through the service provider domain: `*.service-provider-domain.com`

The input for the `hostedCluster.spec.dns` dictates the Ingress for workloads within the hosted cluster. The input for `hostedCluster.spec.services.servicePublishingStrategy.route.hostname` dictates the Ingress for service endpoints within the management cluster.

External DNS creates name records for hosted cluster `Services` that specify a publishing type of `LoadBalancer` or `Route` and provide a hostname for that publishing type. For hosted clusters with `Private` or `PublicAndPrivate` endpoint access types, only the `APIServer` and `OAuth` services support hostnames. For `Private` hosted clusters, the DNS record resolves to a private IP of a Virtual Private Cloud (VPC) Endpoint in your VPC.

A hosted control plane exposes four services:

* `APIServer`
* `OAuthServer`
* `Konnectivity`
* `Ignition`

Each of those services is exposed by using `servicePublishingStrategy` in the `HostedCluster` specification. By default, for the `LoadBalancer` and `Route` types of `servicePublishingStrategy`, you publish the service in one of two ways: 

* By using the hostname of the load balancer that is in the status of the `Service` with the `LoadBalancer` type
* In the `status.host` field of the `Route`

However, when you deploy hosted control planes in a managed service context, those methods can expose the Ingress subdomain of the underlying management cluster and limit options for the management cluster lifecycle and disaster recovery.

When a DNS indirection is layered on the `LoadBalancer` and `Route` publishing types, a managed service operator can publish all public hosted cluster services by using a service-level domain. This architecture allows remapping on the DNS name to a new `LoadBalancer` or `Route` and does not expose the Ingress domain of the management cluster. Hosted control planes uses external DNS to achieve that indirection layer.

You can deploy `external-dns` alongside the `hypershift` Operator in the `hypershift` namespace of the management cluster. The external DNS watches for `Services` or `Routes` that have the `external-dns.alpha.kubernetes.io/hostname` annotation. That annotation is used to create a DNS record that points to the `Service`, such as a record, or the `Route`, such as a CNAME record.

[#external-dns-prereqs-aws]
== Prerequisites

Before you can set up external DNS for hosted control planes, you must meet the following prerequisites:

* An external public domain that you can point to

* Access to the AWS Route53 Management console

[#set-up-external-dns-aws]
== Setting up external DNS for hosted control planes

If you plan to provision hosted control plane clusters with service-level DNS (external DNS), complete the following steps:

. Create an AWS credential secret for the HyperShift Operator and name it `hypershift-operator-external-dns-credentials` in the `local-cluster` namespace.

. See the following table to verify that the secret has the required fields:

+
|===
| Field name | Description | Optional or required

| `provider`
| The DNS provider that manages the service-level DNS zone.
| Required

| `domain-filter`
| The service-level domain.
| Required

| `credentials`
| The credential file that supports all external DNS types.
| Optional when you use AWS keys

| `aws-access-key-id`
| The credential access key id.
| Optional when you use the AWS DNS service

| `aws-secret-access-key`
| The credential access key secret.
| Optional when you use the AWS DNS service
|===

+
The following example shows the sample `hypershift-operator-external-dns-credentials` secret template:

+
----
oc create secret generic hypershift-operator-external-dns-credentials --from-literal=provider=aws --from-literal=domain-filter=service.my.domain.com --from-file=credentials=<credentials-file> -n local-cluster
----

+
*Note:* Disaster recovery backup for the secret is not automatically enabled. To add the label that enables the `hypershift-operator-external-dns-credentials` secret to be backed up for disaster recovery, enter the following command:

+
----
oc label secret hypershift-operator-external-dns-credentials -n local-cluster cluster.open-cluster-management.io/backup=""
----

[#create-public-dns-hosted-zone-aws]
== Creating the public DNS hosted zone

You can create the public DNS hosted zone to use as the external DNS domain-filter in the AWS Route 53 management console:

. In the Route 53 management console, click *Create hosted zone*.

. On the *Hosted zone configuration* page, type a domain name, verify that *Publish hosted zone* is selected as the type, and click *Create hosted zone*.

. After the zone is created, on the *Records* tab, note the values in the *Value/Route traffic to* column.

. In the main domain, create an NS record to redirect the DNS requests to the delegated zone. In the *Value* field, enter the values that you noted in the previous step.

. Click *Create records*.

. Verify that the DNS hosted zone is working by creating a test entry in the new subzone and testing it with a `dig` command like the following example:

+
----
dig +short test.user-dest-public.aws.kerberos.com
192.168.1.1
----

. To create a hosted cluster that sets the hostname for `LoadBalancer` and `Route` services, enter the following command, where `external-dns-domain` matches the public hosted zone that you created:

+
----
hypershift create cluster aws --name=example --endpoint-access=PublicAndPrivate --external-dns-domain=service-provider-domain.com ...
----

This example shows the resulting `services` block for the hosted cluster:

[source,yaml]
----
  platform:
    aws:
      endpointAccess: PublicAndPrivate
...
  services:
  - service: APIServer
    servicePublishingStrategy:
      route:
        hostname: api-example.service-provider-domain.com
      type: Route
  - service: OAuthServer
    servicePublishingStrategy:
      route:
        hostname: oauth-example.service-provider-domain.com
      type: Route
  - service: Konnectivity
    servicePublishingStrategy:
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
----

When the Control Plane Operator creates the `Services` and `Routes`, it annotates them with the `external-dns.alpha.kubernetes.io/hostname` annotation. The value is the `hostname` field in the `servicePublishingStrategy` for that type. The Control Plane Operator uses that name for the service endpoints, and it expects that if the hostname is set, a mechanism exists, such as external-dns or otherwise, which can create the DNS records.

Only public services can have service-level DNS indirection. Private services use the `hypershift.local` private zone, and it is not valid to set `hostname` for services that are private for a given endpoint access type. 

The following table notes when it is valid to set `hostname` for a service and endpoint combination:

|===
|Service |Public |PublicAndPrivate |Private

|`APIServer`
|Y
|Y
|N

|`OAuthServer`
|Y
|Y
|N

|`Konnectivity`
|Y
|N
|N

|`Ignition`
|Y
|N
|N
|===

[#deploy-cluster-cli-external-dns-aws]
== Deploying a cluster by using the command line interface and external DNS

You need to deploy the `hypershift` and `external-dns` Operators when the external public hosted zone already exists. Ensure that the `external-dns` Operator is running and that the internal flags point to the public hosted zone by entering the following commands:

----
export KUBECONFIG=<path_to_management_cluster_kubeconfig>
export AWS_CREDS=~/.aws/credentials
export REGION=<region>

hypershift create cluster aws \
    --aws-creds ${AWS_CREDS} \
    --instance-type m6i.xlarge \
    --region ${REGION} \
    --auto-repair \
    --generate-ssh \
    --name <cluster_name> \
    --namespace clusters \
    --base-domain service-consumer-domain.com \ <1>
    --node-pool-replicas 2 \
    --pull-secret ${HOME}/pull_secret.json \
    --release-image quay.io/openshift-release-dev/ocp-release:4.12.0-ec.3-x86_64 \
    --external-dns-domain=service-provider-domain.com \ <2>
    --endpoint-access=PublicAndPrivate <3>
----

<1> Points to the public hosted zone, `service-consumer-domain.com`, which is typically in an AWS account that the service consumer owns.
<2> Points to the public external DNS hosted zone, `service-provider-domain.com`, which is typically in an AWS account that the service provider owns.
<3> Set as `PublicAndPrivate.` The external DNS can be used with only `Public` or `PublicAndPrivate` configurations.