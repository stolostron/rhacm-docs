[#manage-nodepools-hosted-kubevirt]
= Configuring additional networks, guaranteed CPUs, and VM scheduling for node pools

If you need to configure additional networks for node pools, request a guaranteed CPU access for Virtual Machines (VMs), or manage scheduling of KubeVirt VMs, see the following procedures.

[#add-multiple-networks-nodepool]
== Adding multiple networks to a node pool

By default, nodes generated by a node pool are attached to the pod network. You can attach additional networks to the nodes by using Multus and NetworkAttachmentDefinitions.

. To add multiple networks to nodes, use the `--additional-network` argument by running the following command:
+
[source,bash]
----
hcp create cluster kubevirt \
--name <hosted_cluster_name> \ <1>
--node-pool-replicas <worker_node_count> \ <2>
--pull-secret <path_to_pull_secret> \ <3>
--memory <memory> \ <4>
--cores <cpu> \ <5>
--additional-network name:<namespace/name> \ <6>
–-additional-network name:<namespace/name>
----
+
<1> Specify the name of your hosted cluster, for instance, `example`.
<2> Specify your worker node count, for example, `2`.
<3> Specify the path to your pull secret, for example, `/user/name/pullsecret`.
<4> Specify the memory value, for example, `8Gi`.
<5> Specify the CPU value, for example, `2`.
<6> Set the value of the `–additional-network` argument to `name:<namespace/name>`. Replace `<namespace/name>` with a namespace and name of your NetworkAttachmentDefinitions.

[#use-additional-default-network]
=== Using an additional network as default

You can add your additional network as a default network for the nodes by disabling the default pod network.

. To add an additional network as default to your nodes, run the following command:
+
[source,bash]
----
hcp create cluster kubevirt \
--name <hosted_cluster_name> \ <1>
--node-pool-replicas <worker_node_count> \ <2>
--pull-secret <path_to_pull_secret> \ <3>
--memory <memory> \ <4>
--cores <cpu> \ <5>
--attach-default-network false \ <6>
--additional-network name:<namespace>/<network_name> <7>
----
+
<1> Specify the name of your hosted cluster, for instance, `example`.
<2> Specify your worker node count, for example, `2`.
<3> Specify the path to your pull secret, for example, `/user/name/pullsecret`.
<4> Specify the memory value, for example, `8Gi`.
<5> Specify the CPU value, for example, `2`.
<6> The `--attach-default-network false` argument disables the default pod network.
<7> Specify the additional network that you want to add to your nodes, for example, `name:my-namespace/my-network`.

[#request-guaranteed-cpus]
== Requesting guaranteed CPU resources

By default, KubeVirt VMs might share its CPUs with other workloads on a node. This might impact performance of a VM. To avoid the performance impact, you can request a guaranteed CPU access for VMs.

. To request guaranteed CPU resources, set the `--qos-class` argument to `Guaranteed` by running the following command:
+
[source,bash]
----
hcp create cluster kubevirt \
--name <hosted_cluster_name> \ <1>
--node-pool-replicas <worker_node_count> \ <2>
--pull-secret <path_to_pull_secret> \ <3>
--memory <memory> \ <4>
--cores <cpu> \ <5>
--qos-class Guaranteed <6>
----
+
<1> Specify the name of your hosted cluster, for instance, `example`.
<2> Specify your worker node count, for example, `2`.
<3> Specify the path to your pull secret, for example, `/user/name/pullsecret`.
<4> Specify the memory value, for example, `8Gi`.
<5> Specify the CPU value, for example, `2`.
<6> The `--qos-class Guaranteed` argument guarantees that the specified number of CPU resources are assigned to VMs.

[#schedule-vms-hosted-nodepool]
== Scheduling KubeVirt VMs on a set of nodes

By default, KubeVirt VMs created by a node pool are scheduled to any available nodes. You can schedule KubeVirt VMs on a specific set of nodes that has enough capacity to run the VM.

. To schedule KubeVirt VMs within a node pool on a specific set of nodes, use the `--vm-node-selector` argument by running the following command:
+
[source,bash]
----
hcp create cluster kubevirt \
--name <hosted_cluster_name> \ <1>
--node-pool-replicas <worker_node_count> \ <2>
--pull-secret <path_to_pull_secret> \ <3>
--memory <memory> \ <4>
--cores <cpu> \ <5>
--vm-node-selector <label_key>=<label_value>,<label_key>=<label_value> <6>
----
+
<1> Specify the name of your hosted cluster, for instance, `example`.
<2> Specify your worker node count, for example, `2`.
<3> Specify the path to your pull secret, for example, `/user/name/pullsecret`.
<4> Specify the memory value, for example, `8Gi`.
<5> Specify the CPU value, for example, `2`.
<6> The `--vm-node-selector` flag defines a specific set of nodes that contains the key-value pairs. Replace `<label_key>` and `<label_value>` with the key and value of your labels respectively.
