[#creating-a-hosted-cluster-bm]
= Creating a hosted cluster on bare metal

You can create a hosted cluster or import one. For instructions to import a hosted cluster, see _Importing a hosted cluster_.

. Verify that you have a default storage class configured for your cluster. Otherwise, you might end up with pending PVCs. Enter the following commands, replacing any example variables with your information:

+
----
export CLUSTERS_NAMESPACE="clusters"
export HOSTED_CLUSTER_NAME="example"
export HOSTED_CONTROL_PLANE_NAMESPACE="${CLUSTERS_NAMESPACE}-${HOSTED_CLUSTER_NAME}" <1>
export BASEDOMAIN="krnl.es"
export PULL_SECRET_FILE=$PWD/pull-secret
export MACHINE_CIDR=192.168.122.0/24
export ETCD_STORAGE="lvm-storageclass"
export OCP_RELEASE=4.14.0-multi       
export SSH_KEY=${HOME}/.ssh/id_rsa.pub
oc create ns ${HOSTED_CONTROL_PLANE_NAMESPACE}

hcp create cluster agent \
    --name=${HOSTED_CLUSTER_NAME} \
    --pull-secret=${PULL_SECRET_FILE} \
    --agent-namespace=${HOSTED_CONTROL_PLANE_NAMESPACE} \
    --base-domain=${BASEDOMAIN} \
    --api-server-address=api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN} \
    --etcd-storage-class=${ETCD_STORAGE} \
    --ssh-key  ${SSH_KEY} \
    --namespace ${CLUSTERS_NAMESPACE} \
    --control-plane-availability-policy SingleReplica \
    --release-image=quay.io/openshift-release-dev/ocp-release:${OCP_RELEASE} \
    --api-server-address=api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN}
----

+
<1> Typically, the namespace is created by the HyperShift Operator, but agent cluster creation generates a cluster API provider role that needs the namespace to already exist.

. After a few moments, verify that your hosted control plane pods are up and running by entering the following command:

+
----
oc -n ${HOSTED_CONTROL_PLANE_NAMESPACE} get pods
----

+
See the following example output:

+
----
NAME                                             READY   STATUS    RESTARTS   AGE
capi-provider-7dcf5fc4c4-nr9sq                   1/1     Running   0          4m32s
catalog-operator-6cd867cc7-phb2q                 2/2     Running   0          2m50s
certified-operators-catalog-884c756c4-zdt64      1/1     Running   0          2m51s
cluster-api-f75d86f8c-56wfz                      1/1     Running   0          4m32s
cluster-autoscaler-7977864686-2rz4c              1/1     Running   0          4m13s
cluster-network-operator-754cf4ffd6-lwfm2        1/1     Running   0          2m51s
cluster-policy-controller-784f995d5-7cbrz        1/1     Running   0          2m51s
cluster-version-operator-5c68f7f4f8-lqzcm        1/1     Running   0          2m51s
community-operators-catalog-58599d96cd-vpj2v     1/1     Running   0          2m51s
control-plane-operator-f6b4c8465-4k5dh           1/1     Running   0          4m32s
etcd-0                                           1/1     Running   0          4m13s
hosted-cluster-config-operator-c4776f89f-dt46j   1/1     Running   0          2m51s
ignition-server-7cd8676fc5-hjx29                 1/1     Running   0          4m22s
ingress-operator-75484cdc8c-zhdz5                1/2     Running   0          2m51s
konnectivity-agent-c5485c9df-jsm9s               1/1     Running   0          4m13s
konnectivity-server-85dc754888-7z8vm             1/1     Running   0          4m13s
kube-apiserver-db5fb5549-zlvpq                   3/3     Running   0          4m13s
kube-controller-manager-5fbf7b7b7b-mrtjj         1/1     Running   0          90s
kube-scheduler-776c59d757-kfhv6                  1/1     Running   0          3m12s
machine-approver-c6b947895-lkdbk                 1/1     Running   0          4m13s
oauth-openshift-787b87cff6-trvd6                 2/2     Running   0          87s
olm-operator-69c4657864-hxwzk                    2/2     Running   0          2m50s
openshift-apiserver-67f9d9c5c7-c9bmv             2/2     Running   0          89s
openshift-controller-manager-5899fc8778-q89xh    1/1     Running   0          2m51s
openshift-oauth-apiserver-569c78c4d-568v8        1/1     Running   0          2m52s
packageserver-ddfffb8d7-wlz6l                    2/2     Running   0          2m50s
redhat-marketplace-catalog-7dd77d896-jtxkd       1/1     Running   0          2m51s
redhat-operators-catalog-d66b5c965-qwhn7         1/1     Running   0          2m51s
----

[#hosted-create-bare-metal-console]
== Creating a hosted cluster on bare metal by using the console

. Open the {ocp-short} web console and log in by entering your administrator credentials. For instructions to open the console, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/web_console/web-console[Accessing the web console] in the {ocp-short} documentation.

. In the console header, ensure that **All Clusters** is selected.

. Click **Infrastructure -> Clusters**.

. Click **Create cluster -> Host inventory -> Hosted control plane**.

+
The **Create cluster** page is displayed.

. On the **Create cluster** page, follow the prompts to enter details about the cluster, node pools, networking, and automation.

+
*Note:* As you enter details about the cluster, you might find the following tips useful:

** If you want to use predefined values to automatically populate fields in the console, you can create a host inventory credential. For more information, see _Creating a credential for an on-premises environment_.

** On the *Cluster details* page, the pull secret is your {ocp-short} pull secret that you use to access {ocp-short} resources. If you selected a host inventory credential, the pull secret is automatically populated.

** On the *Node pools* page, the namespace contains the hosts for the node pool. If you created a host inventory by using the console, the console creates a dedicated namespace.

** On the *Networking* page, you select an API server publishing strategy. The API server for the hosted cluster can be exposed either by using an existing load balancer or as a service of the `NodePort` type. A DNS entry must exist for the `api.${HOSTED_CLUSTER_NAME}.${BASEDOMAIN}` setting that points to the destination where the API server can be reached. This entry can be a record that points to one of the nodes in the management cluster or a record that points to a load balancer that redirects incoming traffic to the Ingress pods.

. Review your entries and click **Create**.

+
The **Hosted cluster** view is displayed.

. Monitor the deployment of the hosted cluster in the **Hosted cluster** view. If you do not see information about the hosted cluster, ensure that **All Clusters** is selected, and click the cluster name.

. Wait until the control plane components are ready. This process can take a few minutes.

. To view the node pool status, scroll to the **NodePool** section. The process to install the nodes takes about 10 minutes. You can also click **Nodes** to confirm whether the nodes joined the hosted cluster.

[#hosted-create-bare-metal-additional-resources]
== Additional resources

* To create credentials that you can reuse when you create a hosted cluster with the console, see xref:../credentials/credential_on_prem.adoc#creating-a-credential-for-an-on-premises-environment[Creating a credential for an on-premises environment].

* To import a hosted cluster, see xref:../hosted_control_planes/import_hosted_cluster.adoc#importing-hosted-cluster[Manually importing a hosted control plane cluster].

* To access a hosted cluster, see xref:../hosted_control_planes/access_hosted_cluster.adoc#access-hosted-cluster[Accessing the hosted cluster].