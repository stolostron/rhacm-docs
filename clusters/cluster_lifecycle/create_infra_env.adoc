[#creating-an-infrastructure-environment]
= Creating an infrastructure environment
//iniline links need to be reduced in this file, need to add Additional resources section, plus modularize| MJ | 07/27/23
You can use the console to create an infrastructure environment to manage your hosts and create clusters on those hosts.

Infrastructure environments support the following features:

- Zero Touch Provisioning of clusters: Deploy clusters using a script. See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/scalability_and_performance/index#installing-disconnected-rhacm_ztp-preparing-the-hub-cluster[Installing GitOps ZTP in a disconnected environment] in the {ocp} documentation for more information.
- Late binding: Add a node to an existing cluster. An infrastructure administrator can boot the host, and the cluster creator can bind the host to the existing cluster at a later time. Administrator privileges are not required for the cluster creator to access the infrastructure when using late binding.
- Dual stack: Deploy clusters that have both IPv4 and IPv6 addresses. Dual stack uses the `OVN-Kubernetes` networking implementation to support multiple subnets. 
- Add remote worker nodes: Add remote worker nodes to your clusters after they are created and running, which provides flexibility of adding nodes in other locations for backup purposes.
- Static IP using NMState: Use the NMState API to define static IP addresses for your environment.
- Specify the infrastructure environment kernel arguments by editing the `InfraEnv` custom resource, which is required in some environments. See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html/scalability_and_performance/clusters-at-the-network-far-edge#setting-managed-bare-metal-host-kernel-arguments_ztp-manual-install[Configuring the Discovery ISO kernel arguments for installations by using GitOps ZTP] for the instructions.
- Specify your partition storage layout in a `custom-partitions.yaml` file when you deploy a bare metal cluster. See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html-single/installing/index#configuring-storage-on-nodes_ipi-install-installation-workflow[Optional: Configuring storage on nodes] in the {ocp-short} documentation.

Continue reading the documentation for details. 

* <<infra-env-prerequisites,Prerequisites>>
* <<enable-cim,Enable Central Infrastructure Management service>>
** <<manually-create-cr,Manually create the Provisioning custom resource>>
** <<enable-cim-aws,Enable Central Infrastructure Management on Amazon Web Services>>
* <<creating-your-infra-env-with-the-console,Creating your infrastructure environment with the console>>
* <<accessing-your-infra-env,Accessing your infrastructure environment>>
* <<import-ocp-cluster-infra-env,Importing an {ocp-short} cluster>>
** <<add-nodes-ocp-infra-env,Add worker nodes to {ocp-short} clusters>>

[#infra-env-prerequisites]
== Prerequisites

See the following prerequisites before creating an infrastructure environment:

- You must have a deployed hub cluster on {ocp-short} version 4.11 or later.
- You need Internet access for your hub cluster (connected), or a connection to an internal or mirror registry that has a connection to the Internet (disconnected) to retrieve the required images for creating the environment.
- You need a configured instance of the Central Infrastructure Management (CIM) feature on your hub cluster.
- You must have the Bare Metal Operator installed on your hub cluster.
- You need an {ocp-short} link:https://console.redhat.com/openshift/install/pull-secret[pull secret]. See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html/images/managing-images#using-image-pull-secrets[Using image pull secrets] for more information. 
- You need your SSH key that is in your `id_rsa.pub` file, by default. View the following file path: `~/.ssh/id_rsa.pub`.
- You need a configured storage class.
- For disconnected environments only, complete the procedure for link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html/scalability_and_performance/clusters-at-the-network-far-edge#ztp-acm-preparing-to-install-disconnected-acm_ztp-deploying-disconnected[Clusters at the network far edge] in the {ocp-short} documentation.

[#enable-cim]
== Enabling the Central Infrastructure Management service

The CIM service is provided with the {mce-short} and deploys {ocp-short} clusters. CIM is deployed when you enable the MultiClusterHub Operator on the hub cluster, but must be enabled.

To enable the CIM service, complete the following steps: 

. This step only applies if your hub cluster is installed on one of the following platforms: bare metal, Red Hat OpenStack Platform, VMware vSphere, or was installed by using the user-provisioned infrastructure (UPI) method and the platform is `None`. Skip this step if your hub cluster is on any other platform. 

. Modify the `Provisioning` resource to allow the Bare Metal Operator to watch all namespaces by running the following command:
+
----
oc patch provisioning provisioning-configuration --type merge -p '{"spec":{"watchAllNamespaces": true }}'
----

. For connected environments, continue to step 4.ii. For disconnected environments, create a `ConfigMap` in the same namespace as your infrastructure operator to specify the values for `ca-bundle.crt` and `registries.conf` for your mirror registry. Your file `ConfigMap` should resemble the following example:

+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: <mirror-config>
  namespace: "<infrastructure-operator-namespace>"
  labels:
    app: assisted-service
data:
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    certificate contents
    -----END CERTIFICATE-----

  registries.conf: |
    unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

    [[registry]]
       prefix = ""
       location = "registry.redhat.io/multicluster-engine"
       mirror-by-digest-only = true

       [[registry.mirror]]
       location = "mirror.registry.com:5000/multicluster-engine"
----
+
Registries in the list of `unqualified-search-registries` are automatically added to an authentication ignore list in the `PUBLIC_CONTAINER_REGISTRIES` environment variable. The specified registries do not require authentication when the pull secret of the managed cluster is validated.
+
. Create the `AgentServiceConfig` custom resource by completing the following steps:
+
*Important:* If you are using the late binding feature and the `spec.osImages` releases in the `AgentServiceConfig` custom resource are version 4.13 or later, the {ocp-short} release images that you use when creating your clusters must be version 4.13 or later. The Red Hat Enterprise Linux CoreOS images for version 4.13 and later are not compatible with images earlier than version 4.13.

.. For disconnected environments only, save the following `YAML` content in the `agent_service_config.yaml` file:
+
[source,yaml]
----
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
 name: agent
spec:
  databaseStorage:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: <db_volume_size> 
  filesystemStorage:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: <fs_volume_size>
  mirrorRegistryRef:
    name: <mirror_config> <1>
  unauthenticatedRegistries:
    - <unauthenticated_registry> <2>
  imageStorage:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: <img_volume_size> <3>
  osImages: 
    - openshiftVersion: "<ocp_version>" <4>
      version: "<ocp_release_version>" <5>
      url: "<iso_url>" <6>
      cpuArchitecture: "x86_64"
----
+
<1> Replace `mirror_config` with the name of the `ConfigMap` that contains your mirror registry configuration details.
+
<2> Include the optional `unauthenticated_registry` parameter if you are using a mirror registry that does not require authentication. Entries on this list are not validated or required to have an entry in the pull secret. 
+
<3> Replace `img_volume_size` with the size of the volume for the `imageStorage` field, for example `10Gi` per operating system image. The minimum value is `10Gi`, but the recommended value is at least `50Gi`. This value specifies how much storage is allocated for the images of the clusters. You need to allow 1 GB of image storage for each instance of Red Hat Enterprise Linux CoreOS that is running. You might need to use a higher value if there are many clusters and instances of Red Hat Enterprise Linux CoreOS.
+
<4> Replace `ocp_version` with the {ocp-short} version to install, for example, `4.13`.
+
<5> Replace `ocp_release_version` with the specific install version, for example, `49.83.202103251640-0`.
+
<6> Replace `iso_url` with the ISO url, for example, `https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.10/4.10.3/rhcos-4.10.3-x86_64-live.x86_64.iso`. You can find other values at the link:https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.10/4.10.3/[4.10.3 dependencies].

.. For connected environments only, save the following `YAML` content in the `agent_service_config.yaml` file:
+
[source,yaml]
----
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
 name: agent
spec:
  databaseStorage:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: <db_volume_size> <1>
  filesystemStorage:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: <fs_volume_size> <2>
  imageStorage:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: <img_volume_size> <3>
----
+
<1> Replace `db_volume_size` with the volume size for the `databaseStorage` field, for example `1Gi`. This value specifies how much storage is allocated for storing files such as database tables and database views for the clusters. The minimum value that is required is `1Gi`. You might need to use a higher value if there are many clusters.
+
<2> Replace `fs_volume_size` with the size of the volume for the `filesystemStorage` field, for example `200M` per cluster and `2-3Gi` per supported {ocp-short} version. The minimum value that is required is `1Gi`, but the recommended value is at least `100Gi`. This value specifies how much storage is allocated for storing logs, manifests, and `kubeconfig` files for the clusters. You might need to use a higher value if there are many clusters. 
+
<3> Replace `img_volume_size` with the size of the volume for the `imageStorage` field, for example `10Gi` per operating system image. The minimum value is `10Gi`, but the recommended value is at least `50Gi`. This value specifies how much storage is allocated for the images of the clusters. You need to allow 1 GB of image storage for each instance of Red Hat Enterprise Linux CoreOS that is running. You might need to use a higher value if there are many clusters and instances of Red Hat Enterprise Linux CoreOS.

.. Create the AgentServiceConfig custom resource by running the following command:
+
----
oc create -f agent_service_config.yaml
----
+
The output might resemble the following example:
+
----
agentserviceconfig.agent-install.openshift.io/agent created
----

Your CIM service is configured. You can verify that it is healthy by checking the `assisted-service` and `assisted-image-service` deployments and ensuring that their pods are ready and running. 

[#manually-create-cr]
=== Manually create the Provisioning custom resource

Manually create a `Provisioning` custom resource to enable services for automated provisioning by using the following command:

----
oc create -f provisioning-configuration.yaml
----

Your custom resource might resemble the following sample:

[source,yaml]
----
apiVersion: metal3.io/v1alpha1
kind: Provisioning
metadata:
  name: provisioning-configuration
spec:
  provisioningNetwork: Disabled
  watchAllNamespaces: true
----

[#enable-cim-aws]
=== Enabling Central Infrastructure Management on Amazon Web Services

If you are running your hub cluster on Amazon Web Services and want to enable the CIM service, complete the following additional steps after <<enable-cim,Enabling CIM>>:

. Make sure you are logged in at the hub and find the unique domain configured on the `assisted-image-service` by running the following command:
+
----
oc get routes --all-namespaces | grep assisted-image-service
----
//do we get routes with the integrated console? A bug came out for other topics, could not get a review on this in time, please check next time this is edited. --bcs 6/23
+
Your domain might resemble the following example: `assisted-image-service-multicluster-engine.apps.<yourdomain>.com`

. Make sure you are logged in at the hub and create a new `IngressController` with a unique domain using the `NLB` `type` parameter. See the following example:
+
[source,yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: ingress-controller-with-nlb
  namespace: openshift-ingress-operator
spec:
  domain: nlb-apps.<domain>.com
  routeSelector:
      matchLabels:
        router-type: nlb
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: External
      providerParameters:
        type: AWS
        aws:
          type: NLB
----

. Add `<yourdomain>` to the `domain` parameter in `IngressController` by replacing `<domain>` in `nlb-apps.<domain>.com` with `<yourdomain>`.

. Apply the new `IngressController` by using the following command:
+
----
oc apply -f ingresscontroller.yaml
----

. Make sure that the value of the `spec.domain` parameter of the new `IngressController` is not in conflict with an existing `IngressController` by completing the following steps:

.. List all `IngressControllers` by running the following command:
+
----
oc get ingresscontroller -n openshift-ingress-operator
----

.. Run the following command on each of the `IngressControllers`, except the `ingress-controller-with-nlb` that you just created:
+
----
oc edit ingresscontroller <name> -n openshift-ingress-operator
----
+
If the `spec.domain` report is missing, add a default domain that matches all of the routes that are exposed in the cluster except `nlb-apps.<domain>.com`.
+
If the `spec.domain` report is provided, make sure that the `nlb-apps.<domain>.com` route is excluded from the specified range.

. Run the following command to edit the `assisted-image-service` route to use the `nlb-apps` location:
+
----
oc edit route assisted-image-service -n <namespace>
----
+
The default namespace is where you installed the {mce-short}.

. Add the following lines to the `assisted-image-service` route:
+
[source,yaml]
----
metadata:
  labels:
    router-type: nlb
  name: assisted-image-service
----

. In the `assisted-image-service` route, find the URL value of `spec.host`. The URL might resemble the following example: 

+
----
assisted-image-service-multicluster-engine.apps.<yourdomain>.com
----
+
. Replace `apps` in the URL with `nlb-apps` to match the domain configured in the new `IngressController`.

. To verify that the CIM service is enabled on Amazon Web Services, run the following command to verify that the pods are healthy:

+
----
oc get pods -n multicluster-engine | grep assist
----

. Create a new infrastructure environment and ensure that the download URL uses the new `nlb-apps` URL.

[#creating-your-infra-env-with-the-console]
== Creating your infrastructure environment with the console

To create an infrastructure environment from the console, complete the following steps:

. From the navigation menu, navigate to *Infrastructure* > *Host inventory* and click *Create infrastructure environment*.
. Add the following information to your infrastructure environment settings: 
+
- Name: A unique name for your infrastructure environment. 
- Network type: Specifies which types of hosts can be added to your infrastructure environment.
- Location: Specifies the geographic location of the host. The geographic location can be used to easily determine where your data on a cluster is stored when you are creating the cluster. 
- Labels: Optional field where you can add labels to the infrastructure environment so you can more easily find and group the environment with other environments that share a characteristic. The selections that you made for the network type and location are automatically added to the list of labels.
- Host inventory credential: If you have a host inventory credential, select the credential to automatically populate the fields with information in the credential.  
- Pull secret: Your {ocp-short} link:https://console.redhat.com/openshift/install/pull-secret[pull secret] that enables you to access the {ocp-short} resources. This field is automatically completed if you selected a host inventory credential.
+
- SSH public key: The SSH key that enables the secure communication with the hosts. This is generally in your `id_rsa.pub` file, by default. This field is automatically completed if you selected a host inventory credential that contained the value of an SSH public key. View the following file path:
+
[source,bash]
----
~/.ssh/id_rsa.pub
----

- If you want to enable proxy settings across all of your clusters, select the setting to enable it. This requires that you enter the following information: 
+
- HTTP Proxy URL: The URL that should be used when accessing the discovery service. 
+
- HTTPS Proxy URL: The secure proxy URL that should be used when accessing the discovery service. Note that the format must be `http`, as `https` is not yet supported. 
+
- No Proxy domains: A comma-separated list of domains that should bypass the proxy. Begin a domain name with a period `.` to include all of the subdomains that are in that domain. Add an asterisk `*` to bypass the proxy for all destinations. 

You can now continue by adding hosts to your infrastructure environment. 

[#accessing-your-infra-env]
== Accessing an infrastructure environment

To access an infrastructure environment, select *Infrastructure* > *Host inventory* in the console. Select your infrastructure environment from the list to view the details and hosts for that infrastructure environment.

[#import-ocp-cluster-infra-env]
== Importing an {ocp-short} cluster

You can import an existing {ocp-short} cluster so that you can add additional nodes. Complete the following steps to import an {ocp-short} cluster without a static network or a bare metal host, and prepare it for adding nodes:

. Create a namespace for the {ocp-short} cluster that you want to import by applying the following YAML content:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: managed-cluster
----

. Make sure that a ClusterImageSet matching the {ocp-short} cluster you are importing exists by applying the following YAML content:
+
[source,yaml]
----
apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  name: openshift-v4.11.18
spec:
  releaseImage: quay.io/openshift-release-dev/ocp-release@sha256:22e149142517dfccb47be828f012659b1ccf71d26620e6f62468c264a7ce7863
----

. Add your pull secret to access the image by applying the following YAML content:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
type: kubernetes.io/dockerconfigjson
metadata:
  name: pull-secret
  namespace: managed-cluster
stringData:
  .dockerconfigjson: <pull-secret-json> <1>
----
+
<1> Replace <pull-secret-json> with your pull secret JSON.

. Copy the `kubeconfig` from your {ocp-short} cluster to the hub cluster.

.. Get the `kubeconfig` from your {ocp-short} cluster by running the following command. Make sure that `kubeconfig` is set as the cluster being imported:
+
----
oc get secret -n openshift-kube-apiserver node-kubeconfigs -ojson | jq '.data["lb-ext.kubeconfig"]' --raw-output | base64 -d > /tmp/kubeconfig.some-other-cluster
----
+
.. Copy the `kubeconfig` to the hub cluster by running the following command. Make sure that `kubeconfig` is set as your hub cluster:
+
----
oc -n managed-cluster create secret generic some-other-cluster-admin-kubeconfig --from-file=kubeconfig=/tmp/kubeconfig.some-other-cluster
----

. Create an `AgentClusterInstall` custom resource by applying the following YAML content. Replace values where needed:
+
[source,yaml]
----
apiVersion: extensions.hive.openshift.io/v1beta1
kind: AgentClusterInstall
metadata:
  name: your-cluster-name <1>
  namespace: managed-cluster
spec:
  networking:
    userManagedNetworking: true
  clusterDeploymentRef:
    name: your-cluster
  imageSetRef:
    name: openshift-v4.11.18
  provisionRequirements:
    controlPlaneAgents: 1 <2>
  sshPublicKey: "" <3> 
----
+
<1> Choose a name for your cluster.
<2> Use `1` if you are using a {sno} cluster. Use `3` if you are using a multinode cluster.
<3> You can use the optional `sshPublicKey` field to log in to nodes for troubleshooting purposes.

. Create a `ClusterDeployment` by applying the following YAML content. Replace values where needed:
+
[source,yaml]
----
apiVersion: hive.openshift.io/v1
kind: ClusterDeployment
metadata:
  name: your-cluster-name <1>
  namespace: managed-cluster
spec:
  baseDomain: redhat.com <2>
  installed: true <3>
  clusterMetadata:
      adminKubeconfigSecretRef:
        name: your-cluster-name-admin-kubeconfig <4>
      clusterID: "" <5>
      infraID: "" <5>
  clusterInstallRef:
    group: extensions.hive.openshift.io
    kind: AgentClusterInstall
    name: your-cluster-name-install
    version: v1beta1
  clusterName: your-cluster-name
  platform:
    agentBareMetal:
  pullSecretRef:
    name: pull-secret
----
+
<1> Choose a name for your cluster.
<2> Make sure `baseDomain` matches the domain you are using for your {ocp-short} cluster.
<3> Your {ocp-short} cluster is only automatically imported as a production environment cluster if `installed` is set to `true`.
<4> Make sure that the `kubeconfig` you created in step 4 is referenced in `adminKubeconfigSecretRef`.
<5> In production environments, you can leave `clusterID` and `infraID` empty.

. Add an `InfraEnv` custom resource to discover new hosts to add to your cluster by applying the following YAML content. Replace values where needed:
+
*Note:* The following example might require additional configuration if you are not using a static IP address.
+
[source,yaml]
----
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: your-infraenv
  namespace: managed-cluster
spec:
  clusterRef: <1>
    name: your-cluster-name
    namespace: managed-cluster
  pullSecretRef:
    name: pull-secret
  sshAuthorizedKey: "" <2>
----
+
<1> The `clusterRef` field is optional if you are using late binding. If you are not using late binding, you must add the `clusterRef`, as seen in the example.
<2> You can use the optional `sshPublicKey` field to log in to nodes for troubleshooting.

. If the import is successful, a URL to download an ISO file appears. Download the ISO file by running the following command, replacing <url> with the URL that appears:
+
*Note:* You can automate host discovery by using bare metal host.
+
----
oc get infraenv -n managed-cluster some-other-infraenv -ojson | jq ".status.<url>" --raw-output | xargs curl -k -o /storage0/isos/some-other.iso
----

[#add-nodes-ocp-infra-env]
=== Add worker nodes to {ocp-short} clusters

Complete the following steps to add production environment worker nodes to {ocp-short} clusters:

. Boot the machine that you want to use as a worker node from the ISO you previously downloaded.
+
*Note:* Make sure that the worker node meets the requirements for an {ocp-short} worker node.

. Wait for an agent to register after running the following command:
+
----
watch -n 5 "oc get agent -n managed-cluster"
----

. If the agent registration is succesful, an agent is listed. Approve the agent for installation. This can take a few minutes.
+
*Note:* If the agent is not listed, exit the `watch` command by pressing Ctrl and C, then log in to the worker node to troubleshoot.

. If you are using late binding, run the following command to associate pending unbound agents with your {ocp-short} cluster. Skip to step 5 if you are not using late binding:
+
----
oc get agent -n managed-cluster -ojson | jq -r '.items[] | select(.spec.approved==false) |select(.spec.clusterDeploymentName==null) | .metadata.name'| xargs oc -n managed-cluster patch -p '{"spec":{"clusterDeploymentName":{"name":"some-other-cluster","namespace":"managed-cluster"}}}' --type merge agent
----

. Approve any pending agents for installation by running the following command:
+
----
oc get agent -n managed-cluster -ojson | jq -r '.items[] | select(.spec.approved==false) | .metadata.name'| xargs oc -n managed-cluster patch -p '{"spec":{"approved":true}}' --type merge agent
----

Wait for the installation of the worker node. When the worker node installation is complete, the worker node contacts the managed cluster with a Certificate Signing Request (CSR) to start the joining process. The CSR is automatically signed.
