[#import-ocp-cluster]
= Importing an on-premises {ocp} cluster manually by using {cim}

After you install {mce}, you are ready to import a cluster to manage. You can import an existing {ocp-short} cluster so that you can add additional nodes. Your hub cluster is automatically imported when you install {mce-short}, so you can add nodes to your hub cluster without completing the following procedure. Continue reading the following topics to learn more:

* <<import-ocp-cluster-prereqs,Prerequisites>>
* <<import-ocp-cluster-steps,Importing a cluster>>
* <<import-ocp-cluster-resources,Importing cluster resources>>

[#import-ocp-cluster-prereqs]
== Prerequisites

- Enable the {cim} feature.

[#import-ocp-cluster-steps]
== Importing a cluster

Complete the following steps to import an {ocp-short} cluster manually, without a static network or a bare metal host, and prepare it for adding nodes:

. Create a namespace for the {ocp-short} cluster that you want to import by applying the following YAML content:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: managed-cluster
----

. Make sure that a ClusterImageSet matching the {ocp-short} cluster you are importing exists by applying the following YAML content:
+
[source,yaml]
----
apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  name: openshift-v4.11.18
spec:
  releaseImage: quay.io/openshift-release-dev/ocp-release@sha256:22e149142517dfccb47be828f012659b1ccf71d26620e6f62468c264a7ce7863
----

. Add your pull secret to access the image by applying the following YAML content:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
type: kubernetes.io/dockerconfigjson
metadata:
  name: pull-secret
  namespace: managed-cluster
stringData:
  .dockerconfigjson: <pull-secret-json> <1>
----
+
<1> Replace <pull-secret-json> with your pull secret JSON.

. Copy the `kubeconfig` from your {ocp-short} cluster to the hub cluster.

.. Get the `kubeconfig` from your {ocp-short} cluster by running the following command. Make sure that `kubeconfig` is set as the cluster being imported:
+
----
oc get secret -n openshift-kube-apiserver node-kubeconfigs -ojson | jq '.data["lb-ext.kubeconfig"]' --raw-output | base64 -d > /tmp/kubeconfig.some-other-cluster
----
+
*Note:* If your cluster API is accessed through a custom domain, you must first edit this `kubeconfig` by adding your custom certificates in the `certificate-authority-data` field and by changing the `server` field to match your custom domain.
+
.. Copy the `kubeconfig` to the hub cluster by running the following command. Make sure that `kubeconfig` is set as your hub cluster:
+
----
oc -n managed-cluster create secret generic some-other-cluster-admin-kubeconfig --from-file=kubeconfig=/tmp/kubeconfig.some-other-cluster
----

. Create an `AgentClusterInstall` custom resource by applying the following YAML content. Replace values where needed:
+
[source,yaml]
----
apiVersion: extensions.hive.openshift.io/v1beta1
kind: AgentClusterInstall
metadata:
  name: <your-cluster-name> <1>
  namespace: <managed-cluster>
spec:
  networking:
    userManagedNetworking: true
  clusterDeploymentRef:
    name: <your-cluster>
  imageSetRef:
    name: openshift-v4.11.18
  provisionRequirements:
    controlPlaneAgents: <2>
  sshPublicKey: <""> <3> 
----
+
<1> Choose a name for your cluster.
<2> Use `1` if you are using a {sno} cluster. Use `3` if you are using a multinode cluster.
<3> Add the optional `sshPublicKey` field to log in to nodes for troubleshooting.

. Create a `ClusterDeployment` by applying the following YAML content. Replace values where needed:
+
[source,yaml]
----
apiVersion: hive.openshift.io/v1
kind: ClusterDeployment
metadata:
  name: <your-cluster-name> <1>
  namespace: managed-cluster
spec:
  baseDomain: <redhat.com> <2>
  installed: <true> <3>
  clusterMetadata:
      adminKubeconfigSecretRef:
        name: <your-cluster-name-admin-kubeconfig> <4>
      clusterID: <""> <5>
      infraID: <""> <5>
  clusterInstallRef:
    group: extensions.hive.openshift.io
    kind: AgentClusterInstall
    name: your-cluster-name-install
    version: v1beta1
  clusterName: your-cluster-name
  platform:
    agentBareMetal:
  pullSecretRef:
    name: pull-secret
----
+
<1> Choose a name for your cluster.
<2> Make sure `baseDomain` matches the domain you are using for your {ocp-short} cluster.
<3> Set to `true` to automatically import your {ocp-short} cluster  as a production environment cluster.
<4> Reference the `kubeconfig` you created in step 4.
<5> Leave `clusterID` and `infraID` empty in production environments.

. Add an `InfraEnv` custom resource to discover new hosts to add to your cluster by applying the following YAML content. Replace values where needed:
+
*Note:* The following example might require additional configuration if you are not using a static IP address.
+
[source,yaml]
----
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: your-infraenv
  namespace: managed-cluster
spec:
  clusterRef:
    name: your-cluster-name
    namespace: managed-cluster
  pullSecretRef:
    name: pull-secret
  sshAuthorizedKey: ""
----

.InfraEnv field table
|===
| Field | Optional or required | Description

| `clusterRef`
| Optional
| The `clusterRef` field is optional if you are using late binding. If you are not using late binding, you must add the `clusterRef`.

| `sshAuthorizedKey`
| Optional
| Add the optional `sshAuthorizedKey` field to log in to nodes for troubleshooting.
|===

. If the import is successful, a URL to download an ISO file appears. Download the ISO file by running the following command, replacing <url> with the URL that appears:
+
*Note:* You can automate host discovery by using bare metal host.
+
----
oc get infraenv -n managed-cluster some-other-infraenv -ojson | jq ".status.<url>" --raw-output | xargs curl -k -o /storage0/isos/some-other.iso
----

. *Optional:* If you want to use {acm-short} features, such as policies, on your {ocp-short} cluster, create a `ManagedCluster` resource. Make sure that the name of your `ManagedCluster` resource matches the name of your `ClusterDeplpoyment` resource. If you are missing the `ManagedCluster` resource, your cluster status is `detached` in the console.

[#import-ocp-cluster-resources]
== Importing cluster resources

If your {ocp-short} managed cluster was installed by the {ai}, you can move the managed cluster and its resources from one hub cluster to another hub cluster. 

*Important:* You can only scale down imported {ocp-short} managed clusters if they were installed by the {ai}.

You can import the following resources and continue to manage your cluster with them:

- `Agent`
- `AgentClassification`
- `AgentClusterInstall`
- `BareMetalHost`
- `ClusterDeployment`
- `InfraEnv`
- `NMStateConfig`

You can manage a cluster from a new hub cluster by creating a copy of the original resources and applying them to the new hub cluster. You can then scale down or scale up your managed cluster from the new hub cluster.

Import your cluster resources by creating a copy of your resources on your target hub cluster. See the following example to learn how to create a copy of the `Agent` resource and apply similar steps to your other resources:

. Get the `Agent` resource from your source hub cluster by running the following command. Replace values where needed:

+
[source,bash]
----
oc –kubeconfig <source_hub_kubeconfig> -n <managed_cluster_name> get agent <cluster_provisioning_namespace> -oyaml > agent.yaml
----

. Apply your `Agent` resource on your target hub cluster by running the following command. Replace values where needed:

+
[source,bash]
----
oc –kubeconfig <target_hub_kubeconfig> apply -f agent.yaml
----
