[#auto-add-host-host-inventory]
= Automatically adding bare metal hosts to the host inventory

After creating your host inventory (infrastructure environment) you can discover your hosts and add them to your inventory. You can automate booting the Discovery Image of your infrastructure environment by making the bare metal operator communicate with the Baseboard Management Controller (BMC) of each bare metal host by creating a `BareMetalHost` resource and associated BMC secret for each host. The automation is set by a label on the `BareMetalHost` that references your infrastructure environment.

The automation performs the following actions:

- Boots each bare metal host with the Discovery Image represented by the infrastructure environment
- Reboots each host with the latest Discovery Image in case the infrastructure environment or any associated network configurations is updated
- Associates each `Agent` resource with its corresponding `BareMetalHost` resource upon discovery
- Updates `Agent` resource properties based on information from the `BareMetalHost`, such as hostname, role, and installation disk
- Approves the `Agent` for use as a cluster node

[#auto-add-host-prereqs]
== Prerequisites

- You must enable the central infrastructure management service.
- You must create a host inventory.

[#auto-add-host-steps-console]
== Adding bare metal hosts by using the console

Complete the following steps to automatically add bare metal hosts to your host inventory by using the console:

. Select *Infrastructure* > *Host inventory* in the console.

. Select your infrastructure environment from the list.

. Click *Add hosts* and select *With BMC Form*.

. Add the required information and click *Create*.

[#auto-add-host-steps-cli]
== Adding bare metal hosts by using the command line interface

Complete the following steps to automatically add bare metal hosts to your host inventory by using the command line interface.

. Create a BMC secret by applying the following YAML content and replacing values where needed:
+
[source,YAML]
----
apiVersion: v1
kind: Secret
metadata:
  name: <bmc-secret-name>
  namespace: <your_infraenv_namespace> <1>
type: Opaque
data:
  username: <username>
  password: <password>
----
+
<1> The namespace must be the same as the namespace of your `InfraEnv`.

. Create a bare metal host by applying the following YAML content and replacing values where needed:
+
[source,YAML]
----
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: <bmh-name>
  namespace: <your_infraenv_namespace> <1>
  annotations:
    inspect.metal3.io: disabled
  labels:
    infraenvs.agent-install.openshift.io: <your-infraenv> <2>
spec:
  online: true
  automatedCleaningMode: disabled <3>
  bootMACAddress: <your-mac-address>  <4>
  bmc:
    address: <machine-address> <5>
    credentialsName: <bmc-secret-name> <6>
  rootDeviceHints:
    deviceName: /dev/sda <7>
----
<1> The namespace must be the same as the namespace of your `InfraEnv`.
<2> The name must match the name of your `InfrEnv` and exist in the same namespace.
<3> If you do not set a value, the `metadata` value is automatically used.
<4> Make sure the MAC address matches the MAC address of one of the interaces on your host.
<5> Use the address of the BMC. See _Port access for the out-of-band management IP address_ for more information.
<6> Make sure that the `credentialsName` value matches the name of the BMC secret you created.
<7> *Optional:* Select the installation disk. See _The BareMetalHost spec_ for the available root device hints. After the host is booted with the Discovery Image and the corresponding `Agent` resource is created, the installation disk is set according to this hint.

After turning on the host, the image starts downloading. This might take a few minutes. When the host is discovered, an `Agent` custom resource is created automatically.

[#auto-add-host-steps-converged]
== Disabling converged flow

Converged flow is enabled by default. If your hosts do not appear, you might need to temporarily disable converged flow. To disable converged flow, complete the following steps:

. Create the following config map on your hub cluster:

+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-assisted-service-config
  namespace: multicluster-engine
data:
  ALLOW_CONVERGED_FLOW: "false"
----
+
*Note:* When you set `ALLOW_CONVERGED_FLOW` to `"false"`, you also disable any features enabled by the Ironic Python Agent. 

. Apply the config map by running the following command:

+
[source,bash]
----
oc annotate --overwrite AgentServiceConfig agent unsupported.agent-install.openshift.io/assisted-service-configmap=my-assisted-service-config
----

[#auto-remove-host-steps-cli]
== Removing managed cluster nodes by using the command line interface

To remove managed cluster nodes from a managed cluster, you need a hub cluster that is running on a supported {ocp-short} version. Any static networking configuration required for the node to boot must be available. Make sure to not delete `NMStateConfig` resources when you delete the agent and bare metal host.

[#auto-remove-host-steps-bmh]
=== Removing managed cluster nodes with a bare metal host

If you have a bare metal host on your hub cluster and want remove managed cluster nodes from a managed cluster, complete the following steps:

. Add the following annotation to the `BareMetalHost` resource of the node that you want to delete:
+
----
bmac.agent-install.openshift.io/remove-agent-and-node-on-delete: true
----

. Delete the `BareMetalHost` resource by running the following command. Replace `<bmh-name>` with the name of your `BareMetalHost`:
+
----
oc delete bmh <bmh-name>
----

[#auto-remove-host-steps-no-bmh]
=== Removing managed cluster nodes without a bare metal host

If you do not have a bare metal host on your hub cluster and want remove managed cluster nodes from a managed cluster, follow the link:https://docs.openshift.com/container-platform/4.14/nodes/nodes/nodes-nodes-working.html#deleting-nodes[Deleting nodes] instructions in the {ocp-short} documentation.

[#additional-resources-auto-add-host]
== Additional resources

- For additional information about zero touch provisioning, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/scalability_and_performance/clusters-at-the-network-far-edge[Clusters at the network far edge] in the {ocp-short} documentation.

- To learn about the required ports for using a bare metal host, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.14/html/deploying_installer-provisioned_clusters_on_bare_metal/ipi-install-prerequisites#network-requirements-out-of-band_ipi-install-prerequisites[Port access for the out-of-band management IP address] in the {ocp-short} documentation.

- To learn about root device hints, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/postinstallation_configuration/post-install-bare-metal-configuration#post-install-bare-metal-configuration[Bare metal configuration] in the {ocp-short} documentation.

- See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/images/managing-images#using-image-pull-secrets[Using image pull secrets]

- See xref:../credentials/credential_on_prem.adoc#creating-a-credential-for-an-on-premises-environment[Creating a credential for an on-premises environment]

- To learn more about scaling compute machines, see link:https://docs.openshift.com/container-platform/4.14/machine_management/manually-scaling-machineset.html[Manually scaling a compute machine set] in the {ocp-short} documentation.

- To learn more about converged flow, see xref:../release_notes/known_issues.adoc#deploy-managed-stuck-pending[Managed cluster stuck in _Pending_ status after deployment].
