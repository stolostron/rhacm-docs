[#creating-a-cluster-on-premises]
= Creating a cluster in an on-premises environment

You can use the console to create an on-premises {ocp} cluster.

You can use this procedure to create single-node OpenShift (SNO) clusters, multi-node clusters, and compact three-node clusters on VMware vSphere, Red Hat OpenStack, Red Hat Virtualization Platform, and in a bare metal environment.

There is no platform integration with the platform where you install the cluster, as the platform value is set to `platform=none`. A single-node OpenShift cluster contains only a single node, which hosts the control plane services and the user workloads. This configuration can be helpful when you want to minimize the resource footprint of the cluster. 

You can also provision multiple single-node OpenShift clusters on edge resources by using the zero touch provisioning feature, which is a feature that is available with {ocp}. For more information about zero touch provisioning, see _Clusters at the network far edge_ in the {ocp-short} documentation.

* <<on-prem-prerequisites,Prerequisites>>
* <<on-prem-creating-your-cluster-with-the-console,Creating your cluster with the console>>

[#on-prem-prerequisites]
== Prerequisites

See the following prerequisites before creating a cluster in an on-premises environment:

* You must have a deployed hub cluster on {ocp-short} version 4.9, or later.
* You need a configured infrastructure environment with a host inventory of configured hosts. 
* You must have internet access for your hub cluster (connected), or a connection to an internal or mirror registry that has a connection to the internet (disconnected) to retrieve the required images for creating the cluster.
* You need a configured on-premises credential. 
* You need an {ocp-short} image pull secret. See _Using image pull secrets_.

[#on-prem-creating-your-cluster-with-the-console]
== Creating your cluster with the console

To create a cluster from the console, complete the following steps:

. Navigate to *Infrastructure* > *Clusters*.

. On the _Clusters_ page, click *Create cluster* and complete the steps in the console.

. Select *Host inventory* as the type of cluster.

The following options are available for your assisted installation: 

* *Use existing discovered hosts*: Select your hosts from a list of hosts that are in an existing host inventory.

* *Discover new hosts*: Discover hosts that are not already in an existing infrastructure environment. Discover your own hosts, rather than using one that is already in an infrastructure environment.

If you need to create a credential, see _Creating a credential for an on-premises environment_ for more information.

The name for your cluster is used in the hostname of the cluster.

*Important:* When you create a cluster, the controller creates a namespace for the cluster and its resources. Ensure that you include only resources for that cluster instance in that namespace. Destroying the cluster deletes the namespace and all of the resources in it.

*Tip:* Select *YAML: On* to view content updates as you enter the information in the console.

If you want to add your cluster to an existing cluster set, you must have the correct permissions on the cluster set to add it. If you do not have `cluster-admin` privileges when you are creating the cluster, you must select a cluster set on which you have `clusterset-admin` permissions. If you do not have the correct permissions on the specified cluster set, the cluster creation fails. Contact your cluster administrator to provide you with `clusterset-admin` permissions to a cluster set if you do not have any cluster set options to select.

Every managed cluster must be associated with a managed cluster set. If you do not assign the managed cluster to a `ManagedClusterSet`, it is automatically added to the `default` managed cluster set.

If there is already a base DNS domain that is associated with the selected credential that you configured for your provider account, that value is populated in that field. You can change the value by overwriting it, but this setting cannot be changed after the cluster is created. The base domain of your provider is used to create routes to your {ocp} cluster components. It is configured in the DNS of your cluster provider as a Start of Authority (SOA) record. 

The *OpenShift version* identifies the version of the {ocp-short} image that is used to create the cluster. If the version that you want to use is available, you can select the image from the list of images. If the image that you want to use is not a standard image, you can enter the URL to the image that you want to use. See _Release images_ to learn more.

When you select an OpenShift version that is 4.9 or later, an option to select *Install single-node OpenShift (SNO)* is displayed. A single-node OpenShift cluster contains a single node which hosts the control plane services and the user workloads. See _Scaling hosts to an infrastructure environment_ to learn more about adding nodes to a single-node OpenShift cluster after it is created. 

If you want your cluster to be a single-node OpenShift cluster, select the *single-node OpenShift* option. You can add additional workers to single-node OpenShift clusters by completing the following steps:

. From the console, navigate to *Infrastructure* > *Clusters* and select the name of the cluster that you created or want to access.

. Select *Actions* > *Add hosts* to add additional workers.

*Note:* The single-node OpenShift control plane requires 8 CPU cores, while a control plane node for a multinode control plane cluster only requires 4 CPU cores.  

After you review and save the cluster, your cluster is saved as a draft cluster. You can close the creation process and finish the process later by selecting the cluster name on the _Clusters_ page.

If you are using existing hosts, select whether you want to select the hosts yourself, or if you want them to be selected automatically. The number of hosts is based on the number of nodes that you selected. For example, a SNO cluster only requires one host, while a standard three-node cluster requires three hosts. 

The locations of the available hosts that meet the requirements for this cluster are displayed in the list of _Host locations_. For distribution of the hosts and a more high-availability configuration, select multiple locations.

If you are discovering new hosts with no existing infrastructure environment, complete the steps in xref:../cluster_lifecycle/scale_hosts_infra_env.adoc#scale-hosts-infrastructure-env[Scaling hosts to an infrastructure environment] beginning with step 4 to define your hosts.   

After the hosts are bound, and the validations pass, complete the networking information for your cluster by adding the following IP addresses: 

* API VIP: Specifies the IP address to use for internal API communication.
+
*Note:* This value must match the name that you used to create the DNS records listed in the prerequisites section. If not provided, the DNS must be pre-configured so that `api.` resolves correctly.

* Ingress VIP: Specifies the IP address to use for ingress traffic.
+
*Note:* This value must match the name that you used to create the DNS records listed in the prerequisites section. If not provided, the DNS must be pre-configured so that `test.apps.` resolves correctly.

If you are using {product-title} and want to configure your managed cluster klusterlet to run on specific nodes, see xref:../cluster_lifecycle/adv_config_cluster.adoc#create-cluster-configuring-nodeselector-tolerations[Optional: Configuring the klusterlet to run on specific nodes] for the required steps.

You can view the status of the installation on the _Clusters_ navigation page.

Continue with xref:../cluster_lifecycle/access_cluster.adoc#accessing-your-cluster[Accessing your cluster] for instructions for accessing your cluster. 

[#on-prem-boot-host-bmh]
== Booting a host address by using bare metal hosts for on-premise environments

You can automate booting the Discovery Image of your infrastructure environment by having the baremetal-operator communicate with the BMC of each bare metal host.  The procedure involves creating a BareMetalHost resource and associated BMC secret for each host.  The automation is enabled by a label on the BareMetalHost that references the infrastructure environment.

The automation performs the following actions:
- Boots each bare metal host with the Discovery Image represented by the infrastructure environment.
- Reboots each host with the latest Discovery Image in case the infrastructure environment or any associated network configurations is updated.
- Upon discovery, associates each Agent resource with its corresponding BareMetalHost resource.
- Updates Agent resource properties based on information from the BareMetalHost, such as hostname, role, and installation disk.
- Approves the Agent for use as a cluster node.

Complete the following steps to use an automated procedure for booting a host by using the baremetal-operator:

. Create a BMC secret by applying the following YAML content and replacing values where needed:
+
[source,YAML]
----
apiVersion: v1
kind: Secret
metadata:
  name: <bmc-secret-name>
  namespace: <namespace>
type: Opaque
data:
  username: <username>
  password: <password>
----

. Create a bare metal host by applying the following YAML content and replacing values where needed:
+
[source,YAML]
----
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: <bmh-name>
  namespace: <namespace>
  annotations:
    inspect.metal3.io: disabled
  labels:
    infraenvs.agent-install.openshift.io: <your-infraenv> <1>
spec:
  online: true
  automatedCleaningMode: disabled <2>
  bootMACAddress: <your-mac-address>  <3>
  bmc:
    address: <machine-address> <4>
    credentialsName: <bmc-secret-name> <5>
----
<1> The value must match the InfraEnv name and exist in the same namespace.
<2> If you do not set a value, the `metadata` value is automatically used.
<3> Use your mac address.
<4> Use your machine address.
<5> Make sure that the `credentialsName` value matches the name of the BMC secret you created.

After turning on the host, the image starts downloading. This might take a few mintues. When the host is discovered, an agent custom resource is created automatically.

See the following table to learn about annotations that are available for the agent custom resource:
|===
| Annotation | Optional or required | Description

| `bmac.agent-install.openshift.io/role`
| Required
| Master or worker role.

| `bmac.agent-install.openshift.io/hostname`
| Optional
|

| `bmac.agent-install.openshift.io/machine-config-pool`
| Optional
|

| `bmac.agent-install.openshift.io/installer-args`
| Optional
|

| `bmac.agent-install.openshift.io/ignition-config-overrides`
| Optional
|
|===

See link:https://docs.openshift.com/container-platform/4.13/installing/installing_bare_metal_ipi/ipi-install-prerequisites.html#network-requirements-out-of-band_ipi-install-prerequisites[Port access for the out-of-band management IP address] to learn about the required ports for using a bare metal host.

[#additional-resources-cluster-on-premises]
== Additional resources

- For additional information about zero touch provisioning, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html/scalability_and_performance/clusters-at-the-network-far-edge[Clusters at the network far edge] in the {ocp-short} documentation.

- See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.13/html/images/managing-images#using-image-pull-secrets[Using image pull secrets].

- See xref:../credentials/credential_on_prem.adoc#creating-a-credential-for-an-on-premises-environment[Creating a credential for an on-premises environment].

- See xref:../cluster_lifecycle/release_images.adoc#release-images-intro[Release images].

- See xref:../cluster_lifecycle/scale_hosts_infra_env.adoc#scale-hosts-infrastructure-env[Scaling hosts to an infrastructure environment].

- Return to xref:../cluster_lifecylce/create_cluster_on_prem.adoc#on-prem-creating-your-cluster-with-the-console[Creating your cluster with the console].
