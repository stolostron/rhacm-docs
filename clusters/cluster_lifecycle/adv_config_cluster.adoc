[#advanced-config-cluster]
= Cluster lifecycle advanced configuration 

You can configure some cluster settings during or after installation.

[#custom-api-certificates]
== Customizing API server certificates

The managed clusters communicate with the hub cluster through a mutual connection with the OpenShift Kube API server external load balancer. The default OpenShift Kube API server certificate is issued by an internal {ocp} cluster certificate authority (CA) when {ocp-short} is installed. If necessary, you can add or change certificates.

Changing the API server certificate might impact the communication between the managed cluster and the hub cluster. When you add the named certificate before installing the product, you can avoid an issue that might leave your managed clusters in an offline state. 

The following list contains some examples of when you might need to update your certificates: 

* You want to replace the default API server certificate for the external load balancer with your own certificate. By following the guidance in _Adding API server certificates_ in the {ocp-short} documentation, you can add a named certificate with host name `api.<cluster_name>.<base_domain>` to replace the default API server certificate for the external load balancer. Replacing the certificate might cause some of your managed clusters to move to an offline state. If your clusters are in an offline state after upgrading the certificates, follow the troubleshooting instructions for _Troubleshooting imported clusters offline after certificate change_ to resolve it.
+
*Note:* Adding the named certificate before installing the product helps to avoid your clusters moving to an offline state.

* The named certificate for the external load balancer is expiring and you need to replace it. If both the old and the new certificate share the same root CA certificate, despite the number of intermediate certificates, you can follow the guidance in _Adding API server certificates_ in the {ocp-short} documentation to create a new secret for the new certificate. Then update the serving certificate reference for host name `api.<cluster_name>.<base_domain>` to the new secret in the `APIServer` custom resource. Otherwise, when the old and new certificates have different root CA certificates, complete the following steps to replace the certificate: 
+
. Locate your `APIServer` custom resource, which resembles the following example: 
+
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: APIServer
metadata:
  name: cluster
spec:
  audit:
    profile: Default
  servingCerts:
    namedCertificates:
    - names:
      - api.mycluster.example.com
      servingCertificate:
        name: old-cert-secret
----

. Create a new secret in the `openshift-config` namespace that contains the content of the existing and new certificates by running the following commands:
+
.. Copy the old certificate into a new certificate:
+
[source,bash]
----
cp old.crt combined.crt
----

.. Add the contents of the new certificate to the copy of the old certificate:
+
[source,bash]
----
cat new.crt >> combined.crt
----

.. Apply the combined certificates to create a secret:
+
[source,bash]
----
oc create secret tls combined-certs-secret --cert=combined.crt --key=old.key -n openshift-config
----

. Update your `APIServer` resource to reference the combined certificate as the `servingCertificate`.
+
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: APIServer
metadata:
  name: cluster
spec:
  audit:
    profile: Default
  servingCerts:
    namedCertificates:
    - names:
      - api.mycluster.example.com
      servingCertificate:
        name: combined-cert-secret
----

. After about 15 minutes, the CA bundle containing both new and old certificates is propagated to the managed clusters.

. Create another secret named `new-cert-secret` in the `openshift-config` namespace that contains only the new certificate information by entering the following command:
+
[source,bash]
----
oc create secret tls new-cert-secret --cert=new.crt --key=new.key -n openshift-config {code}
----

. Update the `APIServer` resource by changing the name of `servingCertificate` to reference the `new-cert-secret`. Your resource might resemble the following example: 
+
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: APIServer
metadata:
  name: cluster
spec:
  audit:
    profile: Default
  servingCerts:
    namedCertificates:
    - names:
      - api.mycluster.example.com
      servingCertificate:
        name: new-cert-secret
----
+
After about 15 minutes, the old certificate is removed from the CA bundle, and the change is automatically propagated to the managed clusters.

*Note:* Managed clusters must use the host name `api.<cluster_name>.<base_domain>` to access the hub cluster. You cannot use named certificates that are configured with other host names.

[#config-proxy-hub-cluster]
== Configuring the proxy between hub cluster and managed cluster

To register a managed cluster to your {mce} hub cluster, you need to transport the  managed cluster to your {mce-short} hub cluster. Sometimes your managed cluster cannot directly reach your {mce-short} hub cluster. In this instance, configure the proxy settings to allow the communications from the managed cluster to access the {mce-short} hub cluster through a HTTP or HTTPS proxy server.

For example, the {mce-short} hub cluster is in a public cloud, and the managed cluster is in a private cloud environment behind firewalls. The communications out of the private cloud can only go through a HTTP or HTTPS proxy server. 

[#config-proxy-hub-cluster-prereq]
=== Prerequisites

- You have a HTTP or HTTPS proxy server running that supports HTTP tunnels. For example, HTTP connect method. 
- You have a manged cluster that can reach the HTTP or HTTPS proxy server, and the proxy server can access the {mce-short} hub cluster. 

Complete the following steps to configure the proxy settings between hub cluster and managed cluster:

. Create a `KlusterConfig` resource with proxy settings. 
.. See the following configuration with HTTP proxy:
+
[source,yaml]
----
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: http-proxy
spec:
  hubKubeAPIServerProxyConfig:
    httpProxy: "http://<username>:<password>@<ip>:<port>"
----
+ 
.. See the following configuration with HTTPS proxy:  
+
[source,yaml]
----
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: https-proxy
spec:
  hubKubeAPIServerProxyConfig:
    httpsProxy: "https://<username>:<password>@<ip>:<port>"
    caBundle: <user-ca-bundle> 
----
+ 
*Note:* A CA certificate is required when the HTTPS proxy server is configured. The HTTPS proxy is used if both HTTP proxy and HTTPS proxy are specified.

. When creating a managed cluster, choose the `KlusterletConfig` resource by adding an annotation that refers to the `KlusterletConfig` resource. See the following example:
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  annotations:
    agent.open-cluster-management.io/klusterlet-config: <klusterlet-config-name>
  name:<managed-cluster-name>
spec:
  hubAcceptsClient: true
  leaseDurationSeconds: 60 
----
+ 
*Notes:*
+
* You might need to toggle the YAML view to add the annotation to the `ManagedCluster` resource when you operate on the {mce-short} console.
* You can use a global `KlusterletConfig` to enable the configuration on every managed cluster without using an annotation for binding.

[#disable-proxy-hub-managed]
=== Disabling the proxy between hub cluster and managed cluster

If your development changes, you might need to disable the HTTP or HTTPS proxy. 

. Go to the `ManagedCluster` resource.
. Remove the `agent.open-cluster-management.io/klusterlet-config` annotation.

[#config-klusterlet-nodes]
=== Optional: Configuring the klusterlet to run on specific nodes

When you create a cluster using {acm}, you can specify which nodes you want to run the managed cluster klusterlet to run on by configuring the `nodeSelector` and `tolerations` annotation for the managed cluster. Complete the following steps to configure these settings: 

. Select the managed cluster that you want to update from the clusters page in the console. 

. Set the YAML switch to `On` to view the YAML content.
+
*Note:* The YAML editor is only available when importing or creating a cluster. To edit the managed cluster YAML definition after importing or creating, you must use the {ocp-short} command-line interface or the {acm-short} search feature.

. Add the `nodeSelector` annotation to the managed cluster YAML definition. The key for this annotation is: `open-cluster-management/nodeSelector`. The value of this annotation is a string map with JSON formatting.

. Add the `tolerations` entry to the managed cluster YAML definition. The key of this annotation is: `open-cluster-management/tolerations`. The value of this annotation represents a link:https://github.com/kubernetes/api/blob/release-1.24/core/v1/types.go#L3007[toleration] list with JSON formatting.
The resulting YAML might resemble the following example: 
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  annotations:
    open-cluster-management/nodeSelector: '{"dedicated":"acm"}'
    open-cluster-management/tolerations: '[{"key":"dedicated","operator":"Equal","value":"acm","effect":"NoSchedule"}]' 
----

. To make sure your content is deployed to the correct nodes, complete the steps in link:../../add-ons/configure_nodeselector_tolerations_addons.adoc#configure-nodeselector-tolerations-addons[Configuring nodeSelectors and tolerations for klusterlet add-ons].

[#custom-server-url-ca]
== Customizing the server URL and CA bundle of the hub cluster API server when importing a managed cluster (Technology Preview)

You might not be able to register a managed cluster on your {mce-short} hub cluster if intermediate components exist between the managed cluster and the hub cluster. Example intermediate components include a Virtual IP, load balancer, reverse proxy, or API gateway. If you have an intermediate component, you must use a custom server URL and CA bundle for the hub cluster API server when importing a managed cluster.

[#custom-server-url-ca-prereq]
=== Prerequisites

* You must configure the intermediate component so that the hub cluster API server is accessible for the managed cluster.
* If the intermediate component terminates the SSL connections between the managed cluster and hub cluster API server, you must bridge the SSL connections and pass the authentication information from the original requests to the back end of the hub cluster API server. You can use the User Impersonation feature of the Kubernetes API server to bridge the SSL connections.
+
The intermediate component extracts the client certificate from the original requests, adds Common Name (CN) and Organization (O) of the certificate subject as impersonation headers, and then forwards the modified impersonation requests to the back end of the hub cluster API server.
+
*Note:* If you bridge the SSL connections, the cluster proxy add-on does not work.

[#custom-server-url-ca-steps]
=== Customizing the server URL and hub cluster CA bundle

To use a custom hub API server URL and CA bundle when importing a managed cluster, complete the following steps:

. Create a `KlusterConfig` resource with the custom hub cluster API server URL and CA bundle. See the following example:
+
[source,yaml]
----
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: <1>
spec:
  hubKubeAPIServerURL: "https://api.example.com:6443" <2>
  hubKubeAPIServerCABundle: "LS0tLS1CRU...LS0tCg==" <3>
----
+
<1> Add your klusterlet config name.
<2> Add your custom server URL.
<3> Add your custom CA bundle.

. Select the `KlusterletConfig` resource when creating a managed cluster by adding an annotation that refers to the resource. See the following example:
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  annotations:
    agent.open-cluster-management.io/klusterlet-config: <1>
  name: <2>
spec:
  hubAcceptsClient: true
  leaseDurationSeconds: 60
----
+
<1> Add your klusterlet config name.
<2> Add your cluster name.
+
*Notes:*
+
* If you use the console, you might need to enable the YAML view to add the annotation to the `ManagedCluster` resource.
* You can use a global `KlusterletConfig` to enable the configuration on every managed cluster without using an annotation for binding.

[#config-gloabl-klusterletconfig]
=== Configuring the global _KlusterletConfig_

If you create a `KlusterletConfig` resource and set the name to `global`, the configurations in the global `KlusterletConfig` are automatically applied on every managed cluster.

If you create another `KlusterletConfig` in an environment that has a global `KlusterletConfig` and bind it with a managed cluster, the value of your `KlusterletConfig` overrides the global `KlusterletConfig` value if you set different values for the same field.

See the following example where the `hubKubeAPIServerURL` field has different values set in your `KlusterletConfig` and the global `KlusterletConfig`. The `"example.test.com"` value overrides the `"example.global.com"` value:

[source,yaml]
----
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: test
spec:
  hubKubeAPIServerURL: "example.test.com"
—
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: global
spec:
  hubKubeAPIServerURL: "example.global.com"
----

If you create another `KlusterletConfig` in an environment that has a global `KlusterletConfig` and bind it with a managed cluster, the value of the global `KlusterletConfig` is used if the same field is missing or does not have a value in your `KlusterletConfig`.

See the following example, where the `"example.global.com"` value in the `hubKubeAPIServerURL` field of the global `KlusterletConfig` overrides your `KlusterletConfig`:

[source,yaml]
----
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: test
spec:
  hubKubeAPIServerURL: ""
—
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: global
spec:
  hubKubeAPIServerURL: "example.global.com"
----

See the following example, where the `"example.global.com"` value in the `hubKubeAPIServerURL` field of the global `KlusterletConfig` also overrides your `KlusterletConfig`:

[source,yaml]
----
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: test
—
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: global
spec:
  hubKubeAPIServerURL: "example.global.com"
----

[#add-resources-adv-cluster]
== Additional resources

* link:https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.15/html/security_and_compliance/configuring-certificates#api-server-certificates[Adding API server certificates]

* link:../support_troubleshooting/trouble_cluster_offline_cert_mce.adoc#troubleshooting-imported-clusters-offline-after-certificate-change-mce[Troubleshooting imported clusters offline after certificate change]

* xref:../cluster_lifecycle/cluster_proxy_addon.adoc#cluster-proxy-addon-settings[Configuring proxy settings for cluster proxy add-ons]
