[#known-issues-mce]
= Known issues and limitations for Cluster lifecycle with {mce-short}

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for Cluster lifecycle with {mce-short}. The following list contains known issues and limitations for this release, or known issues that continued from the previous release. For your {ocp-short} cluster, see link:https://docs.redhat.com/documentation/en-us/openshift_container_platform/4.14[{ocp-short} release notes].

Cluster management known issues and limitations are part of the Cluster lifecycle with {mce-short} documentation.

* <<cluster-issues-install,Installation>>
* <<cluster-issues-mce,Cluster management>>
* <<cluster-issues-cim,Central infrastructure management>>
* <<cluster-issues-msa,Managed Service Account add-on>>
//??

[#cluster-issues-install]
== Installation 

Learn about known issues and limitations during {mce-short} installation.

[#status-stuck-rosa-hcp]
=== Status stuck when installing on {rosa} with hosted control plane cluster
//2.7:14566

Installation status might get stuck in the `Installing` state when you install {mce-short} on a {rosa} with hosted control planes cluster. The `local-cluster` might also remain in the `Unknown` state. 

When you check the `klusterlet-agent` pod log in the `open-cluster-management-agent` namespace on your hub cluster, you see an error that resembles the following:

[source,terminal]
----
E0809 18:45:29.450874       1 reflector.go:147] k8s.io/client-go@v0.29.4/tools/cache/reflector.go:229: Failed to watch *v1.CertificateSigningRequest: failed to list *v1.CertificateSigningRequest: Get "https://api.xxx.openshiftapps.com:443/apis/certificates.k8s.io/v1/certificatesigningrequests?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
----

To resolve the problem, configure the hub cluster API server verification strategy. Complete the following steps:

. Create a `KlusterletConfig` resource with name `global` if it does not exist.

. Set the `spec.hubKubeAPIServerConfig.serverVerificationStrategy` to `UseSystemTruststore`. See the following example:

+
[source,yaml]
----
apiVersion: config.open-cluster-management.io/v1alpha1
kind: KlusterletConfig
metadata:
  name: global
spec:
  hubKubeAPIServerConfig:
    serverVerificationStrategy: UseSystemTruststore
----

. Apply the resource by running the following command on the hub cluster. Replace `<filename>` with the name of your file:

+
[source,bash]
----
oc apply -f <filename>
----

. If the `local-cluster` state does not recover in one minute, export and decode the `import.yaml` file by running the following command on the hub cluster:

+
[source,bash]
----
oc get secret local-cluster-import -n local-cluster -o jsonpath={.data.import\.yaml} | base64 --decode > import.yaml
----

. Apply the file by running the following command on the hub cluster:

+
[source,bash]
----
oc apply -f import.yaml
----

[#cluster-issues-mce]
== Cluster

//Need a description here. I will fix later.

[#limitation-with-nmstate]
=== Limitation with _nmstate_
//2.9:9128

Develop quicker by configuring copy and paste features. To configure the `copy-from-mac` feature in the `assisted-installer`, you must add the `mac-address` to the `nmstate` definition interface and the `mac-mapping` interface. The `mac-mapping` interface is provided outside the `nmstate` definition interface. As a result, you must provide the same `mac-address` twice. 

If you have a different version of VolSync installed, replace `v0.6.0` with your installed version. 

[#clusterset-label-not-removed]
=== Deleting a managed cluster set does not automatically remove its label
//2.5:20727

After you delete a `ManagedClusterSet`, the label that is added to each managed cluster that associates the cluster to the cluster set is not automatically removed. Manually remove the label from each of the managed clusters that were included in the deleted managed cluster set. The label resembles the following example: `cluster.open-cluster-management.io/clusterset:<ManagedClusterSet Name>`.

[#hive-cluster-claim]
=== ClusterClaim error
//2.5:19968

If you create a Hive `ClusterClaim` against a `ClusterPool` and manually set the `ClusterClaimspec` lifetime field to an invalid golang time value, the product stops fulfilling and reconciling all `ClusterClaims`, not just the malformed claim.  

You see the following error in the `clusterclaim-controller` pod logs, which is a specific example with the `PoolName` and invalid `lifetime` included:

----
E0203 07:10:38.266841       1 reflector.go:138] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers_map.go:224: Failed to watch *v1.ClusterClaim: failed to list *v1.ClusterClaim: v1.ClusterClaimList.Items: []v1.ClusterClaim: v1.ClusterClaim.v1.ClusterClaim.Spec: v1.ClusterClaimSpec.Lifetime: unmarshalerDecoder: time: unknown unit "w" in duration "1w", error found in #10 byte of ...|time":"1w"}},{"apiVe|..., bigger context ...|clusterPoolName":"policy-aas-hubs","lifetime":"1w"}},{"apiVersion":"hive.openshift.io/v1","kind":"Cl|...
----

You can delete the invalid claim.

If the malformed claim is deleted, claims begin successfully reconciling again without any further interaction.

[#clusterimageset-fast-channel]
=== The product channel out of sync with provisioned cluster
//2.4:17790

The `clusterimageset` is in `fast` channel, but the provisioned cluster is in `stable` channel. Currently the product does not sync the `channel` to the provisioned {ocp-short} cluster. 

Change to the right channel in the {ocp-short} console. Click **Administration** > **Cluster Settings** > **Details Channel**.

[#subnet-required-on-prem-clust-create]
=== Selecting a subnet is required when creating an on-premises cluster
//2.4:18387

When you create an on-premises cluster using the console, you must select an available subnet for your cluster. It is not marked as a required field. 

[#cluster-provision-fails-ansible-proxy]
=== Cluster provision with Ansible automation fails in proxy environment
//2.4:17659

An Automation template that is configured to automatically provision a managed cluster might fail when both of the following conditions are met: 

* The hub cluster has cluster-wide proxy enabled. 
* The {aap-short} can only be reached through the proxy.

[#no-delete-cluster-namespace-before-remove-cluster]
=== Cannot delete managed cluster namespace manually
//2.3:13474

You cannot delete the namespace of a managed cluster manually. The managed cluster namespace is automatically deleted after the managed cluster is detached. If you delete the managed cluster namespace manually before the managed cluster is detached, the managed cluster shows a continuous terminating  status after you delete the managed cluster. To delete this terminating managed cluster, manually remove the finalizers from the managed cluster that you detached.

[#automatic-secret-updates-for-provisioned-clusters-is-not-supported]
=== Automatic secret updates for provisioned clusters is not supported
// 2.0.0:3702

When you change your cloud provider access key on the cloud provider side, you also need to update the corresponding credential for this cloud provider on the console of {mce-short}. This is required when your credentials expire on the cloud provider where the managed cluster is hosted and you try to delete the managed cluster.

[#cluster-might-not-be-destroyed]
=== Process to destroy a cluster does not complete
// 2.1.0:4748

When you destroy a managed cluster, the status continues to display `Destroying` after one hour, and the cluster is not destroyed. To resolve this issue complete the following steps:

. Manually ensure that there are no orphaned resources on your cloud, and that all of the provider resources that are associated with the managed cluster are cleaned up.

. Open the `ClusterDeployment` information for the managed cluster that is being removed by entering the following command:
+
----
oc edit clusterdeployment/<mycluster> -n <namespace>
----
+
Replace `_mycluster_` with the name of the managed cluster that you are destroying.
+
Replace `_namespace_` with the namespace of the managed cluster.

. Remove the `hive.openshift.io/deprovision` finalizer to forcefully stop the process that is trying to clean up the cluster resources in the cloud.

. Save your changes and verify that `ClusterDeployment` is gone.

. Manually remove the namespace of the managed cluster by running the following command:
+
----
oc delete ns <namespace>
----
+
Replace `_namespace_` with the namespace of the managed cluster.

[#no-upgrade-os-on-osd]
=== Cannot upgrade {ocp-short} managed clusters on {ocp-short} Dedicated with the console
// 2.2.0:8922

You cannot use the {acm-short} console to upgrade {ocp-short} managed clusters that are in the {ocp-short} Dedicated environment.

[#work-manager-addon-search]
=== Work manager add-on search details
//2.3.0: 13715

The search details page for a certain resource on a certain managed cluster might fail. You must ensure that the work-manager add-on in the managed cluster is in `Available` status before you can search.

[#non-ocp-logs]
=== Non-{ocp-short} managed clusters require _ManagedServiceAccount_ or _LoadBalancer_ for pod logs 
//2.11:11034

The `ManagedServiceAccount` and cluster proxy add-ons are enabled by default in {acm-short} version 2.10 and newer. If the add-ons are disabled after upgrading, you must enable the `ManagedServiceAccount` and cluster proxy add-ons manually to use the pod log feature on non-{ocp-short} managed clusters.

See link:../../clusters/install_upgrade/adv_config_install.adoc#serviceaccount-addon-intro[ManagedServiceAccount add-on] to learn how to enable `ManagedServiceAccount` and see link:../../clusters/cluster_lifecycle/cluster_proxy_addon.adoc#cluster-proxy-addon[Using cluster proxy add-ons] to learn how to enable a cluster proxy add-on.

[#hypershift-proxy-install-not-supported-ocp-410z]
=== {ocp-short} 4.10.z does not support hosted control plane clusters with proxy configuration
// 2.6:25156

When you create a hosting service cluster with a cluster-wide proxy configuration on {ocp-short} 4.10.z, the `nodeip-configuration.service` service does not start on the worker nodes.

[#client-cannot-reach-ipxe-script]
=== Client cannot reach iPXE script
//2.6:25157

iPXE is an open source network boot firmware. See link:https://ipxe.org/[iPXE] for more details.

When booting a node, the URL length limitation in some DHCP servers cuts off the `ipxeScript` URL in the `InfraEnv` custom resource definition, resulting in the following error message in the console:

`no bootable devices`

To work around the issue, complete the following steps:

. Apply the `InfraEnv` custom resource definition when using an assisted installation to expose the `bootArtifacts`, which might resemble the following file:
+
----
status:
  agentLabelSelector:
    matchLabels:
      infraenvs.agent-install.openshift.io: qe2
  bootArtifacts:
    initrd: https://assisted-image-service-multicluster-engine.redhat.com/images/0000/pxe-initrd?api_key=0000000&arch=x86_64&version=4.11
    ipxeScript: https://assisted-service-multicluster-engine.redhat.com/api/assisted-install/v2/infra-envs/00000/downloads/files?api_key=000000000&file_name=ipxe-script
    kernel: https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.12/latest/rhcos-live-kernel-x86_64
    rootfs: https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.12/latest/rhcos-live-rootfs.x86_64.img
----

. Create a proxy server to expose the `bootArtifacts` with short URLs.

. Copy the `bootArtifacts` and add them them to the proxy by running the following commands:
+
----
for artifact in oc get infraenv qe2 -ojsonpath="{.status.bootArtifacts}" | jq ". | keys[]" | sed "s/\"//g"
do curl -k oc get infraenv qe2 -ojsonpath="{.status.bootArtifacts.${artifact}}"` -o $artifact 
----

. Add the `ipxeScript` artifact proxy URL to the `bootp` parameter in `libvirt.xml`.

[#cannot-delete-clusterdeployment]
=== Cannot delete _ClusterDeployment_ after upgrading {acm-short}

If you are using the removed BareMetalAssets API in {acm-short} 2.6, the `ClusterDeployment` cannot be deleted after upgrading to {acm-short} 2.7 because the BareMetalAssets API is bound to the `ClusterDeployment`.

To work around the issue, run the following command to remove the `finalizers` before upgrading to {acm-short} 2.7:

----
oc patch clusterdeployment <clusterdeployment-name> -p '{"metadata":{"finalizers":null}}' --type=merge 
----

[#deploy-managed-stuck-pending]
=== Managed cluster stuck in _Pending_ status after deployment
//2.11:ACM-12722

The converged flow is the default process of provisioning. When you use the `BareMetalHost` resource for the Bare Metal Operator (BMO) to connect your host to a live ISO, the Ironic Python Agent does the following actions:

* It runs the steps in the Bare Metal installer-provisioned-infrastructure. 
* It starts the {ai} agent, and the agent handles the rest of the install and provisioning process. 

If the {ai} agent starts slowly and you deploy a managed cluster, the managed cluster might become stuck in the `Pending` status and not have any agent resources. You can work around the issue by disabling the converged flow. 

*Important:* When you disable the converged flow, only the {ai} agent runs in the live ISO, reducing the number of open ports and disabling any features you enabled with the  Ironic Python Agent agent, including the following:

* Pre-provisioning disk cleaning 
* iPXE boot firmware 
* BIOS configuration 

To decide what port numbers you want to enable or disable without disabling the converged flow, see link:../../clusters/about/mce_networking.adoc#mce-network-configuration[Network configuration]. 

To disable the converged flow, complete the following steps:

. Create the following ConfigMap on the hub cluster:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-assisted-service-config
  namespace: multicluster-engine
data:
  ALLOW_CONVERGED_FLOW: "false" <1> 
----
<1> When you set the parameter value to "false", you also disable any features enabled by the Ironic Python Agent. 

. Apply the ConfigMap by running the following command:
+
----
oc annotate --overwrite AgentServiceConfig agent unsupported.agent-install.openshift.io/assisted-service-configmap=my-assisted-service-config
----

[#managedclusterset-api-limitation]
=== ManagedClusterSet API specification limitation
//2.9:ACM-6423

The `selectorType: LaberSelector` setting is not supported when using the link:../../apis/clusterset.json.adoc#clustersets-api[Clustersets API]. The `selectorType: ExclusiveClusterSetLabel` setting is supported.

[#bulk-destroy-kubevirt-hosted]
=== Bulk destroy option on KubeVirt hosted cluster does not destroy hosted cluster
//2.10:ACM-10174

Using the bulk destroy option in the console on KubeVirt hosted clusters does not destroy the KubeVirt hosted clusters.

Use the row action drop-down menu to destroy the KubeVirt hosted cluster instead.

[#ocp-ded-support-curator]
=== The Cluster curator does not support {ocp-short} Dedicated clusters
//2.10:ACM-10154

When you upgrade an {ocp-short} Dedicated cluster by using the `ClusterCurator` resource, the upgrade fails because the Cluster curator does not support {ocp-short} Dedicated clusters.

[#custom-ingress-domain-limitation]
=== Custom ingress domain is not applied correctly
//2.8:ACM-6279

You can specify a custom ingress domain by using the `ClusterDeployment` resource while installing a managed cluster, but the change is only applied after the installation by using the `SyncSet` resource. As a result, the `spec` field in the `clusterdeployment.yaml` file displays the custom ingress domain you specified, but the `status` still displays the default domain.

[#cluster-issues-cim]
== Central infrastructure management

[#iso-image-name-too-long]
=== Cluster provisioning with Infrastructure Operator fails
//2.4:17411

When creating {ocp-short} clusters by using the Infrastructure Operator, the file name of the ISO image might be too long. The long image name causes the image provisioning and the cluster provisioning to fail. To determine if this is the problem, complete the following steps: 

. View the bare metal host information for the cluster that you are provisioning by running the following command: 
+
----
oc get bmh -n <cluster_provisioning_namespace>
----

. Run the `describe` command to view the error information:
+
----
oc describe bmh -n <cluster_provisioning_namespace> <bmh_name>
----

. An error similar to the following example indicates that the length of the filename is the problem: 
+
----
Status:
  Error Count:    1
  Error Message:  Image provisioning failed: ... [Errno 36] File name too long ...
----

If this problem occurs, it is typically on the following versions of {ocp-short}, because the infrastructure operator was not using image service:

* 4.8.17 and earlier
* 4.9.6 and earlier

To avoid this error, upgrade your {ocp-short} to version 4.8.18 or later, or 4.9.7 or later.

[#cluster-issues-msa]
== Managed Service Account add-on
//these sections are higher level features of the product; not sure this needs to be separated as a header like install or cluster management--a good way to tell is looking at the console. We would not have a section for every add-on.

[#installnamespace-field-limit]
=== _installNamespace_ field can only have one value
//2.9:ACM-7523

When enabling the `managed-serviceaccount` add-on, the `installNamespace` field in the `ManagedClusterAddOn` resource must have `open-cluster-management-agent-addon` as the value. Other values are ignored. The `managed-serviceaccount` add-on agent is always deployed in the `open-cluster-management-agent-addon` namespace on the managed cluster.

[#settings-limit-msa-agent]
=== _tolerations_ and _nodeSelector_ settings do not affect the _managed-serviceaccount_ agent
//2.9:ACM-7523

The `tolerations` and `nodeSelector` settings configured on the `MultiClusterEngine` and `MultiClusterHub` resources do not affect the `managed-serviceaccount` agent deployed on the local cluster. The `managed-serviceaccount` add-on is not always required on the local cluster.

If the `managed-serviceaccount` add-on is required, you can work around the issue by completing the following steps:

. Create the `addonDeploymentConfig` custom resource.

. Set the `tolerations` and `nodeSelector` values for the local cluster and `managed-serviceaccount` agent.

. Update the `managed-serviceaccount` `ManagedClusterAddon` in the local cluster namespace to use the `addonDeploymentConfig` custom resource you created.

See link:../../add-ons/configure_nodeselector_tolerations_addons.adoc#configure-nodeselector-tolerations-addons[Configuring nodeSelectors and tolerations for klusterlet add-ons] to learn more about how to use the `addonDeploymentConfig` custom resource to configure `tolerations` and `nodeSelector` for add-ons.
