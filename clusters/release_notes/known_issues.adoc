[#known-issues-cluster]
= Cluster lifecycle known issues

////
Please follow this format:

Title of known issue, be sure to match header and make title, header unique

Hidden comment: Release: #issue
Known issue process and when to write:

- Doesn't work the way it should
- Straightforward to describe
- Good to know before getting started
- Quick workaround, of any
- Applies to most, if not all, users
- Something that is likely to be fixed next release (never preannounce)
- Always comment with the issue number and version: //2.4:19417
- Link to customer BugZilla ONLY if it helps; don't link to internal BZs and GH issues.

Or consider a troubleshooting topic.
////

Review the known issues for cluster lifecycle with {mce-short}. The following list contains known issues for this release, or known issues that continued from the previous release. For your {ocp-short} cluster, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14[{ocp-short} release notes].

* <<cluster-lifecycle-issues-mce,Cluster lifecycle>>
* <<hosted-control-plane-mce,Hosted control planes>>

[#cluster-lifecycle-issues-mce]
== Cluster management

Cluster lifecycle known issues and limitations are part of the Cluster lifecycle with {mce-short} documentation.

[#limitation-with-nmstate]
=== Limitation with _nmstate_
//2.9:9128

Develop quicker by configuring copy and paste features. To configure the `copy-from-mac` feature in the `assisted-installer`, you must add the `mac-address` to the `nmstate` definition interface and the `mac-mapping` interface. The `mac-mapping` interface is provided outside the `nmstate` definition interface. As a result, you must provide the same `mac-address` twice. 

[#prehook-failure-hosted-cluster]
=== Prehook failure does not fail the hosted cluster creation 
//2.9:8350

If you use the automation template for the hosted cluster creation and the prehook job fails, then it looks like the hosted cluster creation is still progressing. This is normal because the hosted cluster was designed with no complete failure state, and therefore, it keeps trying to create the cluster.

[#volsync-remove-csv-managed]
=== Manual removal of the VolSync CSV required on managed cluster when removing the add-on
//2.5:21356

When you remove the VolSync `ManagedClusterAddOn` from the hub cluster, it removes the VolSync operator subscription on the managed cluster but does not remove the cluster service version (CSV). To remove the CSV from the managed clusters, run the following command on each managed cluster from which you are removing VolSync:

----
oc delete csv -n openshift-operators volsync-product.v0.6.0
----

If you have a different version of VolSync installed, replace `v0.6.0` with your installed version. 

[#clusterset-label-not-removed]
=== Deleting a managed cluster set does not automatically remove its label
//2.5:20727

After you delete a `ManagedClusterSet`, the label that is added to each managed cluster that associates the cluster to the cluster set is not automatically removed. Manually remove the label from each of the managed clusters that were included in the deleted managed cluster set. The label resembles the following example: `cluster.open-cluster-management.io/clusterset:<ManagedClusterSet Name>`.

[#hive-cluster-claim]
=== ClusterClaim error
//2.5:19968

If you create a Hive `ClusterClaim` against a `ClusterPool` and manually set the `ClusterClaimspec` lifetime field to an invalid golang time value, the product stops fulfilling and reconciling all `ClusterClaims`, not just the malformed claim.  

You see the following error in the `clusterclaim-controller` pod logs, which is a specific example with the `PoolName` and invalid `lifetime` included:

----
E0203 07:10:38.266841       1 reflector.go:138] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers_map.go:224: Failed to watch *v1.ClusterClaim: failed to list *v1.ClusterClaim: v1.ClusterClaimList.Items: []v1.ClusterClaim: v1.ClusterClaim.v1.ClusterClaim.Spec: v1.ClusterClaimSpec.Lifetime: unmarshalerDecoder: time: unknown unit "w" in duration "1w", error found in #10 byte of ...|time":"1w"}},{"apiVe|..., bigger context ...|clusterPoolName":"policy-aas-hubs","lifetime":"1w"}},{"apiVersion":"hive.openshift.io/v1","kind":"Cl|...
----

You can delete the invalid claim.

If the malformed claim is deleted, claims begin successfully reconciling again without any further interaction.

[#clusterimageset-fast-channel]
=== The product channel out of sync with provisioned cluster
//2.4:17790

The `clusterimageset` is in `fast` channel, but the provisioned cluster is in `stable` channel. Currently the product does not sync the `channel` to the provisioned {ocp-short} cluster. 

Change to the right channel in the {ocp-short} console. Click **Administration** > **Cluster Settings** > **Details Channel**.

[#ca-certificate-hub-restore]
=== Restoring the connection of a managed cluster with custom CA certificates to its restored hub cluster might fail
//2.4:19481

After you restore the backup of a hub cluster that managed a cluster with custom CA certificates, the connection between the managed cluster and the hub cluster might fail. This is because the CA certificate was not backed up on the restored hub cluster. To restore the connection, copy the custom CA certificate information that is in the namespace of your managed cluster to the `<managed_cluster>-admin-kubeconfig` secret on the restored hub cluster. 

**Tip:** If you copy this CA certificate to the hub cluster before creating the backup copy, the backup copy includes the secret information. When the backup copy is used to restore in the future, the connection between the hub and managed clusters will automatically complete. 

[#local-cluster-auto]
=== The local-cluster might not be automatically recreated
//2.4:17790

If the local-cluster is deleted while `disableHubSelfManagement` is set to `false`, the local-cluster is recreated by the `MulticlusterHub` operator. After you detach a local-cluster, the local-cluster might not be automatically recreated. 

- To resolve this issue, modify a resource that is watched by the `MulticlusterHub` operator. See the following example:

+
----
oc delete deployment multiclusterhub-repo -n <namespace>
----

- To properly detach the local-cluster, set the `disableHubSelfManagement` to true in the `MultiClusterHub`.  

[#subnet-required-on-prem-clust-create]
=== Selecting a subnet is required when creating an on-premises cluster
//2.4:18387

When you create an on-premises cluster using the console, you must select an available subnet for your cluster. It is not marked as a required field. 

[#iso-image-name-too-long]
=== Cluster provisioning with Infrastructure Operator fails
//2.4:17411

When creating {ocp-short} clusters using the Infrastructure Operator, the file name of the ISO image might be too long. The long image name causes the image provisioning and the cluster provisioning to fail. To determine if this is the problem, complete the following steps: 

. View the bare metal host information for the cluster that you are provisioning by running the following command: 
+
----
oc get bmh -n <cluster_provisioning_namespace>
----

. Run the `describe` command to view the error information:
+
----
oc describe bmh -n <cluster_provisioning_namespace> <bmh_name>
----

. An error similar to the following example indicates that the length of the filename is the problem: 
+
----
Status:
  Error Count:    1
  Error Message:  Image provisioning failed: ... [Errno 36] File name too long ...
----

If this problem occurs, it is typically on the following versions of {ocp-short}, because the infrastructure operator was not using image service:

* 4.8.17 and earlier
* 4.9.6 and earlier

To avoid this error, upgrade your {ocp-short} to version 4.8.18 or later, or 4.9.7 or later.

[#cluster-local-offline-reimport]
=== Local-cluster status offline after reimporting with a different name
//2.4:16977

When you accidentally try to reimport the cluster named `local-cluster` as a cluster with a different name, the status for `local-cluster` and for the reimported cluster display `offline`.

To recover from this case, complete the following steps:

. Run the following command on the hub cluster to edit the setting for self-management of the hub cluster temporarily:
+
----
oc edit mch -n open-cluster-management multiclusterhub
----

. Add the setting `spec.disableSelfManagement=true`.

. Run the following command on the hub cluster to delete and redeploy the local-cluster:
+
----
oc delete managedcluster local-cluster
----

. Enter the following command to remove the `local-cluster` management setting: 
+
----
oc edit mch -n open-cluster-management multiclusterhub
----

. Remove `spec.disableSelfManagement=true` that you previously added.

[#cluster-provision-fails-ansible-proxy]
=== Cluster provision with Ansible automation fails in proxy environment
//2.4:17659

An Automation template that is configured to automatically provision a managed cluster might fail when both of the following conditions are met: 

* The hub cluster has cluster-wide proxy enabled. 
* The {aap-short} can only be reached through the proxy.

[#no-delete-cluster-namespace-before-remove-cluster]
=== Cannot delete managed cluster namespace manually
//2.3:13474

You cannot delete the namespace of a managed cluster manually. The managed cluster namespace is automatically deleted after the managed cluster is detached. If you delete the managed cluster namespace manually before the managed cluster is detached, the managed cluster shows a continuous terminating  status after you delete the managed cluster. To delete this terminating managed cluster, manually remove the finalizers from the managed cluster that you detached.

[#hub-managed-clusters-clock]
=== Hub cluster and managed clusters clock not synced
// 2.1:5636

Hub cluster and manage cluster time might become out-of-sync, displaying in the console `unknown` and eventually `available` within a few minutes. Ensure that the {ocp-short} hub cluster time is configured correctly. See link:https://docs.openshift.com/container-platform/4.13/installing/install_config/installing-customizing.html[Customizing nodes].

[#importing-certain-versions-of-ibm-red-hat-openshift-kubernetes-service-clusters-is-not-supported]
=== Importing certain versions of IBM {ocp-short} Kubernetes Service clusters is not supported
// 1.0.0:2179

You cannot import IBM {ocp-short} Kubernetes Service version 3.11 clusters.
Later versions of IBM OpenShift Kubernetes Service are supported.

[#automatic-secret-updates-for-provisioned-clusters-is-not-supported]
=== Automatic secret updates for provisioned clusters is not supported
// 2.0.0:3702

When you change your cloud provider access key on the cloud provider side, you also need to update the corresponding credential for this cloud provider on the console of {mce-short}. This is required when your credentials expire on the cloud provider where the managed cluster is hosted and you try to delete the managed cluster.

[#node-information-from-the-managed-cluster-cannot-be-viewed-in-search]
=== Node information from the managed cluster cannot be viewed in search
// 2.0.2:4598

Search maps RBAC for resources in the hub cluster. Depending on user RBAC settings, users might not see node data from the managed cluster. Results from search might be different from what is displayed on the _Nodes_ page for a cluster.

[#cluster-might-not-be-destroyed]
=== Process to destroy a cluster does not complete
// 2.1.0:4748

When you destroy a managed cluster, the status continues to display `Destroying` after one hour, and the cluster is not destroyed. To resolve this issue complete the following steps:

. Manually ensure that there are no orphaned resources on your cloud, and that all of the provider resources that are associated with the managed cluster are cleaned up.

. Open the `ClusterDeployment` information for the managed cluster that is being removed by entering the following command:
+
----
oc edit clusterdeployment/<mycluster> -n <namespace>
----
+
Replace `_mycluster_` with the name of the managed cluster that you are destroying.
+
Replace `_namespace_` with the namespace of the managed cluster.

. Remove the `hive.openshift.io/deprovision` finalizer to forcefully stop the process that is trying to clean up the cluster resources in the cloud.

. Save your changes and verify that `ClusterDeployment` is gone.

. Manually remove the namespace of the managed cluster by running the following command:
+
----
oc delete ns <namespace>
----
+
Replace `_namespace_` with the namespace of the managed cluster.

[#no-upgrade-os-on-osd]
=== Cannot upgrade {ocp-short} managed clusters on {ocp-short} Dedicated with the console
// 2.2.0:8922

You cannot use the {acm-short} console to upgrade {ocp-short} managed clusters that are in the {ocp-short} Dedicated environment.

[#work-manager-addon-search]
=== Work manager add-on search details
//2.3.0: 13715

The search details page for a certain resource on a certain managed cluster might fail. You must ensure that the work-manager add-on in the managed cluster is in `Available` status before you can search.

[#non-ocp-logs]
=== Non-{ocp-short} managed clusters require _ManagedServiceAccount_ or _LoadBalancer_ for pod logs 
//2.11:11034

The `ManagedServiceAccount` and cluster proxy add-ons are enabled by default in {acm-short} version 2.10 and newer. If the add-ons are disabled after upgrading, you must enable the `ManagedServiceAccount` and cluster proxy add-ons manually to use the pod log feature on non-{ocp-short} managed clusters.

See link:../../clusters/install_upgrade/adv_config_install.adoc#serviceaccount-addon-intro[ManagedServiceAccount add-on] to learn how to enable `ManagedServiceAccount` and see link:../../clusters/cluster_lifecycle/cluster_proxy_addon.adoc#cluster-proxy-addon[Using cluster proxy add-ons] to learn how to enable a cluster proxy add-on.

[#hypershift-proxy-install-not-supported-ocp-410z]
=== {ocp-short} 4.10.z does not support hosted control plane clusters with proxy configuration
// 2.6:25156

When you create a hosting service cluster with a cluster-wide proxy configuration on {ocp-short} 4.10.z, the `nodeip-configuration.service` service does not start on the worker nodes.

[#provision-ocp-411-azure-fails]
=== Cannot provision {ocp-short} 4.11 cluster on Azure

Provisioning an {ocp-short} 4.11 cluster on Azure fails due to an authentication operator timeout error. To work around the issue, use a different worker node type in the `install-config.yaml` file or set the `vmNetworkingType` parameter to `Basic`. See the following `install-config.yaml` example:

[source,yaml]
----
compute:
- hyperthreading: Enabled
  name: 'worker'
  replicas: 3
  platform:
    azure:
      type:  Standard_D2s_v3
      osDisk:
        diskSizeGB: 128
      vmNetworkingType: 'Basic'
----

[#client-cannot-reach-ipxe-script]
=== Client cannot reach iPXE script
//2.6:25157

iPXE is an open source network boot firmware. See link:https://ipxe.org/[iPXE] for more details.

When booting a node, the URL length limitation in some DHCP servers cuts off the `ipxeScript` URL in the `InfraEnv` custom resource definition, resulting in the following error message in the console:

`no bootable devices`

To work around the issue, complete the following steps:

. Apply the `InfraEnv` custom resource definition when using an assisted installation to expose the `bootArtifacts`, which might resemble the following file:
+
----
status:
  agentLabelSelector:
    matchLabels:
      infraenvs.agent-install.openshift.io: qe2
  bootArtifacts:
    initrd: https://assisted-image-service-multicluster-engine.redhat.com/images/0000/pxe-initrd?api_key=0000000&arch=x86_64&version=4.11
    ipxeScript: https://assisted-service-multicluster-engine.redhat.com/api/assisted-install/v2/infra-envs/00000/downloads/files?api_key=000000000&file_name=ipxe-script
    kernel: https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.12/latest/rhcos-live-kernel-x86_64
    rootfs: https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.12/latest/rhcos-live-rootfs.x86_64.img
----

. Create a proxy server to expose the `bootArtifacts` with short URLs.

. Copy the `bootArtifacts` and add them them to the proxy by running the following commands:
+
----
for artifact in oc get infraenv qe2 -ojsonpath="{.status.bootArtifacts}" | jq ". | keys[]" | sed "s/\"//g"
do curl -k oc get infraenv qe2 -ojsonpath="{.status.bootArtifacts.${artifact}}"` -o $artifact 
----

. Add the `ipxeScript` artifact proxy URL to the `bootp` parameter in `libvirt.xml`.

[#cannot-delete-clusterdeployment]
=== Cannot delete _ClusterDeployment_ after upgrading {acm-short}

If you are using the removed BareMetalAssets API in {acm-short} 2.6, the `ClusterDeployment` cannot be deleted after upgrading to {acm-short} 2.7 because the BareMetalAssets API is bound to the `ClusterDeployment`.

To work around the issue, run the following command to remove the `finalizers` before upgrading to {acm-short} 2.7:

----
oc patch clusterdeployment <clusterdeployment-name> -p '{"metadata":{"finalizers":null}}' --type=merge 
----

[#discon-disc-iso-cluster-no-install]
=== A cluster deployed in a disconnected environment by using the central infrastructure management service might not install
//2.7:ACM3209

When you deploy a cluster in a disconnected environment by using the central infrastructure management service, the cluster nodes might not start installing. 

This issue occurs because the cluster uses a discovery ISO image that is created from the Red Hat Enterprise Linux CoreOS live ISO image that is shipped with {ocp-short} versions 4.12.0 through 4.12.2. The image contains a restrictive `/etc/containers/policy.json` file that requires signatures for images sourcing from `registry.redhat.io` and `registry.access.redhat.com`. In a disconnected environment, the images that are mirrored might not have the signatures mirrored, which results in the image pull failing for cluster nodes at discovery. The Agent image fails to connect with the cluster nodes, which causes communication with the assisted service to fail.

To work around this issue, apply an ignition override to the cluster that sets the `/etc/containers/policy.json` file to unrestrictive. The ignition override can be set in the `InfraEnv` custom resource definition. The following example shows an `InfraEnv` custom resource definition with the override:

[source.yaml]
----
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: cluster
  namespace: cluster
spec:
  ignitionConfigOverride: '{"ignition":{"version":"3.2.0"},"storage":{"files":[{"path":"/etc/containers/policy.json","mode":420,"overwrite":true,"contents":{"source":"data:text/plain;charset=utf-8;base64,ewogICAgImRlZmF1bHQiOiBbCiAgICAgICAgewogICAgICAgICAgICAidHlwZSI6ICJpbnNlY3VyZUFjY2VwdEFueXRoaW5nIgogICAgICAgIH0KICAgIF0sCiAgICAidHJhbnNwb3J0cyI6CiAgICAgICAgewogICAgICAgICAgICAiZG9ja2VyLWRhZW1vbiI6CiAgICAgICAgICAgICAgICB7CiAgICAgICAgICAgICAgICAgICAgIiI6IFt7InR5cGUiOiJpbnNlY3VyZUFjY2VwdEFueXRoaW5nIn1dCiAgICAgICAgICAgICAgICB9CiAgICAgICAgfQp9"}}]}}' 
----

The following example shows the unrestrictive file that is created:
----
{
    "default": [
        {
            "type": "insecureAcceptAnything"
        }
    ],
    "transports": {
        "docker-daemon": {
        "": [
        {
            "type": "insecureAcceptAnything"
        }
        ]
    }
    }
}
----
 
After this setting is changed, the clusters install. 

[#deploy-managed-stuck-pending]
=== Managed cluster stuck in _Pending_ status after deployment
//2.11:ACM-12722

The converged flow is the default process of provisioning. When you use the `BareMetalHost` resource for the Bare Metal Operator (BMO) to connect your host to a live ISO, the Ironic Python Agent does the following actions:

* It runs the steps in the Bare Metal installer-provisioned-infrastructure. 
* It starts the Assisted Installer agent, and the agent handles the rest of the install and provisioning process. 

If the Assisted Installer agent starts slowly and you deploy a managed cluster, the managed cluster might become stuck in the `Pending` status and not have any agent resources. You can work around the issue by disabling the converged flow. 

*Important:* When you disable the converged flow, only the Assisted Installer agent runs in the live ISO, reducing the number of open ports and disabling any features you enabled with the  Ironic Python Agent agent, including the following:

* Pre-provisioning disk cleaning 
* iPXE boot firmware 
* BIOS configuration 

To decide what port numbers you want to enable or disable without disabling the converged flow, see link:../../clusters/about/mce_networking.adoc#mce-network-configuration[Network configuration]. 

To disable the converged flow, complete the following steps:

. Create the following ConfigMap on the hub cluster:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-assisted-service-config
  namespace: multicluster-engine
data:
  ALLOW_CONVERGED_FLOW: "false" <1> 
----
<1> When you set the parameter value to "false", you also disable any features enabled by the Ironic Python Agent. 

. Apply the ConfigMap by running the following command:
+
----
oc annotate --overwrite AgentServiceConfig agent unsupported.agent-install.openshift.io/assisted-service-configmap=my-assisted-service-config
----

[#managedclusterset-api-limitation]
=== ManagedClusterSet API specification limitation
//2.9:ACM-6423

The `selectorType: LaberSelector` setting is not supported when using the link:../../apis/clusterset.json.adoc#clustersets-api[Clustersets API]. The `selectorType: ExclusiveClusterSetLabel` setting is supported.

[#hub-cluster-one-way-limits]
=== Hub cluster communication limitations
//2.9:ACM-6292

The following limitations occur if the hub cluster is not able to reach or communicate with the managed cluster:

- You cannot create a new managed cluster by using the console. You are still able to import a managed cluster manually by using the command line interface or by using the *Run import commands manually* option in the console.
- If you deploy an Application or ApplicationSet by using the console, or if you import a managed cluster into ArgoCD, the hub cluster ArgoCD controller calls the managed cluster API server. You can use AppSub or the ArgoCD pull model to work around the issue.
- The console page for pod logs does not work, and an error message that resembles the following appears:
+
----
Error querying resource logs:
Service unavailable
----

[#manageserviceaccount-addon-limitation]
=== Managed Service Account add-on limitations
//2.9:ACM-8586

The following are known issues and limitations for the `managed-serviceaccount` add-on:

[#installnamespace-field-limit]
==== _installNamespace_ field can only have one value
//2.9:ACM-7523

When enabling the `managed-serviceaccount` add-on, the `installNamespace` field in the `ManagedClusterAddOn` resource must have `open-cluster-management-agent-addon` as the value. Other values are ignored. The `managed-serviceaccount` add-on agent is always deployed in the `open-cluster-management-agent-addon` namespace on the managed cluster.

[#settings-limit-msa-agent]
==== _tolerations_ and _nodeSelector_ settings do not affect the _managed-serviceaccount_ agent
//2.9:ACM-7523

The `tolerations` and `nodeSelector` settings configured on the `MultiClusterEngine` and `MultiClusterHub` resources do not affect the `managed-serviceaccount` agent deployed on the local cluster. The `managed-serviceaccount` add-on is not always required on the local cluster.

If the `managed-serviceaccount` add-on is required, you can work around the issue by completing the following steps:

. Create the `addonDeploymentConfig` custom resource.

. Set the `tolerations` and `nodeSelector` values for the local cluster and `managed-serviceaccount` agent.

. Update the `managed-serviceaccount` `ManagedClusterAddon` in the local cluster namespace to use the `addonDeploymentConfig` custom resource you created.

See link:../../add-ons/configure_nodeselector_tolerations_addons.adoc#configure-nodeselector-tolerations-addons[Configuring nodeSelectors and tolerations for klusterlet add-ons] to learn more about how to use the `addonDeploymentConfig` custom resource to configure `tolerations` and `nodeSelector` for add-ons.

[#bulk-destroy-kubevirt-hosted]
=== Bulk destroy option on KubeVirt hosted cluster does not destroy hosted cluster
//2.10:ACM-10174

Using the bulk destroy option in the console on KubeVirt hosted clusters does not destroy the KubeVirt hosted clusters.

Use the row action drop-down menu to destroy the KubeVirt hosted cluster instead.

[#ocp-ded-support-curator]
=== The Cluster curator does not support {ocp-short} Dedicated clusters
//2.10:ACM-10154

When you upgrade an {ocp-short} Dedicated cluster by using the `ClusterCurator` resource, the upgrade fails because the Cluster curator does not support {ocp-short} Dedicated clusters.

[#custom-ingress-domain-limitation]
=== Custom ingress domain is not applied correctly
//2.8:ACM-6279

You can specify a custom ingress domain by using the `ClusterDeployment` resource while installing a managed cluster, but the change is only applied after the installation by using the `SyncSet` resource. As a result, the `spec` field in the `clusterdeployment.yaml` file displays the custom ingress domain you specified, but the `status` still displays the default domain.

[#hosted-control-plane-mce]
== Hosted control planes

[#console-hosted-pending-import]
=== Console displays hosted cluster as Pending import 
//2.7:25594

If the annotation and `ManagedCluster` name do not match, the console displays the cluster as `Pending import`. The cluster cannot be used by the {mce-short}. The same issue happens when there is no annotation and the `ManagedCluster` name does not match the `Infra-ID` value of the `HostedCluster` resource."

[#add-node-pool-duplicate-version]
=== Console might list the same version multiple times when adding a node pool to a hosted cluster 
//2.7:ACM-2664

When you use the console to add a new node pool to an existing hosted cluster, the same version of {ocp-short} might appear more than once in the list of options. You can select any instance in the list for the version that you want. 

[#deleted-nodes-shown-in-console]
=== The web console lists nodes even after they are removed from the cluster and returned to the infrastructure environment
//2.9:ACM-8334

When a node pool is scaled down to 0 workers, the list of hosts in the console still shows nodes in a `Ready` state. You can verify the number of nodes in two ways:

* In the console, go to the node pool and verify that it has 0 nodes.
* On the command line interface, run the following commands:

** Verify that 0 nodes are in the node pool by running the following command:

+
[source,bash]
----
oc get nodepool -A
----

** Verify that 0 nodes are in the cluster by running the following command:

+
[source,bash]
----
oc get nodes --kubeconfig
----

** Verify that 0 agents are reported as bound to the cluster by running the following command:

+
[source,bash]
----
oc get agents -A
----

[#hosted-cluster-dual-stack]
=== Potential DNS issues in hosted clusters configured for a dual-stack network
//2.9:ACM-8527

When you create a hosted cluster in an environment that uses the dual-stack network, you might encounter the following DNS-related issues:

* `CrashLoopBackOff` state in the `service-ca-operator` pod: When the pod tries to reach the Kubernetes API server through the hosted control plane, the pod cannot reach the server because the data plane proxy in the `kube-system` namespace cannot resolve the request. This issue occurs because in the HAProxy setup, the front end uses an IP address and the back end uses a DNS name that the pod cannot resolve.
* Pods stuck in `ContainerCreating` state: This issue occurs because the `openshift-service-ca-operator` cannot generate the `metrics-tls` secret that the DNS pods need for DNS resolution. As a result, the pods cannot resolve the Kubernetes API server.

To resolve those issues, configure the DNS server settings by following the guidelines in xref:../hosted_control_planes/dual_stack_dns.adoc#dual-stack-dns[Configuring DNS for a dual stack network].

[#hosted-agent-ignition-bug]
=== On bare metal platforms, Agent resources might fail to pull ignition 
//2.9:ACM-8736

On the bare metal (Agent) platform, the hosted control planes feature periodically rotates the token that the Agent uses to pull ignition. A bug causes the new token to not be propagated. As a result, if you have an Agent resource that was created some time ago, it might fail to pull ignition.

As a workaround, in the Agent specification, delete the secret that the `IgnitionEndpointTokenReference` property refers to, and then add or modify any label on the Agent resource. The system can then detect that the Agent resource was modified and re-create the secret with the new token.

[#ibmz-hosts-restart-loop]
=== IBM Z hosts restart in a loop
//2.11:MGMT-17103

In hosted control planes on the IBM Z platform, when you unbind the hosts with the cluster, the hosts restart in loop and are not ready to be used. For a workaround for this issue, see link:../../clusters/hosted_control_planes/destroy_hosted_cluster_x86bm_ibmz.adoc#destroy-hosted-cluster-x86bm-ibmz[Destroying a hosted cluster on x86 bare metal with IBM Z compute nodes].