[#customizing-observability]
= Customizing observability

Review the following sections to learn more about customizing, managing, and viewing data that is collected by the observability service.

Collect logs about new information that is created for observability resources with the `must-gather` command. For more information, see the _Must-gather_ section in the link:../troubleshooting/troubleshooting_intro.adoc[Troubleshooting documentation].

* <<creating-custom-rules,Creating custom rules>>
* <<configuring-rules-for-alertmanager,Configuring rules for AlertManager>>
* <<adding-custom-metrics, Adding custom metrics>>
* <<viewing-and-exploring-data,Viewing and exploring data>>
* <<disable-metrics-collector,Disable _metrics-collector_>>

[#creating-custom-rules]
== Creating custom rules

You can create custom rules for the observability installation by adding Prometheus https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/[recording rules] and https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[alerting rules] to the observability resource. For more information, see https://prometheus.io/docs/prometheus/latest/configuration/configuration/[Prometheus configuration].

** Recording rules provide you the ability to precalculate, or computate expensive expressions as needed. The results are saved as a new set of time series.
** Alerting rules provide you the ability to specify the alert conditions based on how an alert should be sent to an external service.

Define custom rules with Prometheus to create alert conditions, and send notifications to an external messaging service. *Note*: When you update your custom rules, `observability-observatorium-thanos-rule` pods are restarted automatically.

Complete the following steps to create a custom rule: 

. Log in to your {product-title-short} hub cluster.
. Create a ConfigMap named `thanos-ruler-custom-rules` in the `open-cluster-management-observability` namespace. The key must be named, `custom_rules.yaml`, as shown in the following example. You can create multiple rules in the configuration:
+
* By default, the out-of-the-box alert rules are defined in the `thanos-ruler-default-rules` ConfigMap in the  `open-cluster-management-observability` namespace. 
+
For example, you can create a custom alert rule that notifies you when your CPU usage passes your defined value: 
+
----
data:
  custom_rules.yaml: |
    groups:
      - name: cluster-health
        rules:
        - alert: ClusterCPUHealth-jb
          annotations:
            summary: Notify when CPU utilization on a cluster is greater than the defined utilization limit
            description: "The cluster has a high CPU usage: {{ $value }} core for {{ $labels.cluster }} {{ $labels.clusterID }}."
          expr: |
            max(cluster:cpu_usage_cores:sum) by (clusterID, cluster, prometheus) > 0
          for: 5s
          labels:
            cluster: "{{ $labels.cluster }}"
            prometheus: "{{ $labels.prometheus }}"
            severity: critical
----
+
* You can also create a custom recording rule within the `thanos-ruler-custom-rules` ConfigMap.
+
For example, you can create a recording rule that provides you the ability to get the sum of the container memory cache of a pod. Your YAML might resemble the following content:
+
----
data:
  custom_rules.yaml: |
    groups:
      - name: container-memory
        rules:
        - record: pod:container_memory_cache:sum
          expr: sum(container_memory_cache{pod!=""}) BY (pod, container)
----
+
*Note*: If this is the first new custom rule, it is created immediately. For changes to the ConfigMap, you must restart the observability pods with the following command: `kubectl rollout restart statefulset observability-observatorium-thanos-rule -n open-cluster-management-observability`.

. If you want to verify that the alert rules is functioning appropriately, complete the following steps:
.. Access your Grafana dashboard and select the *Explore* icon.
.. In the Metrics exploration bar, type in "ALERTS" and run the query. All the ALERTS that are currently in pending or firing state in the system are displayed.
.. If your alert is not displayed, revisit the rule to see if the expression is accurate.

A custom rule is created.

[#configuring-rules-for-alertmanager]
== Configuring rules for AlertManager

Integrate external messaging tools such as email, Slack, and PagerDuty to receive notifications from AlertManager. You must override the `alertmanager-config` secret in the `open-cluster-management-observability` namespace to add integrations, and configure routes for AlertManager. Complete the following steps to update the custom receiver rules:

. Extract the data from the `alertmanager-config` secret. Run the following command:
+
----
oc -n open-cluster-management-observability get secret alertmanager-config --template='{{ index .data "alertmanager.yaml" }}' |base64 -d > alertmanager.yaml
----

. Edit and save the `alertmanager.yaml` file configuration by running the following command:
+
----
oc -n open-cluster-management-observability create secret generic alertmanager-config --from-file=alertmanager.yaml --dry-run -o=yaml |  oc -n open-cluster-management-observability replace secret --filename=-
----
+
Your updated secret might resemble the following content:
+
----
global
  smtp_smarthost: 'localhost:25'
  smtp_from: 'alertmanager@example.org'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: 'password'
templates: 
- '/etc/alertmanager/template/*.tmpl'
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 3h 
  receiver: team-X-mails
  routes:
  - match_re:
      service: ^(foo1|foo2|baz)$
    receiver: team-X-mails
----

Your changes are applied immediately after it is modified. For an example of AlertManager, see https://github.com/prometheus/alertmanager/blob/master/doc/examples/simple.yml[prometheus/alertmanager].

[#adding-custom-metrics]
== Adding custom metrics

Add metrics to the `metrics_list.yaml` file, to be collected from managed clusters.

Complete the following steps to add custom metrics:

. Log in to your cluster.
. Verify that `mco observability` is enabled. Check for the following message in the `status.conditions.message` reads: `Observability components are deployed and running`. Run the following command:
+
----
oc get mco observability -o yaml
----

. Create a new file `observability-metrics-custom-allowlist.yaml` with the following content. Add the name of the custom metric to the `metrics_list.yaml` parameter. For example, add `node_memory_MemTotal_bytes` to the metric list. Your YAML for the ConfigMap might resemble the following content:
+
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: observability-metrics-custom-allowlist
data:
  metrics_list.yaml: |
    names:
      - node_memory_MemTotal_bytes
----

. Create the `observability-metrics-custom-allowlist` ConfigMap in the 
`open-cluster-management-observability` namespace by running the following command:
+
----
oc apply -n open-cluster-management-observability -f observability-metrics-custom-allowlist.yaml
----

. Verify that your custom metric is being collected from your managed clusters by viewing the metric on the Grafana dashboard. From your hub cluster, select the **Grafana dashboard** link.

. From the Grafana search bar, enter the metric that you want to view.

Data from your custom metric is collected.

[#viewing-and-exploring-data]
== Viewing and exploring data

View the data from your managed clusters by accessing Grafana. Complete the following steps to view the Grafana dashboards from the console:

. Log in to your {product-title-short} hub cluster. 
. From the navigation menu, select *Observe environments* > *Overview* > *Grafana link*. 
+
You can also  access Grafana dashboards from the _Clusters_ page. From the navigation menu, select *Automate infrastructure* > *Clusters* > *Grafana*.
. Access the Prometheus metric explorer by selecting the *Explore* icon from the Grafana navigation menu.

[#disable-metrics-collector]
== Disable _metrics-collector_

You can disable the `metrics-collector`, which stops it from collecting the data and sending the collection data to the observability service. 

[#disable-metrics-collector-on-all-clusters]
=== Disable _metrics-collector_ on all clusters

Disable the `metrics-collector` by removing observability components on all managed clusters. This stops data from being collected and sent to the observability service on the {product-title-short} hub cluster.

Update the `multicluster-observability-operator` resource by setting `enableMetrics` to `false`. Your updated resource might resemble the following change:

----
spec:
  availabilityConfig: High # Available values are High or Basic
  imagePullPolicy: Always
  imagePullSecret: multiclusterhub-operator-pull-secret
  observabilityAddonSpec: # The ObservabilityAddonSpec defines the global settings for all managed clusters which have observability add-on enabled
    enableMetrics: false #indicates the observability addon push metrics to hub server
----

[#disable-metrics-collector-on-a-single-cluster]
=== Disable _metrics-collector_ on a single cluster

You can disable the `metrics-collector` on specific managed clusters by completing one of the following procedures:

* Add the `observability: disabled` label to the custom resource, `managedclusters.cluster.open-cluster-management.io`.
* From the {product-title-short} console _Clusters_ page, add the `observability: disabled` label by completing the following steps:
+
. In the {product-title-short} console navigation, select *Automate infrastructure* > *Clusters*.
. Select the name of the cluster for which you want to disable data collection that is sent to observability. 
. Select *Labels*.
. Create the label that disables the observability collection by adding the following label:
+
----
observability=disabled
----
. Select *Add* to add the label.
. Select *Done* to close the list of labels. 

*Note*: When a managed cluster with the observability component is detached, the `metrics-collector` deployments are removed.

For more information on monitoring data from the console with the observability service, see xref:../observing_environments/observe_environments_intro.adoc#observing-environments-intro[Observing environments introduction].
